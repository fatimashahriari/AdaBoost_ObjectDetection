{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "save_directory = \"/home/azureuser/cloudfiles/code/Users/shahriarizadehfatima\"\n",
        "import os\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageOps\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import functional as F_transform\n",
        "from torchvision.transforms import Normalize as NormalizeTransform\n",
        "from torchvision.ops.boxes import box_iou\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from albumentations import Compose, Resize, HorizontalFlip, BboxParams, VerticalFlip, Normalize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from azureml.core import Dataset, Datastore, Workspace\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using device: cuda\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/Object_Detection/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1743749890639
        },
        "id": "ZWlhDYdB8UJJ",
        "outputId": "fd071291-c586-4286-bc7f-bf48341a4840"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "num_workers = 3"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "json_dir = \"/home/azureuser/cloudfiles/code/Users/shahriarizadehfatima/config.json\"\n",
        "# Define folders to exclude\n",
        "exclude_folders = {}\n",
        "# Load workspace from config.json\n",
        "ws = Workspace.from_config(json_dir)\n",
        "datastore = Datastore.get(ws, \"fasteners_1\")\n",
        "dataset_images = Dataset.File.from_files(path=(datastore, 'images/'))\n",
        "mounted_context_images = dataset_images.mount()\n",
        "mounted_context_images.start()\n",
        "# Get the mount path\n",
        "images_dir = mounted_context_images.mount_point\n",
        "images_dir_folders = [folder for folder in os.listdir(images_dir) if folder not in exclude_folders]\n",
        "#dataset_labels\n",
        "dataset_labels = Dataset.File.from_files(path=(datastore, 'labels/'))\n",
        "mounted_context_labels = dataset_labels.mount()\n",
        "mounted_context_labels.start()\n",
        "# Get the mount path\n",
        "labels_dir = mounted_context_labels.mount_point\n",
        "labels_dir_folders = [folder for folder in os.listdir(labels_dir) if folder not in exclude_folders]\n",
        "classes_file = labels_dir_folders"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Available datastores: ['azureml_globaldatasets', 'fasteners_1', 'workspacefilestore', 'workspaceworkingdirectory', 'workspaceartifactstore', 'workspaceblobstore']\nImages Dataset mounted at: /tmp/tmpy9owx32p ['Cable Straps Edge Mount', 'Cable Straps Hole Mount', 'Tape-on Locators Edge Mount', 'Tape-on Locators Hole Mount']\nLabels Dataset mounted at: /tmp/tmphayze_bw ['Cable Straps Edge Mount', 'Cable Straps Hole Mount', 'Tape-on Locators Edge Mount', 'Tape-on Locators Hole Mount']\nclasses_file: ['Cable Straps Edge Mount', 'Cable Straps Hole Mount', 'Tape-on Locators Edge Mount', 'Tape-on Locators Hole Mount']\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1743749898096
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of class names (subdirectories in images/)\n",
        "class_names = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n",
        "num_classes = len(class_names)+ 1\n",
        "\n",
        "# Lists to store file paths for each split\n",
        "train_files = []\n",
        "test_files = []\n",
        "val_files = []\n",
        "\n",
        "# Split the data and store file paths\n",
        "for class_name in class_names:\n",
        "    image_class_dir = os.path.join(images_dir, class_name)\n",
        "    label_class_dir = os.path.join(labels_dir, class_name)\n",
        "    image_files = [f for f in os.listdir(image_class_dir)]\n",
        "\n",
        "    num_images = len(image_files)\n",
        "    train_split = int(train_ratio * num_images)\n",
        "    test_split = int(test_ratio * num_images)\n",
        "\n",
        "    train_image_files = image_files[:train_split]\n",
        "    test_image_files = image_files[train_split:train_split + test_split]\n",
        "    val_image_files = image_files[train_split + test_split:]\n",
        "\n",
        "    for split, files in zip(['train', 'test', 'val'], [train_image_files, test_image_files, val_image_files]):\n",
        "        for file_name in files:\n",
        "            image_path = os.path.join(image_class_dir, file_name)\n",
        "            label_path = os.path.join(label_class_dir, file_name.replace(os.path.splitext(file_name)[1], '.txt'))\n",
        "            if split == 'train':\n",
        "                train_files.append((image_path, label_path, class_name))\n",
        "            elif split == 'test':\n",
        "                test_files.append((image_path, label_path, class_name))\n",
        "            elif split == 'val':\n",
        "                val_files.append((image_path, label_path, class_name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "len tarain: 275\nlen tarain: 77\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1743749898394
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_list, classes_file, image_size=(224, 224), augmentation=False):\n",
        "        self.file_list = file_list\n",
        "        self.classes = list_subfolders(classes_file)\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.new_width, self.new_height = image_size\n",
        "        self.resize = Resize(height=self.new_height, width=self.new_width)\n",
        "        self.normalize = NormalizeTransform(mean=[0.485, 0.456, 0.406],\n",
        "                                            std=[0.229, 0.224, 0.225])\n",
        "        self.augmentations = get_augmentations() if augmentation else []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list) * (len(self.augmentations) + 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_idx = idx // (len(self.augmentations) + 1)\n",
        "        aug_idx = idx % (len(self.augmentations) + 1)\n",
        "        img_path, label_path, _ = self.file_list[img_idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        img = np.array(image)\n",
        "        img = self.resize(image=img)['image']\n",
        "\n",
        "        bboxes, labels = [], []\n",
        "        with open(label_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"Class:\"):\n",
        "                    class_name = line.split(\":\")[1].strip().strip('\"')\n",
        "                elif line.startswith(\"Bounding Box:\"):\n",
        "                    bbox = eval(line.split(\":\")[1].strip())\n",
        "                    bboxes.append(bbox)\n",
        "                    labels.append(self.classes.index(class_name) + 1)\n",
        "\n",
        "        if not bboxes:\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "        # Scale bboxes\n",
        "        orig_width, orig_height = image.size\n",
        "        scale_x, scale_y = self.new_width / orig_width, self.new_height / orig_height\n",
        "        bboxes = [[x1 * scale_x, y1 * scale_y, x2 * scale_x, y2 * scale_y] for x1, y1, x2, y2 in bboxes]\n",
        "\n",
        "        if aug_idx != 0:\n",
        "            aug = self.augmentations[aug_idx - 1]\n",
        "            augmented = aug(image=img, bboxes=bboxes, class_labels=labels)\n",
        "            img = augmented['image']\n",
        "            bboxes = augmented['bboxes']\n",
        "            labels = augmented['class_labels']\n",
        "            img = torch.from_numpy(img.astype(np.float32) / 255.0).permute(2, 0, 1)\n",
        "        else:\n",
        "            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        img = self.normalize(img)\n",
        "        return idx, img, {\"boxes\": torch.tensor(bboxes, dtype=torch.float32),\n",
        "                          \"labels\": torch.tensor(labels, dtype=torch.int64)}\n",
        "def list_subfolders(main_folder):\n",
        "    if isinstance(main_folder, list):\n",
        "        return main_folder\n",
        "    if not os.path.isdir(main_folder):\n",
        "        raise ValueError(f\"The provided path '{main_folder}' is not valid.\")\n",
        "    return [name for name in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, name))]\n",
        "\n",
        "def custom_collate(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    return torch.stack(images, 0), list(targets)\n",
        "\n",
        "def get_augmentations():\n",
        "    bbox_params = BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"])\n",
        "    return [\n",
        "        Compose([HorizontalFlip()], bbox_params=bbox_params),\n",
        "        Compose([VerticalFlip()], bbox_params=bbox_params)\n",
        "    ]\n",
        "\n",
        "class NormalizeTransform:\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return (tensor - self.mean) / self.std\n",
        "\n",
        "def denormalize(tensor, mean, std):\n",
        "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "    std = torch.tensor(std).view(-1, 1, 1)\n",
        "    return tensor * std + mean\n",
        "\n",
        "transform = ToTensorV2()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/Object_Detection/lib/python3.10/site-packages/albumentations/core/composition.py:250: UserWarning: Got processor for bboxes, but no transform to process it.\n  self._set_keys()\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1743749898641
        },
        "id": "9HSKYqDx9Ly3",
        "outputId": "3b345d14-53a8-4768-a020-257d1434c02c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset instances\n",
        "train_dataset = CustomDataset(file_list = train_files, classes_file=  classes_file , augmentation= Augmentation, image_size=(224,224))\n",
        "test_dataset = CustomDataset(file_list= test_files, classes_file=  classes_file)\n",
        "val_dataset = CustomDataset(file_list= val_files, classes_file=  classes_file)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers= num_workers, shuffle=True, collate_fn=lambda batch: tuple(zip(*batch)))\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,  num_workers= num_workers, shuffle=True, collate_fn=lambda batch: tuple(zip(*batch)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,  num_workers= num_workers, shuffle=True, collate_fn=lambda batch: tuple(zip(*batch)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of training images: 275\nNumber of validation images: 77\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1743749898904
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone_name=\"resnet50\",\n",
        "        pretrained=True,\n",
        "        num_classes=2,\n",
        "        learning_rate=0.005,\n",
        "        weight_decay=0.0005,\n",
        "        num_epochs=10,\n",
        "        detections_per_img=100,\n",
        "        rpn_nms_thresh=0.5,\n",
        "        rpn_score_thresh=0.4,\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the trainer with model and training configurations.\n",
        "\n",
        "        Args:\n",
        "            backbone_name (str): Name of the backbone (e.g., 'resnet34', 'resnet50', 'resnet101').\n",
        "            pretrained (bool): Whether to use a pretrained backbone.\n",
        "            num_classes (int): Number of classes (including background).\n",
        "            learning_rate (float): Initial learning rate for SGD optimizer.\n",
        "            weight_decay (float): Weight decay for regularization.\n",
        "            num_epochs (int): Number of training epochs.\n",
        "            detections_per_img (int): Max detections per image.\n",
        "            rpn_nms_thresh (float): NMS threshold for RPN proposals.\n",
        "            rpn_score_thresh (float): Score threshold for RPN proposals.\n",
        "            device (str): Device to train on ('cuda' or 'cpu').\n",
        "        \"\"\"\n",
        "        self.backbone_name = backbone_name\n",
        "        self.pretrained = pretrained\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.num_epochs = num_epochs\n",
        "        self.detections_per_img = detections_per_img\n",
        "        self.rpn_nms_thresh = rpn_nms_thresh\n",
        "        self.rpn_score_thresh = rpn_score_thresh\n",
        "        self.device = device\n",
        "\n",
        "        self.model = self._build_model()\n",
        "        self.optimizer = torch.optim.SGD(\n",
        "            self.model.parameters(),\n",
        "            lr=self.learning_rate,\n",
        "            momentum=0.9,\n",
        "            weight_decay=self.weight_decay\n",
        "        )\n",
        "        self.lr_scheduler = ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode=\"min\",\n",
        "            factor=0.1,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "        self.metric = MeanAveragePrecision()\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Set up the Faster R-CNN model with custom configurations.\"\"\"\n",
        "        # Define backbone\n",
        "        backbone = resnet_fpn_backbone(self.backbone_name, pretrained=self.pretrained)\n",
        "\n",
        "        # Custom anchor sizes and aspect ratios\n",
        "        anchor_sizes = ((32,), (64,), (128,), (140,), (224,))\n",
        "        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
        "        custom_anchor_generator = AnchorGenerator(\n",
        "            sizes=anchor_sizes,\n",
        "            aspect_ratios=aspect_ratios\n",
        "        )\n",
        "\n",
        "        # Initialize Faster R-CNN model\n",
        "        model = FasterRCNN(\n",
        "            backbone,\n",
        "            num_classes=self.num_classes,\n",
        "            rpn_pre_nms_top_n_train=100,\n",
        "            rpn_post_nms_top_n_train=100,\n",
        "            rpn_nms_thresh=self.rpn_nms_thresh,\n",
        "            rpn_score_thresh=self.rpn_score_thresh,\n",
        "            rpn_anchor_generator=custom_anchor_generator,\n",
        "            box_fg_iou_thresh=0.5,\n",
        "            box_bg_iou_thresh=0.5\n",
        "        )\n",
        "\n",
        "        # Replace the box predictor\n",
        "        representation_size = model.roi_heads.box_predictor.cls_score.in_features\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictor(\n",
        "            representation_size,\n",
        "            self.num_classes\n",
        "        )\n",
        "        model.roi_heads.detections_per_img = self.detections_per_img\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_one_epoch(self, train_loader, epoch):\n",
        "        \"\"\"Train the model for one epoch.\"\"\"\n",
        "        self.model.train()\n",
        "        train_loss = 0.0\n",
        "        train_loader_progress = tqdm(\n",
        "            train_loader,\n",
        "            desc=f\"Training Epoch {epoch + 1}\",\n",
        "            unit=\"batch\"\n",
        "        )\n",
        "\n",
        "        for images, targets in train_loader_progress:\n",
        "            images = [img.to(self.device) for img in images]\n",
        "            targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                loss_dict = self.model(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                train_loss += losses.item()\n",
        "\n",
        "            self.scaler.scale(losses).backward()\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "\n",
        "            train_loader_progress.set_postfix(loss=f\"{losses.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        return avg_train_loss\n",
        "\n",
        "    def validate_one_epoch(self, val_loader, epoch):\n",
        "        \"\"\"Validate the model for one epoch.\"\"\"\n",
        "        self.model.eval()\n",
        "        val_loss = 0.0\n",
        "        self.metric.reset()\n",
        "        val_loader_progress = tqdm(\n",
        "            val_loader,\n",
        "            desc=f\"Validation Epoch {epoch + 1}\",\n",
        "            unit=\"batch\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, targets in val_loader_progress:\n",
        "                images = [img.to(self.device) for img in images]\n",
        "                targets = [\n",
        "                    {k: v.to(self.device) for k, v in t.items()} for t in targets\n",
        "                ]\n",
        "\n",
        "                # Compute validation loss\n",
        "                loss_dict = self.model(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                val_loss += losses.item()\n",
        "\n",
        "                # Compute mAP\n",
        "                outputs = self.model(images)\n",
        "                preds = [\n",
        "                    dict(\n",
        "                        boxes=output[\"boxes\"].detach().cpu(),\n",
        "                        scores=output[\"scores\"].detach().cpu(),\n",
        "                        labels=output[\"labels\"].detach().cpu()\n",
        "                    )\n",
        "                    for output in outputs\n",
        "                ]\n",
        "                gt_targets = [\n",
        "                    dict(\n",
        "                        boxes=t[\"boxes\"].detach().cpu(),\n",
        "                        labels=t[\"labels\"].detach().cpu()\n",
        "                    )\n",
        "                    for t in targets\n",
        "                ]\n",
        "                self.metric.update(preds, gt_targets)\n",
        "\n",
        "                val_loader_progress.set_postfix(loss=f\"{losses.item():.4f}\")\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        mAP_result = self.metric.compute()\n",
        "        mAP_score = mAP_result[\"map\"].item()\n",
        "        return avg_val_loss, mAP_score\n",
        "\n",
        "    def train(self, train_loader, val_loader):\n",
        "        \"\"\"Run the full training loop.\"\"\"\n",
        "        print(\"Learning has kicked off!\")\n",
        "        start_time = datetime.now()\n",
        "        print(f\"Started at: {start_time.strftime('%H:%M:%S')}\")\n",
        "\n",
        "        training_losses = []\n",
        "        validation_losses = []\n",
        "        mAP_values = []\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{self.num_epochs} is underway...\")\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Train\n",
        "            train_loss = self.train_one_epoch(train_loader, epoch)\n",
        "            training_losses.append(train_loss)\n",
        "\n",
        "            # Validate\n",
        "            val_loss, mAP_score = self.validate_one_epoch(val_loader, epoch)\n",
        "            validation_losses.append(val_loss)\n",
        "            mAP_values.append(mAP_score)\n",
        "\n",
        "            # Update learning rate\n",
        "            self.lr_scheduler.step(val_loss)\n",
        "\n",
        "            # Log results\n",
        "            epoch_time = datetime.now()\n",
        "            print(f\"Epoch {epoch + 1}: mAP = {mAP_score:.4f}\")\n",
        "            print(f\"Epoch {epoch + 1} ended at: {epoch_time.strftime('%H:%M:%S')}\")\n",
        "\n",
        "        print(\"Training’s all done!\")\n",
        "        return training_losses, validation_losses, mAP_values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "class_names = test_dataset.classes\n",
        "\n",
        "def test_model_adaboost(models, test_loader, device, class_names, model_weights, visualize=False):\n",
        "    # Move models to device and set to eval mode\n",
        "    for model in models:\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "    # Initialize metrics and accumulators\n",
        "    metric = MeanAveragePrecision()\n",
        "    total_test_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idxs, images, targets in test_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Collect predictions from all models\n",
        "            all_predictions = []\n",
        "            for model in models:\n",
        "                pred = model(images)\n",
        "                all_predictions.append(pred)\n",
        "\n",
        "            # Apply Weighted NMS using AdaBoost weights\n",
        "            final_preds = weighted_nms(all_predictions, model_weights)\n",
        "\n",
        "            # Compute loss (optional, only for evaluation)\n",
        "            total_loss = 0\n",
        "            for model in models:\n",
        "                loss_dict = model(images, targets)\n",
        "                if isinstance(loss_dict, list):\n",
        "                    losses = sum(sum(loss_value.sum() for loss_value in loss.values()) for loss in loss_dict)\n",
        "                elif isinstance(loss_dict, dict):\n",
        "                    losses = sum(loss_value.sum() for loss_value in loss_dict.values())\n",
        "                else:\n",
        "                    raise TypeError(f\"Unexpected loss format: {type(loss_dict)}\")\n",
        "                total_loss += losses.item()\n",
        "\n",
        "            total_test_loss += total_loss / len(models)  # Averaging loss over models\n",
        "\n",
        "            # Convert ground truth to required format for mAP computation\n",
        "            gt_targets = [\n",
        "                dict(\n",
        "                    boxes=t[\"boxes\"].detach().cpu(),\n",
        "                    labels=t[\"labels\"].detach().cpu()\n",
        "                ) for t in targets\n",
        "            ]\n",
        "\n",
        "            # Update metric\n",
        "            metric.update(final_preds, gt_targets)\n",
        "\n",
        "            # Optional: Visualize some predictions\n",
        "            if visualize:\n",
        "                visualize_predictions(images, final_preds, class_names)\n",
        "\n",
        "    # Compute Mean Average Precision (mAP)\n",
        "    mAP_result = metric.compute()\n",
        "    mAP_score = mAP_result[\"map\"].item()\n",
        "    print(f\"\\nMean Average Precision (mAP): {mAP_score:.4f}\")\n",
        "\n",
        "    # Compute average test loss\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    print(f\"\\nAverage Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "    return mAP_score, avg_test_loss\n",
        "\n",
        "def visualize_predictions(images, predictions, class_names):\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        # Denormalize image\n",
        "        img = denormalize(img.cpu(), mean, std)\n",
        "        img = F_transform.to_pil_image(img)  # Convert to PIL image\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "        ax.imshow(img)\n",
        "\n",
        "        # Get predicted boxes, labels, and scores\n",
        "        pred_boxes = predictions[i]['boxes'].cpu().numpy()\n",
        "        pred_labels = predictions[i]['labels'].cpu().numpy()\n",
        "        pred_scores = predictions[i]['scores'].cpu().numpy()\n",
        "\n",
        "        if len(pred_scores) == 0:\n",
        "            print(\"No predictions found for this image.\")\n",
        "            continue\n",
        "\n",
        "        # Find the highest confidence score\n",
        "        pred_scores_tensor = torch.tensor(pred_scores) \n",
        "        max_score_idx = pred_scores_tensor.argmax()  # Get index of highest score\n",
        "\n",
        "        # Get highest confidence box, label, and score\n",
        "        best_box = pred_boxes[max_score_idx]\n",
        "        best_label = pred_labels[max_score_idx]\n",
        "        best_score = pred_scores[max_score_idx]\n",
        "        ixd_best_label = pred_labels[max_score_idx].item()\n",
        "        predicted_label_name = class_names[ixd_best_label - 1]\n",
        "\n",
        "        # Draw Highest Confidence Bounding Box (Red)\n",
        "        xmin, ymin, xmax, ymax = best_box\n",
        "        rect = plt.Rectangle(\n",
        "            (xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "            fill=False, color=\"red\", linewidth=2\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Display label and score\n",
        "        ax.text(\n",
        "            #xmin, ymin - 5, f\"{predicted_label_name}: {best_score:.2f}\",\n",
        "            xmin, ymin - 5, f\"{predicted_label_name}\",\n",
        "            color=\"red\", fontsize=12, bbox=dict(facecolor='white', alpha=0.7)\n",
        "        )\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Highest Confidence Prediction\")\n",
        "        plt.show()"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1743750245814
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on the test dataset\n",
        "test_model(model, val_loader, device, val_dataset.classes, visualize=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernel_info": {
      "name": "mykernel"
    },
    "kernelspec": {
      "name": "mykernel",
      "language": "python",
      "display_name": "new_kernel"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}