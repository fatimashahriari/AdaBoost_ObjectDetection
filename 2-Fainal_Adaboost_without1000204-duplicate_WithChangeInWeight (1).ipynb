{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#bash:  conda create -n Object_Detection python=3.10\n",
        "#bash: conda activate Object_Detection\n",
        "#bash: conda install ipykernel\n",
        "#bash: python -m ipykernel install --user --name=\"MyKernel\" --display-name \"new_kernel\"\n",
        "#bash conda install package_name\n",
        "# bash: torchrun --nproc_per_node=4 Users/shahriarizadehfatima/Faster_R_CNN_all_images_27classes_with augmentation-20 epochs.ipynb\n",
        "#jupyter nbconvert --to script \"Faster_R_CNN_all_images_27classes_with augmentation-20 epochs.ipynb\"\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1743749858288
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1743749858549
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model_Save = \"1000106_.pth\"\n",
        "batch_size = 2\n",
        "#learning_rate_SGD = 0.0025  # Can decay over time\n",
        "#learning_rate_adamW = 0.0001\n",
        "#weight_decay_SGD = 0.001\n",
        "#weight_decay_adamW = 0.01\n",
        "#momentum = 0.9\n",
        "#warmup_epochs = 0\n",
        "#num_epochs = 13\n",
        "seed = 40\n",
        "#DropOut = False\n",
        "#ChangedBackbone = False\n",
        "# Define the split ratios\n",
        "train_ratio = 0.75\n",
        "test_ratio = 0.05\n",
        "val_ratio = 0.20\n",
        "#Backbone_ = \"resnet50\"\n",
        "Pretrained_status = True\n",
        "#Plateau_Factor = 0.5\n",
        "#Plateau_Patience = 2\n",
        "pred_per_image = 15\n",
        "num_workers = 3"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1743749858902
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!which python"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/anaconda/envs/azureml_py38/bin//python\r\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/anaconda/envs/Object_Detection/bin/python\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1743749859631
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/anaconda/envs/jupyter_env/bin/python -m pip install numpy pandas scikit-learn torch torchvision albumentations matplotlib seaborn\n",
        "!/anaconda/envs/Object_Detection/bin/python -m pip install numpy pandas scikit-learn torch torchvision albumentations matplotlib seaborn tqdm torchmetrics"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: numpy in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (2.2.4)\nRequirement already satisfied: pandas in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (2.2.3)\nRequirement already satisfied: scikit-learn in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (1.6.1)\nRequirement already satisfied: torch in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (2.6.0)\nRequirement already satisfied: torchvision in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (0.21.0)\nRequirement already satisfied: albumentations in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (2.0.4)\nRequirement already satisfied: matplotlib in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (3.10.1)\nRequirement already satisfied: seaborn in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: tqdm in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (4.67.1)\nRequirement already satisfied: torchmetrics in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (1.7.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from pandas) (2025.1)\nRequirement already satisfied: scipy>=1.6.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: filelock in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (3.1.5)\nRequirement already satisfied: fsspec in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (2025.2.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torchvision) (11.1.0)\nRequirement already satisfied: PyYAML in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.9.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albumentations) (2.10.6)\nRequirement already satisfied: albucore==0.0.23 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albumentations) (0.0.23)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albumentations) (4.11.0.86)\nRequirement already satisfied: stringzilla>=3.10.4 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albucore==0.0.23->albumentations) (3.12.2)\nRequirement already satisfied: simsimd>=5.9.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albucore==0.0.23->albumentations) (6.2.1)\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from matplotlib) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torchmetrics) (0.14.2)\nRequirement already satisfied: setuptools in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.8.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1743749861594
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "print(torchvision.__version__)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.21.0+cu124\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1743749873083
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/anaconda/envs/Object_Detection/bin/python -m pip install --upgrade pip"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1743749873306
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/anaconda/envs/Object_Detection/bin/python -m pip install --upgrade numpy pandas scikit-learn torchvision"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: numpy in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (2.2.4)\nRequirement already satisfied: pandas in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (2.2.3)\nRequirement already satisfied: scikit-learn in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (1.6.1)\nRequirement already satisfied: torchvision in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (0.21.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from pandas) (2025.1)\nRequirement already satisfied: scipy>=1.6.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: torch==2.6.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torchvision) (2.6.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torchvision) (11.1.0)\nRequirement already satisfied: filelock in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (4.12.2)\nRequirement already satisfied: networkx in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.4.2)\nRequirement already satisfied: jinja2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.1.5)\nRequirement already satisfied: fsspec in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (2025.2.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/anaconda/envs/Object_Detection/bin/python -m pip install azureml.core"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: azureml.core in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (1.59.0.post1)\nRequirement already satisfied: pytz in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (2025.1)\nRequirement already satisfied: backports.tempfile in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (1.0)\nRequirement already satisfied: pathspec<1.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (0.12.1)\nRequirement already satisfied: requests<3.0.0,>=2.19.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml.core) (2.32.3)\nRequirement already satisfied: msal<2.0.0,>=1.15.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (1.31.1)\nRequirement already satisfied: msal-extensions<=2.0.0,>=0.3.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (1.2.0)\nRequirement already satisfied: knack<0.13.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (0.12.0)\nRequirement already satisfied: azure-core<2.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (1.32.0)\nRequirement already satisfied: pkginfo in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (1.12.1.2)\nRequirement already satisfied: argcomplete<4 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (3.5.3)\nRequirement already satisfied: humanfriendly<11.0,>=4.7 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (10.0)\nRequirement already satisfied: paramiko<4.0.0,>=2.0.8 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (3.5.1)\nRequirement already satisfied: azure-mgmt-resource<=24.0.0,>=15.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (23.3.0)\nRequirement already satisfied: azure-mgmt-containerregistry<11,>=8.2.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (10.3.0)\nRequirement already satisfied: azure-mgmt-storage<=22.0.0,>=16.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (22.0.0)\nRequirement already satisfied: azure-mgmt-keyvault<11.0.0,>=0.40.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (10.3.1)\nRequirement already satisfied: azure-mgmt-authorization<5,>=0.40.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (4.0.0)\nRequirement already satisfied: azure-mgmt-network<=29.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (28.1.0)\nRequirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (0.61.2)\nRequirement already satisfied: azure-common<2.0.0,>=1.1.12 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (1.1.28)\nRequirement already satisfied: msrest<=0.7.1,>=0.5.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (0.7.1)\nRequirement already satisfied: msrestazure<=0.7,>=0.4.33 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (0.6.4.post1)\nRequirement already satisfied: urllib3<3.0.0,>1.26.17 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (2.3.0)\nRequirement already satisfied: packaging<=25.0,>=20.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (24.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (2.9.0.post0)\nRequirement already satisfied: ndg-httpsclient<=0.5.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (0.5.1)\nRequirement already satisfied: SecretStorage<4.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (3.3.3)\nRequirement already satisfied: jsonpickle<5.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (4.0.2)\nRequirement already satisfied: contextlib2<22.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (21.6.0)\nRequirement already satisfied: docker<8.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (7.1.0)\nRequirement already satisfied: PyJWT<3.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (2.10.1)\nRequirement already satisfied: adal<=1.2.7,>=1.2.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (1.2.7)\nRequirement already satisfied: pyopenssl<25.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (24.3.0)\nRequirement already satisfied: jmespath<2.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml.core) (1.0.1)\nRequirement already satisfied: cryptography>=1.1.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from adal<=1.2.7,>=1.2.0->azureml.core) (44.0.2)\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azure-core<2.0.0->azureml.core) (1.16.0)\nRequirement already satisfied: typing-extensions>=4.6.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azure-core<2.0.0->azureml.core) (4.12.2)\nRequirement already satisfied: isodate<1.0.0,>=0.6.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azure-mgmt-authorization<5,>=0.40.0->azureml.core) (0.7.2)\nRequirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azure-mgmt-authorization<5,>=0.40.0->azureml.core) (1.5.0)\nRequirement already satisfied: pygments in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from knack<0.13.0->azureml.core) (2.15.1)\nRequirement already satisfied: pyyaml in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from knack<0.13.0->azureml.core) (6.0.2)\nRequirement already satisfied: tabulate in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from knack<0.13.0->azureml.core) (0.9.0)\nRequirement already satisfied: portalocker<3,>=1.4 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from msal-extensions<=2.0.0,>=0.3.0->azureml.core) (2.10.1)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from msrest<=0.7.1,>=0.5.1->azureml.core) (2025.1.31)\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from msrest<=0.7.1,>=0.5.1->azureml.core) (2.0.0)\nRequirement already satisfied: pyasn1>=0.1.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from ndg-httpsclient<=0.5.1->azureml.core) (0.6.1)\nRequirement already satisfied: bcrypt>=3.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from paramiko<4.0.0,>=2.0.8->azureml.core) (4.3.0)\nRequirement already satisfied: pynacl>=1.5 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from paramiko<4.0.0,>=2.0.8->azureml.core) (1.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.1->requests[socks]<3.0.0,>=2.19.1->azureml.core) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.1->requests[socks]<3.0.0,>=2.19.1->azureml.core) (3.10)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml.core) (1.7.1)\nRequirement already satisfied: jeepney>=0.6 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from SecretStorage<4.0.0->azureml.core) (0.9.0)\nRequirement already satisfied: backports.weakref in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from backports.tempfile->azureml.core) (1.0.post1)\nRequirement already satisfied: cffi>=1.12 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from cryptography>=1.1.0->adal<=1.2.7,>=1.2.0->azureml.core) (1.17.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from requests-oauthlib>=0.5.0->msrest<=0.7.1,>=0.5.1->azureml.core) (3.2.2)\nRequirement already satisfied: pycparser in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.1.0->adal<=1.2.7,>=1.2.0->azureml.core) (2.22)\n\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m"
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!/anaconda/envs/Object_Detection/bin/python -m pip install azureml-dataset-runtime --upgrade"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: azureml-dataset-runtime in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (1.59.0)\nRequirement already satisfied: azureml-dataprep<5.2.0a,>=5.1.0a in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml-dataset-runtime) (5.1.6)\nRequirement already satisfied: pyarrow>=0.17.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml-dataset-runtime) (19.0.1)\nCollecting numpy!=1.19.3,<1.24 (from azureml-dataset-runtime)\n  Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nRequirement already satisfied: azureml-dataprep-native<42.0.0,>=41.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (41.0.0)\nRequirement already satisfied: azureml-dataprep-rslex~=2.22.2dev0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (2.22.5)\nRequirement already satisfied: cloudpickle<3.0.0,>=1.1.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (2.2.1)\nRequirement already satisfied: azure-identity>=1.7.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (1.20.0)\nRequirement already satisfied: jsonschema in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (4.23.0)\nRequirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (6.0.2)\nRequirement already satisfied: azure-core>=1.31.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (1.32.0)\nRequirement already satisfied: cryptography>=2.5 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (44.0.2)\nRequirement already satisfied: msal>=1.30.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (1.31.1)\nRequirement already satisfied: msal-extensions>=1.2.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (1.2.0)\nRequirement already satisfied: typing-extensions>=4.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (4.12.2)\nRequirement already satisfied: attrs>=22.2.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (0.23.1)\nRequirement already satisfied: requests>=2.21.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azure-core>=1.31.0->azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (2.32.3)\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from azure-core>=1.31.0->azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (1.16.0)\nRequirement already satisfied: cffi>=1.12 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from cryptography>=2.5->azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (1.17.1)\nRequirement already satisfied: PyJWT<3,>=1.0.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (2.10.1)\nRequirement already satisfied: portalocker<3,>=1.4 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from msal-extensions>=1.2.0->azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (2.10.1)\nRequirement already satisfied: pycparser in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (2.22)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.7.0->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime) (2025.1.31)\nUsing cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.2.4\n    Uninstalling numpy-2.2.4:\n      Successfully uninstalled numpy-2.2.4\n\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nalbucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nalbumentations 2.0.4 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nnumba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 1.23.5 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.23.5\n"
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!/anaconda/envs/Object_Detection/bin/python -m pip install numpy=1.24.4\n",
        "!/anaconda/envs/Object_Detection/bin/python -m pip install albumentations==2.0.4"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[31mERROR: Invalid requirement: 'numpy=1.24.4': Expected end or semicolon (after name and no valid version specifier)\n    numpy=1.24.4\n         ^\nHint: = is not a valid operator. Did you mean == ?\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: albumentations==2.0.4 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (2.0.4)\nCollecting numpy>=1.24.4 (from albumentations==2.0.4)\n  Using cached numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\nRequirement already satisfied: scipy>=1.10.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albumentations==2.0.4) (1.15.2)\nRequirement already satisfied: PyYAML in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albumentations==2.0.4) (6.0.2)\nRequirement already satisfied: pydantic>=2.9.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albumentations==2.0.4) (2.10.6)\nRequirement already satisfied: albucore==0.0.23 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albumentations==2.0.4) (0.0.23)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albumentations==2.0.4) (4.11.0.86)\nRequirement already satisfied: stringzilla>=3.10.4 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albucore==0.0.23->albumentations==2.0.4) (3.12.2)\nRequirement already satisfied: simsimd>=5.9.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from albucore==0.0.23->albumentations==2.0.4) (6.2.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations==2.0.4) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations==2.0.4) (2.27.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations==2.0.4) (4.12.2)\nUsing cached numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Uninstalling numpy-1.23.5:\n      Successfully uninstalled numpy-1.23.5\n\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nazureml-dataset-runtime 1.59.0 requires numpy!=1.19.3,<1.24; sys_platform == \"linux\", but you have numpy 2.2.4 which is incompatible.\nnumba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 2.2.4 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.2.4\n"
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!/anaconda/envs/Object_Detection/bin/python -m pip install numpy --upgrade"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: numpy in /anaconda/envs/Object_Detection/lib/python3.10/site-packages (2.2.4)\n\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/anaconda/envs/Object_Detection/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m"
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#!/anaconda/envs/Object_Detection/bin/python -m pip uninstall numpy scipy -y\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1743749884382
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/anaconda/envs/Object_Detection/bin/python -m pip install numpy==2.1 scipy==1.11\n"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1743749884600
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Check if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
        "\n",
        "    \n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Install necessary libraries (if not already installed)\n",
        "#!pip install torch torchvision --quiet\n",
        "\n",
        "#save_directory = \"/home/azureuser/cloudfiles/code/Users/4006039\"\n",
        "\n",
        "save_directory = \"/home/azureuser/cloudfiles/code/Users/shahriarizadehfatima\"\n",
        "\n",
        "import os\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageOps\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import TwoMLPHead\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection.roi_heads import RoIHeads\n",
        "from torchvision.models.detection import _utils as det_utils\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import functional as F_transform\n",
        "from torchvision.transforms import Resize\n",
        "from torchvision.transforms import Normalize as NormalizeTransform\n",
        "\n",
        "from torchvision.ops.boxes import box_iou\n",
        "from torchvision.ops import boxes as box_ops\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "from albumentations import Compose, Resize, HorizontalFlip, RandomBrightnessContrast, Rotate, BboxParams, VerticalFlip, RandomRotate90, OneOf, ColorJitter, Normalize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import seaborn as sns\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "#from sklearn.model_selection import KFold\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import cv2\n",
        "\n",
        "from tqdm import tqdm  # Import tqdm for progress bar\n",
        "import threading"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using device: cuda\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/Object_Detection/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWlhDYdB8UJJ",
        "outputId": "fd071291-c586-4286-bc7f-bf48341a4840",
        "gather": {
          "logged": 1743749890639
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset, Datastore, Workspace\n",
        "\n",
        "json_dir = \"/home/azureuser/cloudfiles/code/Users/shahriarizadehfatima/config.json\"\n",
        "'''subscription_id = '171c9d75-6118-4b7a-bca6-f47fed8a4ece'\n",
        "resource_group = 'Test_Onedrive'\n",
        "workspace_name = \"Object_Detection\"'''\n",
        "\n",
        "# Define folders to exclude\n",
        "exclude_folders = {}\n",
        "# Load workspace from config.json\n",
        "ws = Workspace.from_config(json_dir)\n",
        "#fasteners_1\n",
        "#xxxxxxxxxxxx\n",
        "#ws = Workspace(subscription_id, resource_group, workspace_name)\n",
        "datastore = Datastore.get(ws, \"fasteners_1\")\n",
        "print(\"Available datastores:\", list(ws.datastores))\n",
        "\n",
        "#dataset_images\n",
        "dataset_images = Dataset.File.from_files(path=(datastore, 'images/'))\n",
        "#dataset_images = Dataset.File.from_files(path=(datastore, 'images_balanced/'))\n",
        "mounted_context_images = dataset_images.mount()\n",
        "mounted_context_images.start()\n",
        "# Get the mount path\n",
        "images_dir = mounted_context_images.mount_point\n",
        "#images_dir_folders = os.listdir(images_dir)\n",
        "images_dir_folders = [folder for folder in os.listdir(images_dir) if folder not in exclude_folders]\n",
        "print(\"Images Dataset mounted at:\", images_dir, images_dir_folders)\n",
        "\n",
        "#dataset_labels\n",
        "dataset_labels = Dataset.File.from_files(path=(datastore, 'labels/'))\n",
        "#dataset_labels = Dataset.File.from_files(path=(datastore, 'labels_balanced/'))\n",
        "mounted_context_labels = dataset_labels.mount()\n",
        "mounted_context_labels.start()\n",
        "# Get the mount path\n",
        "labels_dir = mounted_context_labels.mount_point\n",
        "#labels_dir_folders = os.listdir(labels_dir)\n",
        "labels_dir_folders = [folder for folder in os.listdir(labels_dir) if folder not in exclude_folders]\n",
        "print(\"Labels Dataset mounted at:\", labels_dir, labels_dir_folders)\n",
        "\n",
        "classes_file = labels_dir_folders\n",
        "print(f\"classes_file: {classes_file}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Available datastores: ['azureml_globaldatasets', 'fasteners_1', 'workspacefilestore', 'workspaceworkingdirectory', 'workspaceartifactstore', 'workspaceblobstore']\nImages Dataset mounted at: /tmp/tmpy9owx32p ['Cable Straps Edge Mount', 'Cable Straps Hole Mount', 'Tape-on Locators Edge Mount', 'Tape-on Locators Hole Mount']\nLabels Dataset mounted at: /tmp/tmphayze_bw ['Cable Straps Edge Mount', 'Cable Straps Hole Mount', 'Tape-on Locators Edge Mount', 'Tape-on Locators Hole Mount']\nclasses_file: ['Cable Straps Edge Mount', 'Cable Straps Hole Mount', 'Tape-on Locators Edge Mount', 'Tape-on Locators Hole Mount']\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1743749898096
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import albumentations as A\n",
        "\n",
        "\n",
        "#selected_classes = [\"Bolt\",\"Cable Straps Air Mount\", \"Cable Straps Edge Mount\", \"Cable Straps Hole Mount\"]\n",
        "images_dir_mounted = os.path.join(images_dir, 'images') # Adjust 'images' if your structure is different\n",
        "#print(images_dir_mounted)\n",
        "\n",
        "# Get the list of class names (subdirectories in images/)\n",
        "class_names = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n",
        "num_classes = len(class_names)+ 1\n",
        "#print(f\"{num_classes}: {num_classes}\")\n",
        "\n",
        "# Lists to store file paths for each split\n",
        "train_files = []\n",
        "test_files = []\n",
        "val_files = []\n",
        "\n",
        "# Split the data and store file paths\n",
        "for class_name in class_names:\n",
        "    image_class_dir = os.path.join(images_dir, class_name)\n",
        "    label_class_dir = os.path.join(labels_dir, class_name)\n",
        "    image_files = [f for f in os.listdir(image_class_dir)]\n",
        "    random.shuffle(image_files)\n",
        "\n",
        "    num_images = len(image_files)\n",
        "    train_split = int(train_ratio * num_images)\n",
        "    test_split = int(test_ratio * num_images)\n",
        "\n",
        "    train_image_files = image_files[:train_split]\n",
        "    test_image_files = image_files[train_split:train_split + test_split]\n",
        "    val_image_files = image_files[train_split + test_split:]\n",
        "    #val_image_files = image_files[train_split:]\n",
        "\n",
        "    for split, files in zip(['train', 'test', 'val'], [train_image_files, test_image_files, val_image_files]):\n",
        "    #for split, files in zip(['train', 'val'], [train_image_files, val_image_files]):\n",
        "        for file_name in files:\n",
        "            image_path = os.path.join(image_class_dir, file_name)\n",
        "            label_path = os.path.join(label_class_dir, file_name.replace(os.path.splitext(file_name)[1], '.txt'))\n",
        "            if split == 'train':\n",
        "                train_files.append((image_path, label_path, class_name))\n",
        "            elif split == 'test':\n",
        "                test_files.append((image_path, label_path, class_name))\n",
        "            elif split == 'val':\n",
        "                val_files.append((image_path, label_path, class_name))\n",
        "\n",
        "#print(f\"train files: {train_files}\")\n",
        "\n",
        "print(f\"len tarain: {len(train_files)}\")\n",
        "print(f\"len tarain: {len(val_files)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "len tarain: 275\nlen tarain: 77\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1743749898394
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class names\n",
        "def list_subfolders(main_folder):\n",
        "    if isinstance(main_folder, list):\n",
        "        return main_folder\n",
        "    if not os.path.isdir(main_folder):\n",
        "        raise ValueError(f\"The provided path '{main_folder}' is not a valid directory.\")\n",
        "    subfolders = [name for name in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, name))]\n",
        "    return subfolders\n",
        "\n",
        "bbox_params = BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"])\n",
        "\n",
        "def get_augmentations():\n",
        "    # Horizontal flipping with rotations\n",
        "    horizontal_rotations = [\n",
        "        Compose([HorizontalFlip()], bbox_params=bbox_params)\n",
        "        #Compose(Rotate(limit=(90, 90)), bbox_params=bbox_params),\n",
        "        #Compose(Rotate(limit=(270, 270)), bbox_params=bbox_params)\n",
        "    ]\n",
        "\n",
        "    # Vertical flipping with rotations\n",
        "    vertical_rotations = [\n",
        "        Compose([VerticalFlip()], bbox_params=bbox_params)\n",
        "    ]\n",
        "\n",
        "    # Combine all augmentations\n",
        "    augmentations = horizontal_rotations + vertical_rotations\n",
        "\n",
        "    return augmentations\n",
        "\n",
        "# Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_list, classes_file, image_size=(200, 200), augmentation = False):\n",
        "        self.file_list = file_list\n",
        "\n",
        "        self.classes = list_subfolders(classes_file)\n",
        "        #print(\"class_names:\", self.classes)\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        self.new_width, self.new_height = image_size\n",
        "        self.resize = Resize(height= self.new_height, width= self.new_width)\n",
        "\n",
        "        # Define normalization (ImageNet values)\n",
        "        self.normalize = NormalizeTransform(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        # Load augmentations\n",
        "        if augmentation is False:\n",
        "            self.augmentations = []\n",
        "        else:\n",
        "            self.augmentations = get_augmentations()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list) * (len(self.augmentations) + 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(\"index: \", idx)\n",
        "        # Determine which image and augmentation to use\n",
        "        img_idx = idx // (len(self.augmentations) + 1)\n",
        "        aug_idx = idx % (len(self.augmentations) + 1)\n",
        "\n",
        "        (img_path, label_path, _) = self.file_list[img_idx]\n",
        "\n",
        "        # Load image\n",
        "        #img_path = self.image_paths[img_idx]\n",
        "        imageee = Image.open(img_path).convert(\"RGB\")\n",
        "        img = np.array(imageee)  # Convert to NumPy array (for Albumentations)\n",
        "\n",
        "        resized = self.resize(image = img)\n",
        "        img = resized['image']  # Resize the image\n",
        "\n",
        "        # Load label\n",
        "        #label_path = self.label_paths[img_idx]\n",
        "\n",
        "        bboxes = []  # To store all bounding boxes\n",
        "        labels = []  # To store all class labels\n",
        "\n",
        "        with open(label_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                  if line.startswith(\"Class:\"):\n",
        "                    class_name = line.split(\":\")[1].strip().strip('\"')\n",
        "                    #print(f\"class_name: {class_name}\")\n",
        "\n",
        "                  elif line.startswith(\"Bounding Box:\"):\n",
        "                      bbox = eval(line.split(\":\")[1].strip())  # Convert string to list\n",
        "                      bboxes.append(bbox)\n",
        "                      labels.append(self.classes.index(class_name) + 1)  # Convert class name to index\n",
        "\n",
        "\n",
        "        # Scale bounding boxes\n",
        "        orig_width, orig_height = imageee.size\n",
        "        scale_x = self.new_width / orig_width\n",
        "        scale_y = self.new_height / orig_height\n",
        "        # Normalize bounding box coordinates\n",
        "        #bbox = [bbox[0] * scale_x, bbox[1] * scale_y, bbox[2] * scale_x, bbox[3] * scale_y]\n",
        "        bboxes = [[bbox[0] * scale_x, bbox[1] * scale_y, bbox[2] * scale_x, bbox[3] * scale_y] for bbox in bboxes]\n",
        "        #print(\"bbox_before: \", bbox)\n",
        "\n",
        "\n",
        "        # Added: If there are no bounding boxes, skip this sample.\n",
        "        if len(bboxes) == 0:\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "\n",
        "        if aug_idx == 0:\n",
        "            img_resized = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0  # Convert original image to tensor\n",
        "\n",
        "        # If it's not the original image (augmented), apply augmentation\n",
        "        if aug_idx != 0:\n",
        "            augmentation = self.augmentations[aug_idx - 1]  # -1 because first index is the original image\n",
        "            #augmented = augmentation(image=img, bboxes=[bbox], class_labels=[class_name])\n",
        "            augmented = augmentation(image=img, bboxes= bboxes, class_labels= labels)\n",
        "            img_augmented = augmented['image']\n",
        "            bboxes = augmented['bboxes']\n",
        "            labels = augmented['class_labels']\n",
        "            #bbox = augmented['bboxes'][0]\n",
        "            # Normalize the image to [0, 1] and convert to float32\n",
        "            img_retype = img_augmented.astype(np.float32) / 255.0  # Convert to float32 using NumPy\n",
        "            img_resized = torch.from_numpy(img_retype).permute(2, 0, 1)  # [C, H, W]\n",
        "\n",
        "        # Apply normalization\n",
        "        img_normalized = self.normalize(img_resized)\n",
        "        # Convert bbox and class to tensors\n",
        "        #bbox = torch.tensor(bbox, dtype=torch.float32).unsqueeze(0)  # Shape: [1, 4]\n",
        "        bboxes = torch.tensor(bboxes, dtype=torch.float32)  # Shape: [N, 4]\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)  # Shape: [N]\n",
        "\n",
        "        # Create target dictionary\n",
        "        #target = {\"boxes\": bbox, \"labels\": label}\n",
        "        target = {\"boxes\": bboxes, \"labels\": labels}\n",
        "\n",
        "        return idx, img_normalized, target\n",
        "\n",
        "\n",
        "# Save model\n",
        "def save_model(model, directory, model_name=\"custom_object_detection_model.pth\"):\n",
        "    #os.makedirs(save_directory, exist_ok=True)\n",
        "    save_path = os.path.join(save_directory, model_name)\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "    # Check if the file exists\n",
        "    if os.path.isfile(save_path):\n",
        "        print(f\"Model saved successfully to {save_path}\")\n",
        "    else:\n",
        "        print(f\"Error: Model was not saved to {save_path}\")\n",
        "\n",
        "\n",
        "transform = Compose(\n",
        "    [\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=BboxParams(\n",
        "        format=\"coco\",  # Specify COCO-style bounding box format\n",
        "        label_fields=[\"class_labels\"],  # Ensure transformations can handle class labels\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "def denormalize(tensor, mean, std):\n",
        "    # Convert mean and std to tensors and reshape to match the image tensor dimensions.\n",
        "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "    std = torch.tensor(std).view(-1, 1, 1)\n",
        "    # Invert the normalization\n",
        "    tensor = tensor * std + mean\n",
        "    return tensor\n",
        "\n",
        "# Example usage:\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "def custom_collate(batch):\n",
        "    images, targets = zip(*batch)  # unzip the list of tuples\n",
        "    # Stack images since they are all the same size:\n",
        "    images = torch.stack(images, 0)\n",
        "    # Return targets as a list (to handle variable-length target tensors)\n",
        "    return images, list(targets)\n",
        "\n",
        "\n",
        "#load_model\n",
        "def load_model(model, directory, model_name):\n",
        "    load_path = os.path.join(directory, model_name)\n",
        "    model.load_state_dict(torch.load(load_path))\n",
        "    #model.eval()  # Set model to evaluation mode\n",
        "    print(f\"Model loaded from {load_path}\")\n",
        "    return model"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/Object_Detection/lib/python3.10/site-packages/albumentations/core/composition.py:250: UserWarning: Got processor for bboxes, but no transform to process it.\n  self._set_keys()\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "id": "9HSKYqDx9Ly3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b345d14-53a8-4768-a020-257d1434c02c",
        "gather": {
          "logged": 1743749898641
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset instances\n",
        "train_dataset = CustomDataset(file_list = train_files, classes_file=  classes_file , augmentation= False, image_size=(200,200))\n",
        "test_dataset = CustomDataset(file_list= test_files, classes_file=  classes_file)\n",
        "val_dataset = CustomDataset(file_list= val_files, classes_file=  classes_file)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers= num_workers,collate_fn=lambda batch: tuple(zip(*batch)))\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda batch: tuple(zip(*batch)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers= num_workers, collate_fn=lambda batch: tuple(zip(*batch)))\n",
        "\n",
        "print(f\"Number of training images: {len(train_dataset)}\")\n",
        "print(f\"Number of validation images: {len(val_dataset)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of training images: 275\nNumber of validation images: 77\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1743749898904
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the backbone\n",
        "backbone_1 = resnet_fpn_backbone(\"resnet50\", pretrained= Pretrained_status) # or pretrained=True if you want a pretrained backbone\n",
        "backbone_2 = resnet_fpn_backbone(\"resnet101\", pretrained= Pretrained_status) # or pretrained=True if you want a pretrained backbone\n",
        "backbone_3 = resnet_fpn_backbone(\"resnet34\", pretrained= Pretrained_status) # or pretrained=True if you want a pretrained backbone\n",
        "\n",
        "backbone = [backbone_1, backbone_2, backbone_3]\n",
        "\n",
        "# Define custom anchor sizes and aspect ratios\n",
        "anchor_sizes = ((32,), (64,), (128,),(140,),(200,)) # Sizes for each FPN level\n",
        "aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)  # Aspect ratios for all levels\n",
        "# Create a custom AnchorGenerator\n",
        "custom_anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/Object_Detection/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n  warnings.warn(\n/anaconda/envs/Object_Detection/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/anaconda/envs/Object_Detection/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/anaconda/envs/Object_Detection/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/anaconda/envs/Object_Detection/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1743749906026
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#model_path_list = [\"1000201_.pth\", \"1000202_.pth\", \"1000203_.pth\", \"1000204_.pth\", \"1000205_.pth\", \"1000206_.pth\", \"1000207_.pth\"]\n",
        "#model_path_list = [\"1000201_.pth\", \"1000202_.pth\", \"1000203_.pth\", \"1000204_.pth\", \"1000205_.pth\", \"1000206_.pth\", \"1000207_.pth\", \"1000208_.pth\"]\n",
        "model_path_list = [\"1000204_.pth\", \"1000206_.pth\", \"1000202_.pth\", \"1000203_.pth\", \"1000201_.pth\", \"1000205_.pth\", \"1000207_.pth\", \"1000208_.pth\"]\n",
        "#model_path_list = [\"1000202_.pth\", \"1000204_.pth\", \"1000206_.pth\"]\n",
        "#model_path_list = [ \"1000208_.pth\", \"1000207_.pth\", \"1000205_.pth\",  \"1000203_.pth\", \"1000201_.pth\",  \"1000202_.pth\",  \"1000206_.pth\", \"1000204_.pth\"]\n",
        "\n",
        "\n",
        "# Load the pretrained model\n",
        "def making_models_list(backbone_list, model_path_list, num_classes, pred_per_image, device, custom_anchor_generator):\n",
        "    model_list = []\n",
        "    backbones_list = []\n",
        "    \n",
        "    for path in model_path_list:   \n",
        "        if path in [\"1000203_.pth\"]: #ResNet101\n",
        "            backbone = backbone_list[1]\n",
        "            backbones_list.append(\"ResNet101\")\n",
        "        elif path in [\"1000202_.pth\"]:  #ResNet34\n",
        "            backbone = backbone_list[2]\n",
        "            backbones_list.append(\"ResNet34\")\n",
        "        else:                            #ResNet50\n",
        "            backbone = backbone_list[0]\n",
        "            backbones_list.append(\"ResNet50\")\n",
        "        checkpoint = torch.load(path, map_location = device)\n",
        "        model = torchvision.models.detection.FasterRCNN(backbone,\n",
        "                                                        num_classes= num_classes,\n",
        "                                                        #class_weights=class_weights,\n",
        "                                                        rpn_pre_nms_top_n_train=100,\n",
        "                                                        rpn_post_nms_top_n_train=100,\n",
        "                                                        rpn_nms_thresh= 0.5, #Lower (e.g., 0.3): Stricter filtering (removes more overlapping boxes),\n",
        "                                                        #Higher (e.g., 0.6): Allows more overlapping boxes (useful if objects are close together).\n",
        "                                                        rpn_score_thresh=0.4,\n",
        "                                                        rpn_anchor_generator= custom_anchor_generator,\n",
        "                                                        box_fg_iou_thresh= 0.5, # Positive threshold,\n",
        "                                                        box_bg_iou_thresh= 0.5) # Negative threshold)\n",
        "        #rpn_score_thresh (float): only return proposals with an objectness score greater than rpn_score_thresh default 0.5\n",
        "        #rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals default 0.7 #rpn_pre_nms_top_n_train=  Number of proposals before NMS\n",
        "        #rpn_post_nms_top_n_train=  Number of proposals after NMS #rpn_pre_nms_top_n_test= Pre-NMS top proposals during inference\n",
        "        #rpn_post_nms_top_n_test= Post-NMS top proposals during inference\n",
        "\n",
        "        representation_size = model.roi_heads.box_predictor.cls_score.in_features\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictor(representation_size, num_classes)\n",
        "        model.roi_heads.detections_per_img = pred_per_image  # Reduce number of false positives\n",
        "        #print(model.roi_heads)\n",
        "        #print(model.roi_heads.__dict__.keys())\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
        "        model_list.append(model)\n",
        "    return model_list, backbone_list\n"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1743749906267
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models, backbones = making_models_list(backbone_list=backbone, model_path_list= model_path_list, num_classes= num_classes,\n",
        "                                                pred_per_image =pred_per_image, device=device,\n",
        "                                                custom_anchor_generator = custom_anchor_generator)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1743749947732
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "weights = {str(i): 1/len(train_dataset) for i in range(len(train_dataset))}\n",
        "# Create a sampler based on the weights\n",
        "\n",
        "def compute_weighted_error(model, sample_weights, dataset, device):\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    weights_list = [sample_weights[str(i)] for i in range(len(dataset))]\n",
        "\n",
        "    sampler = WeightedRandomSampler(weights_list, num_samples=len(weights), replacement=True)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers= num_workers,\n",
        "                                                sampler= sampler, collate_fn=lambda batch: tuple(zip(*batch)))\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    update_status= {}\n",
        "    for idxs, images, targets in dataloader:\n",
        "        idx = [i for i in idxs]\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        with torch.no_grad():\n",
        "            for k in range(len(images)):\n",
        "                loss_dict = model(images[k].unsqueeze(0), [targets[k]])\n",
        "                loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "\n",
        "                if 'loss_classifier' in loss_dict and 'loss_box_reg' in loss_dict:\n",
        "                    if loss_dict['loss_classifier'] < 0.05 and loss_dict['loss_box_reg'] < 0.05:\n",
        "                        update_status[str(idx[k])] = 'NO'\n",
        "                        print(f\" id : {str(idx[k])} value: {update_status[str(idx[k])]}\")\n",
        "                    else:\n",
        "                        update_status[str(idx[k])] = 'YES'\n",
        "                        total_loss += loss * weights[str(idx[k])]\n",
        "                        print(f\" id : {str(idx[k])} value: {update_status[str(idx[k])]}\")\n",
        "                elif isinstance(loss_dict, list):\n",
        "                    total_classifier_loss = sum(d.get('loss_classifier', 0) for d in loss_dict)\n",
        "                    total_box_reg_loss = sum(d.get('loss_box_reg', 0) for d in loss_dict)\n",
        "                    if total_classifier_loss <= 0.05 and total_box_reg_loss < 0.05:\n",
        "                            update_status[str(idx[k])] = 'NO'\n",
        "                            print(f\" id : {str(idx[k])} value: {update_status[str(idx[k])]}\")\n",
        "                    else:\n",
        "                            update_status[str(idx[k])] = 'YES'\n",
        "                            print(f\" id : {str(idx[k])} value: {update_status[str(idx[k])]}\")\n",
        "                            total_loss += loss * weights[str(idx[k])]\n",
        "\n",
        "    error = total_loss / len(dataloader.dataset)\n",
        "    print(\"Model training Done!\")\n",
        "    return error, update_status\n",
        "\n",
        "def adaboost_faster_rcnn(models, dataset, sample_weights, device):\n",
        "    model_weights = []\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        print(\"Start updating weights!\")\n",
        "        error, update_status = compute_weighted_error(model=model, sample_weights = sample_weights,\n",
        "                                                                                dataset=dataset,  device=device)\n",
        "        if error == 0:\n",
        "            alpha = 1  # Strong classifier\n",
        "        else:\n",
        "            alpha = 0.5 * math.log((1 - error) / error)\n",
        "\n",
        "        model_weights.append(alpha)\n",
        "\n",
        "        for i in sample_weights.keys():\n",
        "            if i in update_status.keys():\n",
        "                if update_status[i] == \"NO\":  # Correctly referencing dictionary keys\n",
        "                    sample_weights[i] *= math.exp(-alpha)\n",
        "                else:\n",
        "                    sample_weights[i] *= math.exp(alpha)\n",
        "\n",
        "        # Normalize sample weights\n",
        "        total_weight = np.sum(list(sample_weights.values()))  # Convert to list for sum\n",
        "        if total_weight > 0:\n",
        "            sample_weights = {k: v / total_weight for k, v in sample_weights.items()}\n",
        "    \n",
        "    return model_weights, sample_weights\n",
        "\n",
        "def weighted_nms(predictions, model_weights, iou_threshold=0.5):\n",
        "    batch_size = len(predictions[0])  # Get batch size\n",
        "    final_preds = []\n",
        "\n",
        "    for batch_idx in range(batch_size):\n",
        "        combined_preds = []\n",
        "\n",
        "        for preds, weight in zip(predictions, model_weights):\n",
        "            for box, score, label in zip(\n",
        "                preds[batch_idx]['boxes'], preds[batch_idx]['scores'], preds[batch_idx]['labels']\n",
        "            ):\n",
        "                combined_preds.append((box, score * weight, label))\n",
        "\n",
        "        combined_preds.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        filtered_preds = []\n",
        "        while combined_preds:\n",
        "            best = combined_preds.pop(0)\n",
        "            filtered_preds.append(best)\n",
        "\n",
        "            combined_preds = [p for p in combined_preds if iou(p[0], best[0]) < iou_threshold]\n",
        "\n",
        "        # Convert back to the required format\n",
        "        if filtered_preds:\n",
        "            final_preds.append({\n",
        "                \"boxes\": torch.stack([p[0] for p in filtered_preds]),\n",
        "                \"scores\": torch.tensor([p[1] for p in filtered_preds]),\n",
        "                \"labels\": torch.tensor([p[2] for p in filtered_preds])\n",
        "            })\n",
        "        else:\n",
        "            # Handle empty case\n",
        "            final_preds.append({\"boxes\": torch.tensor([]), \"scores\": torch.tensor([]), \"labels\": torch.tensor([])})\n",
        "\n",
        "    return final_preds\n",
        "\n",
        "def iou(box1, box2):\n",
        "    # Assuming box1 and box2 are in (x_min, y_min, x_max, y_max) format\n",
        "    # Convert them to the format expected by box_iou: (N, 4) and (M, 4)\n",
        "    # For single boxes, reshape them to (1, 4)\n",
        "    box1_tensor = torch.tensor(box1).unsqueeze(0)\n",
        "    box2_tensor = torch.tensor(box2).unsqueeze(0)\n",
        "    return box_iou(box1_tensor, box2_tensor).item() # Returns a single IoU value\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1743749947965
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply AdaBoost\n",
        "model_weights , sample_weights = adaboost_faster_rcnn(models= models, dataset = train_dataset,\n",
        "                                                                sample_weights =weights, device=device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_3146/2660485587.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " id : 20 value: YES\n id : 183 value: YES\n id : 55 value: YES\n id : 39 value: YES\n id : 165 value: YES\n id : 97 value: YES\n id : 21 value: YES\n id : 178 value: YES\n id : 222 value: YES\n id : 116 value: YES\n id : 240 value: YES\n id : 261 value: YES\n id : 192 value: YES\n id : 203 value: YES\n id : 257 value: YES\n id : 217 value: YES\n id : 94 value: YES\n id : 45 value: YES\n id : 98 value: YES\n id : 6 value: YES\n id : 64 value: YES\n id : 27 value: YES\n id : 199 value: YES\n id : 28 value: YES\n id : 230 value: YES\n id : 188 value: YES\n id : 218 value: YES\n id : 168 value: YES\n id : 152 value: YES\n id : 61 value: YES\n id : 3 value: YES\n id : 184 value: YES\n id : 220 value: YES\n id : 105 value: YES\n id : 102 value: YES\n id : 31 value: YES\n id : 169 value: YES\n id : 2 value: YES\n id : 172 value: YES\n id : 19 value: YES\n id : 272 value: YES\n id : 86 value: YES\n id : 187 value: YES\n id : 68 value: YES\n id : 208 value: YES\n id : 80 value: YES\n id : 139 value: YES\n id : 30 value: YES\n id : 192 value: YES\n id : 75 value: YES\n id : 30 value: YES\n id : 239 value: YES\n id : 104 value: YES\n id : 129 value: YES\n id : 110 value: YES\n id : 231 value: YES\n id : 102 value: YES\n id : 202 value: YES\n id : 64 value: YES\n id : 116 value: YES\n id : 210 value: YES\n id : 169 value: YES\n id : 255 value: YES\n id : 191 value: YES\n id : 19 value: YES\n id : 271 value: YES\n id : 124 value: YES\n id : 231 value: YES\n id : 191 value: YES\n id : 230 value: YES\n id : 64 value: YES\n id : 238 value: YES\n id : 225 value: YES\n id : 124 value: YES\n id : 123 value: YES\n id : 263 value: YES\n id : 55 value: YES\n id : 44 value: YES\n id : 231 value: YES\n id : 258 value: YES\n id : 119 value: YES\n id : 30 value: YES\n id : 152 value: YES\n id : 153 value: YES\n id : 262 value: YES\n id : 119 value: YES\n id : 253 value: YES\n id : 77 value: YES\n id : 46 value: YES\n id : 116 value: YES\n id : 197 value: YES\n id : 231 value: YES\n id : 253 value: YES\n id : 174 value: YES\n id : 226 value: YES\n id : 74 value: YES\n id : 51 value: YES\n id : 61 value: YES\n id : 159 value: YES\n id : 168 value: YES\n id : 55 value: YES\n id : 105 value: YES\n id : 191 value: YES\n id : 59 value: YES\n id : 206 value: YES\n id : 65 value: YES\n id : 219 value: YES\n id : 11 value: YES\n id : 18 value: YES\n id : 122 value: YES\n id : 133 value: YES\n id : 1 value: YES\n id : 157 value: YES\n id : 84 value: YES\n id : 27 value: YES\n id : 257 value: YES\n id : 54 value: YES\n id : 127 value: YES\n id : 51 value: YES\n id : 107 value: YES\n id : 272 value: YES\n id : 251 value: YES\n id : 176 value: YES\n id : 94 value: YES\n id : 264 value: YES\n id : 157 value: YES\n id : 45 value: YES\n id : 224 value: YES\n id : 222 value: YES\n id : 52 value: YES\n id : 97 value: YES\n id : 259 value: YES\n id : 179 value: YES\n id : 106 value: YES\n id : 163 value: YES\n id : 197 value: YES\n id : 65 value: YES\n id : 239 value: YES\n id : 258 value: YES\n id : 140 value: YES\n id : 221 value: YES\n id : 211 value: YES\n id : 69 value: YES\n id : 100 value: YES\n id : 145 value: YES\n id : 261 value: YES\n id : 186 value: YES\n id : 90 value: YES\n id : 134 value: YES\n id : 75 value: YES\n id : 195 value: YES\n id : 183 value: YES\n id : 2 value: YES\n id : 204 value: YES\n id : 69 value: YES\n id : 87 value: YES\n id : 94 value: YES\n id : 183 value: YES\n id : 169 value: YES\n id : 151 value: YES\n id : 16 value: YES\n id : 233 value: YES\n id : 257 value: YES\n id : 27 value: YES\n id : 165 value: YES\n id : 21 value: YES\n id : 24 value: YES\n id : 163 value: YES\n id : 18 value: YES\n id : 75 value: YES\n id : 127 value: YES\n id : 82 value: YES\n id : 115 value: YES\n id : 227 value: YES\n id : 233 value: YES\n id : 190 value: YES\n id : 95 value: YES\n id : 189 value: YES\n id : 264 value: YES\n id : 41 value: YES\n id : 53 value: YES\n id : 94 value: YES\n id : 235 value: YES\n id : 185 value: YES\n id : 72 value: YES\n id : 208 value: YES\n id : 150 value: YES\n id : 233 value: YES\n id : 107 value: YES\n id : 228 value: YES\n id : 209 value: YES\n id : 237 value: YES\n id : 27 value: YES\n id : 248 value: YES\n id : 30 value: YES\n id : 38 value: YES\n id : 146 value: YES\n id : 196 value: YES\n id : 134 value: YES\n id : 160 value: YES\n id : 217 value: YES\n id : 215 value: YES\n id : 147 value: YES\n id : 266 value: YES\n id : 43 value: YES\n id : 81 value: YES\n id : 70 value: YES\n id : 236 value: YES\n id : 208 value: YES\n id : 137 value: YES\n id : 11 value: YES\n id : 112 value: YES\n id : 64 value: YES\n id : 172 value: YES\n id : 52 value: YES\n id : 91 value: YES\n id : 151 value: YES\n id : 22 value: YES\n id : 6 value: YES\n id : 213 value: YES\n id : 274 value: YES\n id : 111 value: YES\n id : 220 value: YES\n id : 108 value: YES\n id : 36 value: YES\n id : 69 value: YES\n id : 58 value: YES\n id : 208 value: YES\n id : 123 value: YES\n id : 2 value: YES\n id : 213 value: YES\n id : 192 value: YES\n id : 4 value: YES\n id : 80 value: YES\n id : 156 value: YES\n id : 23 value: YES\n id : 98 value: YES\n id : 79 value: YES\n id : 251 value: YES\n id : 128 value: YES\n id : 73 value: YES\n id : 59 value: YES\n id : 248 value: YES\n id : 202 value: YES\n id : 64 value: YES\n id : 222 value: YES\n id : 10 value: YES\n id : 141 value: YES\n id : 88 value: YES\n id : 70 value: YES\n id : 64 value: YES\n id : 13 value: YES\n id : 79 value: YES\n id : 122 value: YES\n id : 77 value: YES\n id : 39 value: YES\n id : 21 value: YES\n id : 119 value: YES\n id : 47 value: YES\n id : 150 value: YES\n id : 157 value: YES\n id : 68 value: YES\n id : 136 value: YES\n id : 209 value: YES\n id : 16 value: YES\n id : 177 value: YES\n id : 172 value: YES\n id : 190 value: YES\n id : 186 value: YES\n id : 239 value: YES\n id : 23 value: YES\n id : 260 value: YES\n id : 263 value: YES\n id : 229 value: YES\n id : 225 value: YES\nModel training Done!\nStart updating weights!\n id : 192 value: YES\n id : 87 value: YES\n id : 3 value: YES\n id : 229 value: YES\n id : 107 value: YES\n id : 190 value: YES\n id : 202 value: YES\n id : 23 value: YES\n id : 164 value: YES\n id : 196 value: YES\n id : 208 value: YES\n id : 255 value: YES\n id : 124 value: YES\n id : 86 value: YES\n id : 88 value: YES\n id : 224 value: YES\n id : 222 value: YES\n id : 253 value: YES\n id : 128 value: YES\n id : 271 value: YES\n id : 185 value: YES\n id : 160 value: YES\n id : 272 value: YES\n id : 220 value: YES\n id : 177 value: YES\n id : 239 value: YES\n id : 208 value: YES\n id : 137 value: YES\n id : 226 value: YES\n id : 24 value: YES\n id : 21 value: YES\n id : 174 value: YES\n id : 238 value: YES\n id : 199 value: YES\n id : 163 value: YES\n id : 123 value: YES\n id : 169 value: YES\n id : 75 value: YES\n id : 52 value: YES\n id : 74 value: YES\n id : 272 value: YES\n id : 28 value: YES\n id : 10 value: YES\n id : 147 value: YES\n id : 2 value: YES\n id : 140 value: YES\n id : 115 value: YES\n id : 204 value: YES\n id : 94 value: YES\n id : 263 value: YES\n id : 53 value: YES\n id : 209 value: YES\n id : 263 value: YES\n id : 253 value: YES\n id : 47 value: YES\n id : 30 value: YES\n id : 105 value: YES\n id : 147 value: YES\n id : 188 value: YES\n id : 145 value: YES\n id : 64 value: YES\n id : 110 value: YES\n id : 44 value: YES\n id : 27 value: YES\n id : 98 value: YES\n id : 236 value: YES\n id : 264 value: YES\n id : 22 value: YES\n id : 192 value: YES\n id : 86 value: YES\n id : 75 value: YES\n id : 84 value: YES\n id : 220 value: YES\n id : 253 value: YES\n id : 68 value: YES\n id : 259 value: YES\n id : 151 value: YES\n id : 82 value: YES\n id : 259 value: YES\n id : 90 value: YES\n id : 199 value: YES\n id : 239 value: YES\n id : 185 value: YES\n id : 264 value: YES\n id : 53 value: YES\n id : 224 value: YES\n id : 107 value: YES\n id : 221 value: YES\n id : 10 value: YES\n id : 168 value: YES\n id : 82 value: YES\n id : 233 value: YES\n id : 145 value: YES\n id : 14 value: YES\n id : 177 value: YES\n id : 44 value: YES\n id : 157 value: YES\n id : 235 value: YES\n id : 156 value: YES\n id : 69 value: YES\n id : 4 value: YES\n id : 88 value: YES\n id : 105 value: YES\n id : 51 value: YES\n id : 184 value: YES\n id : 231 value: YES\n id : 229 value: YES\n id : 203 value: YES\n id : 102 value: YES\n id : 170 value: YES\n id : 222 value: YES\n id : 44 value: YES\n id : 90 value: YES\n id : 145 value: YES\n id : 243 value: YES\n id : 248 value: YES\n id : 174 value: YES\n id : 105 value: YES\n id : 24 value: YES\n id : 59 value: YES\n id : 132 value: YES\n id : 191 value: YES\n id : 157 value: YES\n id : 215 value: YES\n id : 24 value: YES\n id : 88 value: YES\n id : 70 value: YES\n id : 271 value: YES\n id : 255 value: YES\n id : 81 value: YES\n id : 141 value: YES\n id : 24 value: YES\n id : 100 value: YES\n id : 174 value: YES\n id : 226 value: YES\n id : 236 value: YES\n id : 187 value: YES\n id : 87 value: YES\n id : 267 value: YES\n id : 28 value: YES\n id : 82 value: YES\n id : 21 value: YES\n id : 86 value: YES\n id : 220 value: YES\n id : 41 value: YES\n id : 202 value: YES\n id : 179 value: YES\n id : 178 value: YES\n id : 36 value: YES\n id : 274 value: YES\n id : 51 value: YES\n id : 262 value: YES\n id : 49 value: YES\n id : 116 value: YES\n id : 61 value: YES\n id : 227 value: YES\n id : 264 value: YES\n id : 64 value: YES\n id : 190 value: YES\n id : 232 value: YES\n id : 220 value: YES\n id : 213 value: YES\n id : 19 value: YES\n id : 111 value: YES\n id : 183 value: YES\n id : 69 value: YES\n id : 196 value: YES\n id : 129 value: YES\n id : 156 value: YES\n id : 206 value: YES\n id : 264 value: YES\n id : 95 value: YES\n id : 204 value: YES\n id : 156 value: YES\n id : 203 value: YES\n id : 189 value: YES\n id : 262 value: YES\n id : 141 value: YES\n id : 197 value: YES\n id : 260 value: YES\n id : 151 value: YES\n id : 226 value: YES\n id : 91 value: YES\n id : 75 value: YES\n id : 253 value: YES\n id : 43 value: YES\n id : 217 value: YES\n id : 128 value: YES\n id : 84 value: YES\n id : 262 value: YES\n id : 188 value: YES\n id : 224 value: YES\n id : 45 value: YES\n id : 163 value: YES\n id : 91 value: YES\n id : 124 value: YES\n id : 221 value: YES\n id : 107 value: YES\n id : 87 value: YES\n id : 184 value: YES\n id : 165 value: YES\n id : 2 value: YES\n id : 160 value: YES\n id : 100 value: YES\n id : 263 value: YES\n id : 23 value: YES\n id : 106 value: YES\n id : 51 value: YES\n id : 59 value: YES\n id : 39 value: YES\n id : 116 value: YES\n id : 228 value: YES\n id : 39 value: YES\n id : 239 value: YES\n id : 259 value: YES\n id : 226 value: YES\n id : 165 value: YES\n id : 187 value: YES\n id : 23 value: YES\n id : 6 value: YES\n id : 13 value: YES\n id : 141 value: YES\n id : 233 value: YES\n id : 4 value: YES\n id : 185 value: YES\n id : 87 value: YES\n id : 262 value: YES\n id : 240 value: YES\n id : 4 value: YES\n id : 54 value: YES\n id : 2 value: YES\n id : 172 value: YES\n id : 226 value: YES\n id : 81 value: YES\n id : 19 value: YES\n id : 105 value: YES\n id : 233 value: YES\n id : 163 value: YES\n id : 128 value: YES\n id : 188 value: YES\n id : 46 value: YES\n id : 55 value: YES\n id : 122 value: YES\n id : 72 value: YES\n id : 28 value: YES\n id : 215 value: YES\n id : 69 value: YES\n id : 228 value: YES\n id : 253 value: YES\n id : 177 value: YES\n id : 225 value: YES\n id : 274 value: YES\n id : 111 value: YES\n id : 177 value: YES\n id : 38 value: YES\n id : 221 value: YES\n id : 3 value: YES\n id : 54 value: YES\n id : 211 value: YES\n id : 89 value: YES\n id : 209 value: YES\n id : 217 value: YES\n id : 221 value: YES\n id : 105 value: YES\n id : 22 value: YES\n id : 169 value: YES\n id : 218 value: YES\n id : 79 value: YES\n id : 44 value: YES\n id : 235 value: YES\n id : 105 value: YES\n id : 238 value: YES\n id : 142 value: YES\n id : 68 value: YES\n id : 153 value: YES\nModel training Done!\nStart updating weights!\n id : 47 value: YES\n id : 52 value: YES\n id : 225 value: YES\n id : 163 value: YES\n id : 36 value: YES\n id : 36 value: YES\n id : 272 value: YES\n id : 195 value: YES\n id : 274 value: YES\n id : 39 value: YES\n id : 165 value: YES\n id : 94 value: YES\n id : 72 value: YES\n id : 45 value: YES\n id : 168 value: YES\n id : 51 value: YES\n id : 64 value: YES\n id : 54 value: YES\n id : 100 value: YES\n id : 30 value: YES\n id : 137 value: YES\n id : 28 value: YES\n id : 188 value: YES\n id : 4 value: YES\n id : 112 value: YES\n id : 94 value: YES\n id : 55 value: YES\n id : 27 value: YES\n id : 95 value: YES\n id : 255 value: YES\n id : 165 value: YES\n id : 80 value: YES\n id : 98 value: YES\n id : 91 value: YES\n id : 123 value: YES\n id : 209 value: YES\n id : 54 value: YES\n id : 179 value: YES\n id : 10 value: YES\n id : 21 value: YES\n id : 184 value: YES\n id : 174 value: YES\n id : 52 value: YES\n id : 217 value: YES\n id : 199 value: YES\n id : 54 value: YES\n id : 156 value: YES\n id : 188 value: YES\n id : 271 value: YES\n id : 105 value: YES\n id : 199 value: YES\n id : 208 value: YES\n id : 211 value: YES\n id : 13 value: YES\n id : 13 value: YES\n id : 6 value: YES\n id : 36 value: YES\n id : 52 value: YES\n id : 74 value: YES\n id : 178 value: YES\n id : 122 value: YES\n id : 271 value: YES\n id : 199 value: YES\n id : 91 value: YES\n id : 199 value: YES\n id : 107 value: YES\n id : 19 value: YES\n id : 111 value: YES\n id : 86 value: YES\n id : 203 value: YES\n id : 61 value: YES\n id : 41 value: YES\n id : 206 value: YES\n id : 59 value: YES\n id : 229 value: YES\n id : 79 value: YES\n id : 189 value: YES\n id : 251 value: YES\n id : 16 value: YES\n id : 213 value: YES\n id : 228 value: YES\n id : 235 value: YES\n id : 168 value: YES\n id : 179 value: YES\n id : 226 value: YES\n id : 3 value: YES\n id : 206 value: YES\n id : 68 value: YES\n id : 197 value: YES\n id : 6 value: YES\n id : 157 value: YES\n id : 8 value: YES\n id : 217 value: YES\n id : 13 value: YES\n id : 221 value: YES\n id : 30 value: YES\n id : 145 value: YES\n id : 128 value: YES\n id : 272 value: YES\n id : 87 value: YES\n id : 151 value: YES\n id : 22 value: YES\n id : 115 value: YES\n id : 73 value: YES\n id : 208 value: YES\n id : 129 value: YES\n id : 65 value: YES\n id : 98 value: YES\n id : 206 value: YES\n id : 235 value: YES\n id : 84 value: YES\n id : 240 value: YES\n id : 115 value: YES\n id : 208 value: YES\n id : 178 value: YES\n id : 160 value: YES\n id : 74 value: YES\n id : 87 value: YES\n id : 82 value: YES\n id : 40 value: YES\n id : 206 value: YES\n id : 111 value: YES\n id : 41 value: YES\n id : 94 value: YES\n id : 90 value: YES\n id : 160 value: YES\n id : 115 value: YES\n id : 215 value: YES\n id : 105 value: YES\n id : 156 value: YES\n id : 177 value: YES\n id : 260 value: YES\n id : 2 value: YES\n id : 196 value: YES\n id : 188 value: YES\n id : 10 value: YES\n id : 220 value: YES\n id : 233 value: YES\n id : 4 value: YES\n id : 38 value: YES\n id : 28 value: YES\n id : 145 value: YES\n id : 188 value: YES\n id : 257 value: YES\n id : 253 value: YES\n id : 163 value: YES\n id : 257 value: YES\n id : 43 value: YES\n id : 231 value: YES\n id : 271 value: YES\n id : 141 value: YES\n id : 226 value: YES\n id : 225 value: YES\n id : 140 value: YES\n id : 184 value: YES\n id : 235 value: YES\n id : 74 value: YES\n id : 216 value: YES\n id : 145 value: YES\n id : 208 value: YES\n id : 21 value: YES\n id : 204 value: YES\n id : 47 value: YES\n id : 30 value: YES\n id : 98 value: YES\n id : 22 value: YES\n id : 108 value: YES\n id : 163 value: YES\n id : 191 value: YES\n id : 170 value: YES\n id : 87 value: YES\n id : 6 value: YES\n id : 74 value: YES\n id : 124 value: YES\n id : 75 value: YES\n id : 28 value: YES\n id : 38 value: YES\n id : 102 value: YES\n id : 208 value: YES\n id : 174 value: YES\n id : 75 value: YES\n id : 30 value: YES\n id : 238 value: YES\n id : 260 value: YES\n id : 235 value: YES\n id : 183 value: YES\n id : 55 value: YES\n id : 82 value: YES\n id : 184 value: YES\n id : 236 value: YES\n id : 102 value: YES\n id : 238 value: YES\n id : 185 value: YES\n id : 74 value: YES\n id : 110 value: YES\n id : 196 value: YES\n id : 260 value: YES\n id : 272 value: YES\n id : 243 value: YES\n id : 19 value: YES\n id : 37 value: YES\n id : 47 value: YES\n id : 163 value: YES\n id : 61 value: YES\n id : 157 value: YES\n id : 208 value: YES\n id : 36 value: YES\n id : 55 value: YES\n id : 264 value: YES\n id : 199 value: YES\n id : 236 value: YES\n id : 255 value: YES\n id : 116 value: YES\n id : 15 value: YES\n id : 218 value: YES\n id : 98 value: YES\n id : 69 value: YES\n id : 271 value: YES\n id : 74 value: YES\n id : 160 value: YES\n id : 219 value: YES\n id : 45 value: YES\n id : 105 value: YES\n id : 100 value: YES\n id : 179 value: YES\n id : 55 value: YES\n id : 98 value: YES\n id : 90 value: YES\n id : 88 value: YES\n id : 147 value: YES\n id : 129 value: YES\n id : 16 value: YES\n id : 271 value: YES\n id : 82 value: YES\n id : 147 value: YES\n id : 38 value: YES\n id : 10 value: YES\n id : 208 value: YES\n id : 203 value: YES\n id : 124 value: YES\n id : 218 value: YES\n id : 211 value: YES\n id : 100 value: YES\n id : 10 value: YES\n id : 240 value: YES\n id : 274 value: YES\n id : 81 value: YES\n id : 165 value: YES\n id : 199 value: YES\n id : 147 value: YES\n id : 69 value: YES\n id : 178 value: YES\n id : 163 value: YES\n id : 227 value: YES\n id : 43 value: YES\n id : 271 value: YES\n id : 137 value: YES\n id : 123 value: YES\n id : 183 value: YES\n id : 4 value: YES\n id : 204 value: YES\n id : 4 value: YES\n id : 189 value: YES\n id : 184 value: YES\n id : 227 value: YES\n id : 154 value: YES\n id : 100 value: YES\n id : 1 value: YES\n id : 172 value: YES\n id : 145 value: YES\n id : 186 value: YES\n id : 61 value: YES\n id : 191 value: YES\n id : 151 value: YES\n id : 235 value: YES\nModel training Done!\nStart updating weights!\n id : 147 value: YES\n id : 72 value: YES\n id : 140 value: YES\n id : 271 value: YES\n id : 16 value: YES\n id : 28 value: YES\n id : 61 value: YES\n id : 10 value: YES\n id : 219 value: YES\n id : 189 value: YES\n id : 199 value: YES\n id : 240 value: YES\n id : 183 value: YES\n id : 220 value: YES\n id : 3 value: YES\n id : 174 value: YES\n id : 220 value: YES\n id : 6 value: YES\n id : 209 value: YES\n id : 115 value: YES\n id : 260 value: YES\n id : 100 value: YES\n id : 183 value: YES\n id : 184 value: YES\n id : 52 value: YES\n id : 82 value: YES\n id : 100 value: YES\n id : 43 value: YES\n id : 233 value: YES\n id : 236 value: YES\n id : 79 value: YES\n id : 38 value: YES\n id : 165 value: YES\n id : 165 value: YES\n id : 79 value: YES\n id : 218 value: YES\n id : 38 value: YES\n id : 54 value: YES\n id : 91 value: YES\n id : 174 value: YES\n id : 128 value: YES\n id : 199 value: YES\n id : 90 value: YES\n id : 13 value: YES\n id : 6 value: YES\n id : 30 value: YES\n id : 163 value: YES\n id : 174 value: YES\n id : 264 value: YES\n id : 81 value: YES\n id : 206 value: YES\n id : 123 value: YES\n id : 72 value: YES\n id : 64 value: YES\n id : 160 value: YES\n id : 156 value: YES\n id : 111 value: YES\n id : 51 value: YES\n id : 274 value: YES\n id : 59 value: YES\n id : 13 value: YES\n id : 64 value: YES\n id : 272 value: YES\n id : 24 value: YES\n id : 105 value: YES\n id : 95 value: YES\n id : 59 value: YES\n id : 111 value: YES\n id : 160 value: YES\n id : 13 value: YES\n id : 81 value: YES\n id : 157 value: YES\n id : 218 value: YES\n id : 272 value: YES\n id : 185 value: YES\n id : 226 value: YES\n id : 217 value: YES\n id : 188 value: YES\n id : 118 value: YES\n id : 75 value: YES\n id : 225 value: YES\n id : 82 value: YES\n id : 27 value: YES\n id : 206 value: YES\n id : 91 value: YES\n id : 211 value: YES\n id : 105 value: YES\n id : 86 value: YES\n id : 189 value: YES\n id : 82 value: YES\n id : 264 value: YES\n id : 253 value: YES\n id : 39 value: YES\n id : 188 value: YES\n id : 226 value: YES\n id : 87 value: YES\n id : 91 value: YES\n id : 13 value: YES\n id : 19 value: YES\n id : 110 value: YES\n id : 36 value: YES\n id : 54 value: YES\n id : 196 value: YES\n id : 91 value: YES\n id : 255 value: YES\n id : 235 value: YES\n id : 191 value: YES\n id : 102 value: YES\n id : 184 value: YES\n id : 28 value: YES\n id : 188 value: YES\n id : 21 value: YES\n id : 140 value: YES\n id : 235 value: YES\n id : 206 value: YES\n id : 21 value: YES\n id : 160 value: YES\n id : 271 value: YES\n id : 28 value: YES\n id : 28 value: YES\n id : 64 value: YES\n id : 211 value: YES\n id : 203 value: YES\n id : 255 value: YES\n id : 240 value: YES\n id : 147 value: YES\n id : 69 value: YES\n id : 90 value: YES\n id : 100 value: YES\n id : 84 value: YES\n id : 220 value: YES\n id : 272 value: YES\n id : 59 value: YES\n id : 88 value: YES\n id : 105 value: YES\n id : 45 value: YES\n id : 105 value: YES\n id : 145 value: YES\n id : 94 value: YES\n id : 233 value: YES\n id : 208 value: YES\n id : 107 value: YES\n id : 111 value: YES\n id : 141 value: YES\n id : 236 value: YES\n id : 81 value: YES\n id : 84 value: YES\n id : 128 value: YES\n id : 209 value: YES\n id : 22 value: YES\n id : 30 value: YES\n id : 141 value: YES\n id : 211 value: YES\n id : 4 value: YES\n id : 218 value: YES\n id : 196 value: YES\n id : 218 value: YES\n id : 150 value: YES\n id : 129 value: YES\n id : 235 value: YES\n id : 137 value: YES\n id : 235 value: YES\n id : 10 value: YES\n id : 52 value: YES\n id : 221 value: YES\n id : 24 value: YES\n id : 28 value: YES\n id : 19 value: YES\n id : 151 value: YES\n id : 95 value: YES\n id : 123 value: YES\n id : 74 value: YES\n id : 263 value: YES\n id : 27 value: YES\n id : 19 value: YES\n id : 82 value: YES\n id : 90 value: YES\n id : 140 value: YES\n id : 122 value: YES\n id : 188 value: YES\n id : 229 value: YES\n id : 124 value: YES\n id : 165 value: YES\n id : 13 value: YES\n id : 129 value: YES\n id : 27 value: YES\n id : 226 value: YES\n id : 129 value: YES\n id : 185 value: YES\n id : 69 value: YES\n id : 235 value: YES\n id : 204 value: YES\n id : 13 value: YES\n id : 4 value: YES\n id : 105 value: YES\n id : 22 value: YES\n id : 272 value: YES\n id : 105 value: YES\n id : 39 value: YES\n id : 107 value: YES\n id : 43 value: YES\n id : 128 value: YES\n id : 209 value: YES\n id : 271 value: YES\n id : 91 value: YES\n id : 208 value: YES\n id : 21 value: YES\n id : 178 value: YES\n id : 45 value: YES\n id : 228 value: YES\n id : 69 value: YES\n id : 59 value: YES\n id : 61 value: YES\n id : 94 value: YES\n id : 45 value: YES\n id : 225 value: YES\n id : 87 value: YES\n id : 3 value: YES\n id : 45 value: YES\n id : 151 value: YES\n id : 177 value: YES\n id : 213 value: YES\n id : 115 value: YES\n id : 10 value: YES\n id : 236 value: YES\n id : 82 value: YES\n id : 124 value: YES\n id : 52 value: YES\n id : 184 value: YES\n id : 54 value: YES\n id : 137 value: YES\n id : 82 value: YES\n id : 240 value: YES\n id : 111 value: YES\n id : 238 value: YES\n id : 236 value: YES\n id : 184 value: YES\n id : 36 value: YES\n id : 115 value: YES\n id : 128 value: YES\n id : 213 value: YES\n id : 27 value: YES\n id : 94 value: YES\n id : 240 value: YES\n id : 190 value: YES\n id : 174 value: YES\n id : 4 value: YES\n id : 191 value: YES\n id : 147 value: YES\n id : 260 value: YES\n id : 221 value: YES\n id : 100 value: YES\n id : 227 value: YES\n id : 191 value: YES\n id : 160 value: YES\n id : 36 value: YES\n id : 91 value: YES\n id : 39 value: YES\n id : 68 value: YES\n id : 82 value: YES\n id : 233 value: YES\n id : 36 value: YES\n id : 213 value: YES\n id : 264 value: YES\n id : 219 value: YES\n id : 38 value: YES\n id : 13 value: YES\n id : 88 value: YES\n id : 227 value: YES\n id : 231 value: YES\n id : 172 value: YES\n id : 229 value: YES\n id : 75 value: YES\n id : 6 value: YES\n id : 153 value: YES\nModel training Done!\nStart updating weights!\n id : 122 value: YES\n id : 260 value: YES\n id : 219 value: YES\n id : 69 value: YES\n id : 218 value: YES\n id : 272 value: YES\n id : 215 value: YES\n id : 220 value: YES\n id : 105 value: YES\n id : 43 value: YES\n id : 86 value: YES\n id : 151 value: YES\n id : 190 value: YES\n id : 211 value: YES\n id : 204 value: YES\n id : 122 value: YES\n id : 111 value: YES\n id : 231 value: YES\n id : 172 value: YES\n id : 213 value: YES\n id : 172 value: YES\n id : 209 value: YES\n id : 151 value: YES\n id : 253 value: YES\n id : 69 value: YES\n id : 100 value: YES\n id : 221 value: YES\n id : 102 value: YES\n id : 151 value: YES\n id : 177 value: YES\n id : 211 value: YES\n id : 68 value: YES\n id : 61 value: YES\n id : 186 value: YES\n id : 274 value: YES\n id : 255 value: YES\n id : 87 value: YES\n id : 145 value: YES\n id : 137 value: YES\n id : 185 value: YES\n id : 141 value: YES\n id : 255 value: YES\n id : 238 value: YES\n id : 22 value: YES\n id : 91 value: YES\n id : 235 value: YES\n id : 129 value: YES\n id : 45 value: YES\n id : 188 value: YES\n id : 183 value: YES\n id : 225 value: YES\n id : 217 value: YES\n id : 235 value: YES\n id : 141 value: YES\n id : 3 value: YES\n id : 220 value: YES\n id : 157 value: YES\n id : 195 value: YES\n id : 253 value: YES\n id : 238 value: YES\n id : 128 value: YES\n id : 54 value: YES\n id : 160 value: YES\n id : 203 value: YES\n id : 271 value: YES\n id : 185 value: YES\n id : 226 value: YES\n id : 189 value: YES\n id : 52 value: YES\n id : 255 value: YES\n id : 27 value: YES\n id : 229 value: YES\n id : 105 value: YES\n id : 238 value: YES\n id : 172 value: YES\n id : 22 value: YES\n id : 115 value: YES\n id : 128 value: YES\n id : 129 value: YES\n id : 235 value: YES\n id : 59 value: YES\n id : 102 value: YES\n id : 81 value: YES\n id : 21 value: YES\n id : 75 value: YES\n id : 87 value: YES\n id : 74 value: YES\n id : 217 value: YES\n id : 229 value: YES\n id : 95 value: YES\n id : 10 value: YES\n id : 55 value: YES\n id : 160 value: YES\n id : 236 value: YES\n id : 157 value: YES\n id : 218 value: YES\n id : 100 value: YES\n id : 271 value: YES\n id : 102 value: YES\n id : 51 value: YES\n id : 86 value: YES\n id : 115 value: YES\n id : 10 value: YES\n id : 6 value: YES\n id : 151 value: YES\n id : 203 value: YES\n id : 272 value: YES\n id : 183 value: YES\n id : 44 value: YES\n id : 185 value: YES\n id : 240 value: YES\n id : 98 value: YES\n id : 6 value: YES\n id : 257 value: YES\n id : 178 value: YES\n id : 86 value: YES\n id : 38 value: YES\n id : 45 value: YES\n id : 220 value: YES\n id : 264 value: YES\n id : 140 value: YES\n id : 45 value: YES\n id : 174 value: YES\n id : 160 value: YES\n id : 177 value: YES\n id : 75 value: YES\n id : 191 value: YES\n id : 4 value: YES\n id : 115 value: YES\n id : 54 value: YES\n id : 209 value: YES\n id : 228 value: YES\n id : 228 value: YES\n id : 90 value: YES\n id : 185 value: YES\n id : 220 value: YES\n id : 141 value: YES\n id : 253 value: YES\n id : 128 value: YES\n id : 28 value: YES\n id : 137 value: YES\n id : 233 value: YES\n id : 22 value: YES\n id : 231 value: YES\n id : 128 value: YES\n id : 228 value: YES\n id : 218 value: YES\n id : 74 value: YES\n id : 90 value: YES\n id : 91 value: YES\n id : 157 value: YES\n id : 45 value: YES\n id : 59 value: YES\n id : 123 value: YES\n id : 122 value: YES\n id : 157 value: YES\n id : 75 value: YES\n id : 217 value: YES\n id : 229 value: YES\n id : 185 value: YES\n id : 111 value: YES\n id : 157 value: YES\n id : 211 value: YES\n id : 219 value: YES\n id : 240 value: YES\n id : 178 value: YES\n id : 140 value: YES\n id : 82 value: YES\n id : 64 value: YES\n id : 79 value: YES\n id : 39 value: YES\n id : 178 value: YES\n id : 129 value: YES\n id : 160 value: YES\n id : 213 value: YES\n id : 39 value: YES\n id : 231 value: YES\n id : 59 value: YES\n id : 156 value: YES\n id : 255 value: YES\n id : 68 value: YES\n id : 253 value: YES\n id : 55 value: YES\n id : 72 value: YES\n id : 208 value: YES\n id : 226 value: YES\n id : 45 value: YES\n id : 74 value: YES\n id : 253 value: YES\n id : 264 value: YES\n id : 240 value: YES\n id : 122 value: YES\n id : 28 value: YES\n id : 160 value: YES\n id : 229 value: YES\n id : 260 value: YES\n id : 178 value: YES\n id : 177 value: YES\n id : 59 value: YES\n id : 81 value: YES\n id : 199 value: YES\n id : 100 value: YES\n id : 122 value: YES\n id : 4 value: YES\n id : 52 value: YES\n id : 163 value: YES\n id : 199 value: YES\n id : 248 value: YES\n id : 74 value: YES\n id : 107 value: YES\n id : 82 value: YES\n id : 206 value: YES\n id : 88 value: YES\n id : 30 value: YES\n id : 233 value: YES\n id : 238 value: YES\n id : 61 value: YES\n id : 88 value: YES\n id : 228 value: YES\n id : 208 value: YES\n id : 69 value: YES\n id : 6 value: YES\n id : 51 value: YES\n id : 260 value: YES\n id : 177 value: YES\n id : 4 value: YES\n id : 98 value: YES\n id : 64 value: YES\n id : 105 value: YES\n id : 217 value: YES\n id : 233 value: YES\n id : 84 value: YES\n id : 184 value: YES\n id : 115 value: YES\n id : 233 value: YES\n id : 51 value: YES\n id : 188 value: YES\n id : 64 value: YES\n id : 87 value: YES\n id : 188 value: YES\n id : 38 value: YES\n id : 4 value: YES\n id : 147 value: YES\n id : 79 value: YES\n id : 3 value: YES\n id : 27 value: YES\n id : 227 value: YES\n id : 172 value: YES\n id : 163 value: YES\n id : 160 value: YES\n id : 260 value: YES\n id : 221 value: YES\n id : 236 value: YES\n id : 21 value: YES\n id : 21 value: YES\n id : 184 value: YES\n id : 68 value: YES\n id : 3 value: YES\n id : 74 value: YES\n id : 51 value: YES\n id : 264 value: YES\n id : 79 value: YES\n id : 38 value: YES\n id : 147 value: YES\n id : 145 value: YES\n id : 52 value: YES\n id : 82 value: YES\n id : 51 value: YES\n id : 137 value: YES\n id : 255 value: YES\n id : 45 value: YES\n id : 43 value: YES\n id : 238 value: YES\n id : 28 value: YES\n id : 172 value: YES\nModel training Done!\nStart updating weights!\n id : 110 value: YES\n id : 233 value: YES\n id : 231 value: YES\n id : 271 value: YES\n id : 213 value: YES\n id : 79 value: YES\n id : 240 value: YES\n id : 218 value: YES\n id : 43 value: YES\n id : 213 value: YES\n id : 238 value: YES\n id : 172 value: YES\n id : 105 value: YES\n id : 264 value: YES\n id : 191 value: YES\n id : 233 value: YES\n id : 28 value: YES\n id : 163 value: YES\n id : 115 value: YES\n id : 189 value: YES\n id : 105 value: YES\n id : 211 value: YES\n id : 30 value: YES\n id : 122 value: YES\n id : 10 value: YES\n id : 123 value: YES\n id : 68 value: YES\n id : 111 value: YES\n id : 160 value: YES\n id : 90 value: YES\n id : 27 value: YES\n id : 233 value: YES\n id : 81 value: YES\n id : 156 value: YES\n id : 274 value: YES\n id : 260 value: YES\n id : 68 value: YES\n id : 75 value: YES\n id : 54 value: YES\n id : 107 value: YES\n id : 163 value: YES\n id : 64 value: YES\n id : 45 value: YES\n id : 199 value: YES\n id : 140 value: YES\n id : 129 value: YES\n id : 43 value: YES\n id : 38 value: YES\n id : 191 value: YES\n id : 75 value: YES\n id : 238 value: YES\n id : 191 value: YES\n id : 195 value: YES\n id : 225 value: YES\n id : 45 value: YES\n id : 255 value: YES\n id : 30 value: YES\n id : 191 value: YES\n id : 226 value: YES\n id : 91 value: YES\n id : 228 value: YES\n id : 220 value: YES\n id : 28 value: YES\n id : 102 value: YES\n id : 21 value: YES\n id : 238 value: YES\n id : 51 value: YES\n id : 225 value: YES\n id : 264 value: YES\n id : 3 value: YES\n id : 91 value: YES\n id : 111 value: YES\n id : 213 value: YES\n id : 199 value: YES\n id : 185 value: YES\n id : 128 value: YES\n id : 107 value: YES\n id : 88 value: YES\n id : 208 value: YES\n id : 52 value: YES\n id : 22 value: YES\n id : 226 value: YES\n id : 30 value: YES\n id : 36 value: YES\n id : 88 value: YES\n id : 55 value: YES\n id : 10 value: YES\n id : 199 value: YES\n id : 100 value: YES\n id : 72 value: YES\n id : 86 value: YES\n id : 157 value: YES\n id : 13 value: YES\n id : 156 value: YES\n id : 160 value: YES\n id : 30 value: YES\n id : 88 value: YES\n id : 235 value: YES\n id : 145 value: YES\n id : 90 value: YES\n id : 74 value: YES\n id : 54 value: YES\n id : 231 value: YES\n id : 189 value: YES\n id : 211 value: YES\n id : 82 value: YES\n id : 147 value: YES\n id : 217 value: YES\n id : 156 value: YES\n id : 55 value: YES\n id : 218 value: YES\n id : 79 value: YES\n id : 21 value: YES\n id : 107 value: YES\n id : 87 value: YES\n id : 6 value: YES\n id : 231 value: YES\n id : 203 value: YES\n id : 107 value: YES\n id : 4 value: YES\n id : 199 value: YES\n id : 229 value: YES\n id : 231 value: YES\n id : 82 value: YES\n id : 160 value: YES\n id : 157 value: YES\n id : 105 value: YES\n id : 147 value: YES\n id : 188 value: YES\n id : 21 value: YES\n id : 229 value: YES\n id : 190 value: YES\n id : 236 value: YES\n id : 229 value: YES\n id : 45 value: YES\n id : 177 value: YES\n id : 217 value: YES\n id : 75 value: YES\n id : 157 value: YES\n id : 184 value: YES\n id : 79 value: YES\n id : 184 value: YES\n id : 81 value: YES\n id : 229 value: YES\n id : 184 value: YES\n id : 74 value: YES\n id : 95 value: YES\n id : 206 value: YES\n id : 102 value: YES\n id : 174 value: YES\n id : 151 value: YES\n id : 41 value: YES\n id : 213 value: YES\n id : 264 value: YES\n id : 189 value: YES\n id : 123 value: YES\n id : 90 value: YES\n id : 233 value: YES\n id : 157 value: YES\n id : 79 value: YES\n id : 260 value: YES\n id : 22 value: YES\n id : 147 value: YES\n id : 87 value: YES\n id : 115 value: YES\n id : 27 value: YES\n id : 156 value: YES\n id : 229 value: YES\n id : 213 value: YES\n id : 160 value: YES\n id : 75 value: YES\n id : 226 value: YES\n id : 21 value: YES\n id : 55 value: YES\n id : 264 value: YES\n id : 255 value: YES\n id : 188 value: YES\n id : 6 value: YES\n id : 156 value: YES\n id : 163 value: YES\n id : 229 value: YES\n id : 10 value: YES\n id : 140 value: YES\n id : 238 value: YES\n id : 3 value: YES\n id : 123 value: YES\n id : 128 value: YES\n id : 151 value: YES\n id : 105 value: YES\n id : 213 value: YES\n id : 111 value: YES\n id : 45 value: YES\n id : 178 value: YES\n id : 225 value: YES\n id : 163 value: YES\n id : 209 value: YES\n id : 3 value: YES\n id : 227 value: YES\n id : 177 value: YES\n id : 227 value: YES\n id : 107 value: YES\n id : 185 value: YES\n id : 163 value: YES\n id : 219 value: YES\n id : 51 value: YES\n id : 229 value: YES\n id : 3 value: YES\n id : 157 value: YES\n id : 84 value: YES\n id : 238 value: YES\n id : 233 value: YES\n id : 218 value: YES\n id : 145 value: YES\n id : 115 value: YES\n id : 98 value: YES\n id : 218 value: YES\n id : 115 value: YES\n id : 28 value: YES\n id : 90 value: YES\n id : 30 value: YES\n id : 52 value: YES\n id : 128 value: YES\n id : 61 value: YES\n id : 264 value: YES\n id : 128 value: YES\n id : 54 value: YES\n id : 128 value: YES\n id : 3 value: YES\n id : 260 value: YES\n id : 209 value: YES\n id : 55 value: YES\n id : 236 value: YES\n id : 68 value: YES\n id : 79 value: YES\n id : 51 value: YES\n id : 160 value: YES\n id : 3 value: YES\n id : 204 value: YES\n id : 19 value: YES\n id : 74 value: YES\n id : 236 value: YES\n id : 141 value: YES\n id : 51 value: YES\n id : 229 value: YES\n id : 204 value: YES\n id : 140 value: YES\n id : 177 value: YES\n id : 178 value: YES\n id : 54 value: YES\n id : 3 value: YES\n id : 163 value: YES\n id : 28 value: YES\n id : 177 value: YES\n id : 236 value: YES\n id : 191 value: YES\n id : 10 value: YES\n id : 28 value: YES\n id : 64 value: YES\n id : 79 value: YES\n id : 174 value: YES\n id : 51 value: YES\n id : 111 value: YES\n id : 253 value: YES\n id : 203 value: YES\n id : 6 value: YES\n id : 90 value: YES\n id : 240 value: YES\n id : 264 value: YES\n id : 253 value: YES\n id : 217 value: YES\n id : 157 value: YES\n id : 38 value: YES\n id : 227 value: YES\n id : 75 value: YES\n id : 145 value: YES\nModel training Done!\nStart updating weights!\n id : 189 value: YES\n id : 226 value: YES\n id : 160 value: YES\n id : 52 value: YES\n id : 235 value: YES\n id : 235 value: YES\n id : 185 value: YES\n id : 45 value: YES\n id : 209 value: YES\n id : 189 value: YES\n id : 184 value: YES\n id : 100 value: YES\n id : 123 value: YES\n id : 160 value: YES\n id : 90 value: YES\n id : 255 value: YES\n id : 191 value: YES\n id : 178 value: YES\n id : 95 value: YES\n id : 157 value: YES\n id : 184 value: YES\n id : 185 value: YES\n id : 156 value: YES\n id : 183 value: YES\n id : 240 value: YES\n id : 157 value: YES\n id : 79 value: YES\n id : 140 value: YES\n id : 107 value: YES\n id : 123 value: YES\n id : 204 value: YES\n id : 204 value: YES\n id : 145 value: YES\n id : 157 value: YES\n id : 208 value: YES\n id : 235 value: YES\n id : 129 value: YES\n id : 174 value: YES\n id : 87 value: YES\n id : 217 value: YES\n id : 203 value: YES\n id : 220 value: YES\n id : 122 value: YES\n id : 82 value: YES\n id : 115 value: YES\n id : 55 value: YES\n id : 86 value: YES\n id : 255 value: YES\n id : 72 value: YES\n id : 10 value: YES\n id : 189 value: YES\n id : 115 value: YES\n id : 255 value: YES\n id : 64 value: YES\n id : 225 value: YES\n id : 30 value: YES\n id : 145 value: YES\n id : 217 value: YES\n id : 271 value: YES\n id : 238 value: YES\n id : 141 value: YES\n id : 115 value: YES\n id : 191 value: YES\n id : 225 value: YES\n id : 255 value: YES\n id : 213 value: YES\n id : 91 value: YES\n id : 229 value: YES\n id : 177 value: YES\n id : 208 value: YES\n id : 209 value: YES\n id : 21 value: YES\n id : 84 value: YES\n id : 81 value: YES\n id : 100 value: YES\n id : 156 value: YES\n id : 52 value: YES\n id : 91 value: YES\n id : 160 value: YES\n id : 10 value: YES\n id : 54 value: YES\n id : 185 value: YES\n id : 10 value: YES\n id : 10 value: YES\n id : 156 value: YES\n id : 206 value: YES\n id : 189 value: YES\n id : 82 value: YES\n id : 220 value: YES\n id : 140 value: YES\n id : 123 value: YES\n id : 87 value: YES\n id : 233 value: YES\n id : 129 value: YES\n id : 174 value: YES\n id : 111 value: YES\n id : 27 value: YES\n id : 191 value: YES\n id : 220 value: YES\n id : 68 value: YES\n id : 45 value: YES\n id : 100 value: YES\n id : 220 value: YES\n id : 260 value: YES\n id : 74 value: YES\n id : 84 value: YES\n id : 13 value: YES\n id : 30 value: YES\n id : 6 value: YES\n id : 129 value: YES\n id : 217 value: YES\n id : 229 value: YES\n id : 86 value: YES\n id : 163 value: YES\n id : 82 value: YES\n id : 185 value: YES\n id : 105 value: YES\n id : 69 value: YES\n id : 217 value: YES\n id : 174 value: YES\n id : 220 value: YES\n id : 213 value: YES\n id : 226 value: YES\n id : 27 value: YES\n id : 95 value: YES\n id : 190 value: YES\n id : 233 value: YES\n id : 75 value: YES\n id : 253 value: YES\n id : 160 value: YES\n id : 22 value: YES\n id : 260 value: YES\n id : 28 value: YES\n id : 72 value: YES\n id : 30 value: YES\n id : 105 value: YES\n id : 185 value: YES\n id : 72 value: YES\n id : 79 value: YES\n id : 10 value: YES\n id : 6 value: YES\n id : 235 value: YES\n id : 74 value: YES\n id : 233 value: YES\n id : 28 value: YES\n id : 88 value: YES\n id : 115 value: YES\n id : 86 value: YES\n id : 177 value: YES\n id : 147 value: YES\n id : 122 value: YES\n id : 240 value: YES\n id : 190 value: YES\n id : 10 value: YES\n id : 226 value: YES\n id : 221 value: YES\n id : 233 value: YES\n id : 213 value: YES\n id : 253 value: YES\n id : 123 value: YES\n id : 111 value: YES\n id : 69 value: YES\n id : 38 value: YES\n id : 225 value: YES\n id : 272 value: YES\n id : 255 value: YES\n id : 191 value: YES\n id : 72 value: YES\n id : 255 value: YES\n id : 229 value: YES\n id : 172 value: YES\n id : 172 value: YES\n id : 199 value: YES\n id : 90 value: YES\n id : 64 value: YES\n id : 69 value: YES\n id : 156 value: YES\n id : 147 value: YES\n id : 157 value: YES\n id : 87 value: YES\n id : 208 value: YES\n id : 43 value: YES\n id : 199 value: YES\n id : 160 value: YES\n id : 86 value: YES\n id : 204 value: YES\n id : 64 value: YES\n id : 122 value: YES\n id : 228 value: YES\n id : 86 value: YES\n id : 13 value: YES\n id : 218 value: YES\n id : 84 value: YES\n id : 87 value: YES\n id : 221 value: YES\n id : 84 value: YES\n id : 228 value: YES\n id : 157 value: YES\n id : 52 value: YES\n id : 253 value: YES\n id : 238 value: YES\n id : 61 value: YES\n id : 204 value: YES\n id : 274 value: YES\n id : 95 value: YES\n id : 185 value: YES\n id : 229 value: YES\n id : 225 value: YES\n id : 145 value: YES\n id : 107 value: YES\n id : 156 value: YES\n id : 218 value: YES\n id : 54 value: YES\n id : 189 value: YES\n id : 4 value: YES\n id : 157 value: YES\n id : 156 value: YES\n id : 84 value: YES\n id : 72 value: YES\n id : 86 value: YES\n id : 30 value: YES\n id : 45 value: YES\n id : 274 value: YES\n id : 95 value: YES\n id : 74 value: YES\n id : 233 value: YES\n id : 61 value: YES\n id : 102 value: YES\n id : 59 value: YES\n id : 227 value: YES\n id : 87 value: YES\n id : 231 value: YES\n id : 229 value: YES\n id : 235 value: YES\n id : 203 value: YES\n id : 51 value: YES\n id : 3 value: YES\n id : 105 value: YES\n id : 72 value: YES\n id : 61 value: YES\n id : 68 value: YES\n id : 72 value: YES\n id : 10 value: YES\n id : 30 value: YES\n id : 90 value: YES\n id : 123 value: YES\n id : 204 value: YES\n id : 19 value: YES\n id : 218 value: YES\n id : 231 value: YES\n id : 10 value: YES\n id : 211 value: YES\n id : 6 value: YES\n id : 226 value: YES\n id : 22 value: YES\n id : 129 value: YES\n id : 191 value: YES\n id : 231 value: YES\n id : 91 value: YES\n id : 163 value: YES\n id : 203 value: YES\n id : 68 value: YES\n id : 84 value: YES\n id : 30 value: YES\n id : 3 value: YES\n id : 227 value: YES\n id : 45 value: YES\n id : 81 value: YES\n id : 68 value: YES\n id : 141 value: YES\n id : 52 value: YES\n id : 255 value: YES\n id : 274 value: YES\n id : 45 value: YES\n id : 64 value: YES\nModel training Done!\nStart updating weights!\n id : 4 value: YES\n id : 199 value: YES\n id : 10 value: YES\n id : 110 value: YES\n id : 185 value: YES\n id : 82 value: YES\n id : 157 value: YES\n id : 110 value: YES\n id : 87 value: YES\n id : 204 value: YES\n id : 75 value: YES\n id : 191 value: YES\n id : 145 value: YES\n id : 231 value: YES\n id : 160 value: YES\n id : 88 value: YES\n id : 21 value: YES\n id : 217 value: YES\n id : 228 value: YES\n id : 174 value: YES\n id : 54 value: YES\n id : 204 value: YES\n id : 218 value: YES\n id : 178 value: YES\n id : 217 value: YES\n id : 141 value: YES\n id : 147 value: YES\n id : 102 value: YES\n id : 72 value: YES\n id : 227 value: YES\n id : 129 value: YES\n id : 238 value: YES\n id : 111 value: YES\n id : 19 value: YES\n id : 45 value: YES\n id : 81 value: YES\n id : 172 value: YES\n id : 145 value: YES\n id : 68 value: YES\n id : 72 value: YES\n id : 225 value: YES\n id : 163 value: YES\n id : 64 value: YES\n id : 51 value: YES\n id : 10 value: YES\n id : 203 value: YES\n id : 199 value: YES\n id : 59 value: YES\n id : 82 value: YES\n id : 209 value: YES\n id : 228 value: YES\n id : 79 value: YES\n id : 91 value: YES\n id : 174 value: YES\n id : 91 value: YES\n id : 22 value: YES\n id : 107 value: YES\n id : 51 value: YES\n id : 81 value: YES\n id : 107 value: YES\n id : 209 value: YES\n id : 217 value: YES\n id : 88 value: YES\n id : 140 value: YES\n id : 72 value: YES\n id : 91 value: YES\n id : 79 value: YES\n id : 140 value: YES\n id : 6 value: YES\n id : 27 value: YES\n id : 4 value: YES\n id : 75 value: YES\n id : 10 value: YES\n id : 4 value: YES\n id : 189 value: YES\n id : 231 value: YES\n id : 88 value: YES\n id : 122 value: YES\n id : 107 value: YES\n id : 233 value: YES\n id : 233 value: YES\n id : 123 value: YES\n id : 115 value: YES\n id : 21 value: YES\n id : 185 value: YES\n id : 177 value: YES\n id : 102 value: YES\n id : 102 value: YES\n id : 274 value: YES\n id : 91 value: YES\n id : 271 value: YES\n id : 221 value: YES\n id : 227 value: YES\n id : 6 value: YES\n id : 82 value: YES\n id : 95 value: YES\n id : 274 value: YES\n id : 28 value: YES\n id : 123 value: YES\n id : 45 value: YES\n id : 163 value: YES\n id : 236 value: YES\n id : 64 value: YES\n id : 253 value: YES\n id : 4 value: YES\n id : 177 value: YES\n id : 84 value: YES\n id : 74 value: YES\n id : 185 value: YES\n id : 225 value: YES\n id : 160 value: YES\n id : 253 value: YES\n id : 115 value: YES\n id : 75 value: YES\n id : 52 value: YES\n id : 43 value: YES\n id : 231 value: YES\n id : 235 value: YES\n id : 30 value: YES\n id : 264 value: YES\n id : 206 value: YES\n id : 274 value: YES\n id : 91 value: YES\n id : 238 value: YES\n id : 172 value: YES\n id : 185 value: YES\n id : 64 value: YES\n id : 211 value: YES\n id : 28 value: YES\n id : 10 value: YES\n id : 86 value: YES\n id : 229 value: YES\n id : 235 value: YES\n id : 6 value: YES\n id : 163 value: YES\n id : 79 value: YES\n id : 129 value: YES\n id : 211 value: YES\n id : 82 value: YES\n id : 75 value: YES\n id : 10 value: YES\n id : 28 value: YES\n id : 220 value: YES\n id : 233 value: YES\n id : 95 value: YES\n id : 51 value: YES\n id : 209 value: YES\n id : 88 value: YES\n id : 204 value: YES\n id : 82 value: YES\n id : 45 value: YES\n id : 229 value: YES\n id : 240 value: YES\n id : 95 value: YES\n id : 45 value: YES\n id : 274 value: YES\n id : 72 value: YES\n id : 225 value: YES\n id : 43 value: YES\n id : 129 value: YES\n id : 87 value: YES\n id : 68 value: YES\n id : 87 value: YES\n id : 4 value: YES\n id : 191 value: YES\n id : 81 value: YES\n id : 4 value: YES\n id : 86 value: YES\n id : 45 value: YES\n id : 209 value: YES\n id : 95 value: YES\n id : 86 value: YES\n id : 208 value: YES\n id : 240 value: YES\n id : 145 value: YES\n id : 100 value: YES\n id : 4 value: YES\n id : 43 value: YES\n id : 217 value: YES\n id : 10 value: YES\n id : 172 value: YES\n id : 236 value: YES\n id : 51 value: YES\n id : 81 value: YES\n id : 86 value: YES\n id : 81 value: YES\n id : 271 value: YES\n id : 189 value: YES\n id : 209 value: YES\n id : 174 value: YES\n id : 59 value: YES\n id : 91 value: YES\n id : 115 value: YES\n id : 172 value: YES\n id : 79 value: YES\n id : 140 value: YES\n id : 4 value: YES\n id : 228 value: YES\n id : 30 value: YES\n id : 22 value: YES\n id : 208 value: YES\n id : 203 value: YES\n id : 184 value: YES\n id : 227 value: YES\n id : 145 value: YES\n id : 203 value: YES\n id : 218 value: YES\n id : 188 value: YES\n id : 21 value: YES\n id : 211 value: YES\n id : 51 value: YES\n id : 107 value: YES\n id : 211 value: YES\n id : 172 value: YES\n id : 54 value: YES\n id : 122 value: YES\n id : 87 value: YES\n id : 240 value: YES\n id : 84 value: YES\n id : 123 value: YES\n id : 231 value: YES\n id : 206 value: YES\n id : 163 value: YES\n id : 64 value: YES\n id : 208 value: YES\n id : 102 value: YES\n id : 228 value: YES\n id : 145 value: YES\n id : 227 value: YES\n id : 59 value: YES\n id : 225 value: YES\n id : 233 value: YES\n id : 79 value: YES\n id : 28 value: YES\n id : 213 value: YES\n id : 122 value: YES\n id : 30 value: YES\n id : 255 value: YES\n id : 91 value: YES\n id : 10 value: YES\n id : 157 value: YES\n id : 157 value: YES\n id : 225 value: YES\n id : 21 value: YES\n id : 107 value: YES\n id : 45 value: YES\n id : 140 value: YES\n id : 21 value: YES\n id : 238 value: YES\n id : 55 value: NO\n id : 81 value: YES\n id : 227 value: YES\n id : 228 value: YES\n id : 235 value: YES\n id : 271 value: YES\n id : 102 value: YES\n id : 226 value: YES\n id : 123 value: YES\n id : 27 value: YES\n id : 140 value: YES\n id : 145 value: YES\n id : 64 value: YES\n id : 45 value: YES\n id : 22 value: YES\n id : 123 value: YES\n id : 72 value: YES\n id : 54 value: YES\n id : 64 value: YES\n id : 189 value: YES\n id : 253 value: YES\n id : 203 value: YES\n id : 81 value: YES\n id : 229 value: YES\n id : 271 value: YES\n id : 231 value: YES\nModel training Done!\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1743750105335
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_weights)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[2.5798268603328642, 1.355877325060082, 1.7440529047542193, 1.7606545104640545, 1.4459289276104286, 1.3422684219743737, 1.4650970537150028, 1.94854436956064]\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1743750105558
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model_weights = [2.5729405335802804, 1.3565664357253402, 1.7249615891562646, 1.4542013154065523, 1.7345294091845531, 1.3445906404161652, 1.4784657771416077, 0.09319053042920802]\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "\n",
        "metric = MeanAveragePrecision()\n",
        "\n",
        "#images, _ = next(iter(val_loader))\n",
        "for model in models:\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "with torch.no_grad():\n",
        "    for idxs, images, targets in val_loader:\n",
        "                images = [img.to(device) for img in images]\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "\n",
        "                \"\"\"for img in images:\n",
        "                        preds1 = models[0](img.unsqueeze(0))\n",
        "                        print(preds1)\n",
        "                        preds2 = models[1](img.unsqueeze(0))\n",
        "                        preds3 = models[2](img.unsqueeze(0))\n",
        "                        preds4 = models[3](img.unsqueeze(0))\"\"\"\n",
        "                preds = []\n",
        "                for model in models:\n",
        "\n",
        "                    pred = model(images)\n",
        "                    #print(preds1)\n",
        "                    #preds2 = models[1](images)\n",
        "                    #preds3 = models[2](images)\n",
        "                    #preds4 = models[3](images)\n",
        "                    preds.append(pred)\n",
        "                # Combine predictions\n",
        "                final_preds = weighted_nms(preds, model_weights)\n",
        "\n",
        "                gt_targets = [\n",
        "                            dict(\n",
        "                                boxes=t[\"boxes\"].detach().cpu(),\n",
        "                                labels=t[\"labels\"].detach().cpu()\n",
        "                            ) for t in targets\n",
        "                        ]\n",
        "                \n",
        "                metric.update(final_preds, gt_targets)\n",
        "\n",
        "                #print(final_preds)\n",
        "\n",
        "    mAP_result = metric.compute()\n",
        "    mAP_score = mAP_result[\"map\"].item()\n",
        "    mAP_50 = mAP_result[\"map_50\"].item()\n",
        "    print(mAP_score)\n",
        "    print(mAP_50)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_3146/2660485587.py:119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  box1_tensor = torch.tensor(box1).unsqueeze(0)\n/tmp/ipykernel_3146/2660485587.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  box2_tensor = torch.tensor(box2).unsqueeze(0)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.7960504293441772\n0.962081253528595\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1743750226179
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yyy =[\"1000208_.pth\"]\n",
        "xxx = [1]\n",
        "\n",
        "\n",
        "\n",
        "# Reconstruct models with their respective weights\n",
        "zzz, backbones = making_models_list(backbone_list=backbone, model_path_list= yyy, num_classes= num_classes,\n",
        "                                                pred_per_image =pred_per_image, device=device,\n",
        "                                                custom_anchor_generator = custom_anchor_generator)\n",
        "\n",
        "# Get predictions\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "\n",
        "metric = MeanAveragePrecision()\n",
        "\n",
        "#images, _ = next(iter(val_loader))\n",
        "for model in zzz:\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "with torch.no_grad():\n",
        "    for idxs, images, targets in val_loader:\n",
        "                images = [img.to(device) for img in images]\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "\n",
        "                \"\"\"for img in images:\n",
        "                        preds1 = models[0](img.unsqueeze(0))\n",
        "                        print(preds1)\n",
        "                        preds2 = models[1](img.unsqueeze(0))\n",
        "                        preds3 = models[2](img.unsqueeze(0))\n",
        "                        preds4 = models[3](img.unsqueeze(0))\"\"\"\n",
        "                preds = []\n",
        "                for model in zzz:\n",
        "\n",
        "                    pred = model(images)\n",
        "                    preds.append(pred)\n",
        "                # Combine predictions\n",
        "                final_preds = weighted_nms(preds, xxx)\n",
        "\n",
        "                gt_targets = [\n",
        "                            dict(\n",
        "                                boxes=t[\"boxes\"].detach().cpu(),\n",
        "                                labels=t[\"labels\"].detach().cpu()\n",
        "                            ) for t in targets\n",
        "                        ]\n",
        "                \n",
        "                metric.update(final_preds, gt_targets)\n",
        "\n",
        "                #print(final_preds)\n",
        "\n",
        "    mAP_result = metric.compute()\n",
        "    mAP_score = mAP_result[\"map\"].item()\n",
        "    mAP_50 = mAP_result[\"map_50\"].item()\n",
        "    print(mAP_score)\n",
        "    print(mAP_50)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_3146/2660485587.py:119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  box1_tensor = torch.tensor(box1).unsqueeze(0)\n/tmp/ipykernel_3146/2660485587.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  box2_tensor = torch.tensor(box2).unsqueeze(0)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.7880712151527405\n0.94431471824646\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1743750237236
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Save each model with its assigned AdaBoost weight\n",
        "model_data = {\n",
        "    \"models\": [],\n",
        "    \"model_weights\": model_weights,  # Save AdaBoost model weights (alphas)\n",
        "    \"model_backbone\": backbones,  #list of backbones of the models\n",
        "}\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    model_state = {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"alpha\": model_weights[i],  # Save the weight assigned by AdaBoost\n",
        "    }\n",
        "    model_data[\"models\"].append(model_state)\n",
        "\n",
        "torch.save(model_data, \"adaboost_faster_rcnn.pth\")\n",
        "print(\"Model and weights saved successfully!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model and weights saved successfully!\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1743750245522
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "class_names = val_dataset.classes\n",
        "\n",
        "def test_model_adaboost(models, test_loader, device, class_names, model_weights, visualize=False):\n",
        "    # Move models to device and set to eval mode\n",
        "    for model in models:\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "    # Initialize metrics and accumulators\n",
        "    metric = MeanAveragePrecision()\n",
        "    total_test_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idxs, images, targets in test_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Collect predictions from all models\n",
        "            all_predictions = []\n",
        "            for model in models:\n",
        "                pred = model(images)\n",
        "                all_predictions.append(pred)\n",
        "\n",
        "            # Apply Weighted NMS using AdaBoost weights\n",
        "            final_preds = weighted_nms(all_predictions, model_weights)\n",
        "\n",
        "            # Compute loss (optional, only for evaluation)\n",
        "            total_loss = 0\n",
        "            for model in models:\n",
        "                loss_dict = model(images, targets)\n",
        "                if isinstance(loss_dict, list):\n",
        "                    losses = sum(sum(loss_value.sum() for loss_value in loss.values()) for loss in loss_dict)\n",
        "                elif isinstance(loss_dict, dict):\n",
        "                    losses = sum(loss_value.sum() for loss_value in loss_dict.values())\n",
        "                else:\n",
        "                    raise TypeError(f\"Unexpected loss format: {type(loss_dict)}\")\n",
        "                total_loss += losses.item()\n",
        "            \n",
        "            total_test_loss += total_loss / len(models)  # Averaging loss over models\n",
        "\n",
        "            # Convert ground truth to required format for mAP computation\n",
        "            gt_targets = [\n",
        "                dict(\n",
        "                    boxes=t[\"boxes\"].detach().cpu(),\n",
        "                    labels=t[\"labels\"].detach().cpu()\n",
        "                ) for t in targets\n",
        "            ]\n",
        "\n",
        "            # Update metric\n",
        "            metric.update(final_preds, gt_targets)\n",
        "\n",
        "            # Optional: Visualize some predictions\n",
        "            if visualize:\n",
        "                visualize_predictions(images, final_preds, class_names)\n",
        "\n",
        "    # Compute Mean Average Precision (mAP)\n",
        "    mAP_result = metric.compute()\n",
        "    mAP_score = mAP_result[\"map\"].item()\n",
        "    print(f\"\\nMean Average Precision (mAP): {mAP_score:.4f}\")\n",
        "\n",
        "    # Compute average test loss\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    print(f\"\\nAverage Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "    return mAP_score, avg_test_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def visualize_predictions(images, predictions, class_names):\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        # Denormalize image\n",
        "        img = denormalize(img.cpu(), mean, std)\n",
        "        img = F_transform.to_pil_image(img)  # Convert to PIL image\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "        ax.imshow(img)\n",
        "\n",
        "        # Get predicted boxes, labels, and scores\n",
        "        pred_boxes = predictions[i]['boxes'].cpu().numpy()\n",
        "        pred_labels = predictions[i]['labels'].cpu().numpy()\n",
        "        pred_scores = predictions[i]['scores'].cpu().numpy()\n",
        "\n",
        "        if len(pred_scores) == 0:\n",
        "            print(\"No predictions found for this image.\")\n",
        "            continue\n",
        "\n",
        "        # Find the highest confidence score\n",
        "        pred_scores_tensor = torch.tensor(pred_scores) \n",
        "        max_score_idx = pred_scores_tensor.argmax()  # Get index of highest score\n",
        "\n",
        "        # Get highest confidence box, label, and score\n",
        "        best_box = pred_boxes[max_score_idx]\n",
        "        best_label = pred_labels[max_score_idx]\n",
        "        best_score = pred_scores[max_score_idx]\n",
        "        ixd_best_label = pred_labels[max_score_idx].item()\n",
        "        predicted_label_name = class_names[ixd_best_label - 1]\n",
        "\n",
        "        # Draw Highest Confidence Bounding Box (Red)\n",
        "        xmin, ymin, xmax, ymax = best_box\n",
        "        rect = plt.Rectangle(\n",
        "            (xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "            fill=False, color=\"red\", linewidth=2\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Display label and score\n",
        "        ax.text(\n",
        "            #xmin, ymin - 5, f\"{predicted_label_name}: {best_score:.2f}\",\n",
        "            xmin, ymin - 5, f\"{predicted_label_name}\",\n",
        "            color=\"red\", fontsize=12, bbox=dict(facecolor='white', alpha=0.7)\n",
        "        )\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Highest Confidence Prediction\")\n",
        "        plt.show()"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1743750245814
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on the test dataset\n",
        "# Run evaluation using AdaBoost predictions\n",
        "mAP_score, avg_loss = test_model_adaboost(\n",
        "    models=models,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    class_names=val_dataset.classes,\n",
        "    model_weights=model_weights,\n",
        "    visualize=True  # Set to False if you don't need visualization\n",
        ")\n",
        "print(mAP_score,avg_loss)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_3146/2660485587.py:119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  box1_tensor = torch.tensor(box1).unsqueeze(0)\n/tmp/ipykernel_3146/2660485587.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  box2_tensor = torch.tensor(box2).unsqueeze(0)\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1743750248894
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "mykernel",
      "language": "python",
      "display_name": "new_kernel"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "mykernel"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}