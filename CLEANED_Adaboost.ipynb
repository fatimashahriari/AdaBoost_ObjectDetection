{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "seed = 40\n",
        "train_ratio = 0.75\n",
        "test_ratio = 0.25\n",
        "pred_per_image = 15\n",
        "num_workers = 3"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1743749858902
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "save_directory = \"/home/azureuser/cloudfiles/code/Users/shahriarizadehfatima\"\n",
        "import os\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageOps\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import functional as F_transform\n",
        "from torchvision.transforms import Normalize as NormalizeTransform\n",
        "from torchvision.ops.boxes import box_iou\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from albumentations import Compose, Resize, HorizontalFlip, BboxParams, VerticalFlip, Normalize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from azureml.core import Dataset, Datastore, Workspace\n",
        "from torchmetrics.detection import MeanAveragePrecision"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using device: cuda\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/Object_Detection/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1743749890639
        },
        "id": "ZWlhDYdB8UJJ",
        "outputId": "fd071291-c586-4286-bc7f-bf48341a4840"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_dir = \"/home/azureuser/cloudfiles/code/Users/shahriarizadehfatima/config.json\"\n",
        "# Define folders to exclude\n",
        "exclude_folders = {}\n",
        "# Load workspace from config.json\n",
        "ws = Workspace.from_config(json_dir)\n",
        "datastore = Datastore.get(ws, \"fasteners_1\")\n",
        "dataset_images = Dataset.File.from_files(path=(datastore, 'images/'))\n",
        "mounted_context_images = dataset_images.mount()\n",
        "mounted_context_images.start()\n",
        "# Get the mount path\n",
        "images_dir = mounted_context_images.mount_point\n",
        "images_dir_folders = [folder for folder in os.listdir(images_dir) if folder not in exclude_folders]\n",
        "#dataset_labels\n",
        "dataset_labels = Dataset.File.from_files(path=(datastore, 'labels/'))\n",
        "mounted_context_labels = dataset_labels.mount()\n",
        "mounted_context_labels.start()\n",
        "# Get the mount path\n",
        "labels_dir = mounted_context_labels.mount_point\n",
        "labels_dir_folders = [folder for folder in os.listdir(labels_dir) if folder not in exclude_folders]\n",
        "classes_file = labels_dir_folders"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Available datastores: ['azureml_globaldatasets', 'fasteners_1', 'workspacefilestore', 'workspaceworkingdirectory', 'workspaceartifactstore', 'workspaceblobstore']\nImages Dataset mounted at: /tmp/tmpy9owx32p ['Cable Straps Edge Mount', 'Cable Straps Hole Mount', 'Tape-on Locators Edge Mount', 'Tape-on Locators Hole Mount']\nLabels Dataset mounted at: /tmp/tmphayze_bw ['Cable Straps Edge Mount', 'Cable Straps Hole Mount', 'Tape-on Locators Edge Mount', 'Tape-on Locators Hole Mount']\nclasses_file: ['Cable Straps Edge Mount', 'Cable Straps Hole Mount', 'Tape-on Locators Edge Mount', 'Tape-on Locators Hole Mount']\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1743749898096
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of class names (subdirectories in images/)\n",
        "class_names = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n",
        "num_classes = len(class_names)+ 1\n",
        "# Lists to store file paths for each split\n",
        "train_files = []\n",
        "test_files = []\n",
        "# Split the data and store file paths\n",
        "for class_name in class_names:\n",
        "    image_class_dir = os.path.join(images_dir, class_name)\n",
        "    label_class_dir = os.path.join(labels_dir, class_name)\n",
        "    image_files = [f for f in os.listdir(image_class_dir)]\n",
        "    random.shuffle(image_files)\n",
        "\n",
        "    num_images = len(image_files)\n",
        "    train_split = int(train_ratio * num_images)\n",
        "    test_split = int(test_ratio * num_images)\n",
        "\n",
        "    train_image_files = image_files[:train_split]\n",
        "    test_image_files = image_files[train_split:train_split + test_split]\n",
        "\n",
        "    for split, files in zip(['train', 'test'], [train_image_files, test_image_files]):\n",
        "        for file_name in files:\n",
        "            image_path = os.path.join(image_class_dir, file_name)\n",
        "            label_path = os.path.join(label_class_dir, file_name.replace(os.path.splitext(file_name)[1], '.txt'))\n",
        "            if split == 'train':\n",
        "                train_files.append((image_path, label_path, class_name))\n",
        "            elif split == 'test':\n",
        "                test_files.append((image_path, label_path, class_name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "len tarain: 275\nlen tarain: 77\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1743749898394
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_list, classes_file, image_size=(224, 224), augmentation=False):\n",
        "        self.file_list = file_list\n",
        "        self.classes = list_subfolders(classes_file)\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.new_width, self.new_height = image_size\n",
        "        self.resize = Resize(height=self.new_height, width=self.new_width)\n",
        "        self.normalize = NormalizeTransform(mean=[0.485, 0.456, 0.406],\n",
        "                                            std=[0.229, 0.224, 0.225])\n",
        "        self.augmentations = get_augmentations() if augmentation else []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list) * (len(self.augmentations) + 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_idx = idx // (len(self.augmentations) + 1)\n",
        "        aug_idx = idx % (len(self.augmentations) + 1)\n",
        "        img_path, label_path, _ = self.file_list[img_idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        img = np.array(image)\n",
        "        img = self.resize(image=img)['image']\n",
        "\n",
        "        bboxes, labels = [], []\n",
        "        with open(label_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"Class:\"):\n",
        "                    class_name = line.split(\":\")[1].strip().strip('\"')\n",
        "                elif line.startswith(\"Bounding Box:\"):\n",
        "                    bbox = eval(line.split(\":\")[1].strip())\n",
        "                    bboxes.append(bbox)\n",
        "                    labels.append(self.classes.index(class_name) + 1)\n",
        "\n",
        "        if not bboxes:\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "        # Scale bboxes\n",
        "        orig_width, orig_height = image.size\n",
        "        scale_x, scale_y = self.new_width / orig_width, self.new_height / orig_height\n",
        "        bboxes = [[x1 * scale_x, y1 * scale_y, x2 * scale_x, y2 * scale_y] for x1, y1, x2, y2 in bboxes]\n",
        "\n",
        "        if aug_idx != 0:\n",
        "            aug = self.augmentations[aug_idx - 1]\n",
        "            augmented = aug(image=img, bboxes=bboxes, class_labels=labels)\n",
        "            img = augmented['image']\n",
        "            bboxes = augmented['bboxes']\n",
        "            labels = augmented['class_labels']\n",
        "            img = torch.from_numpy(img.astype(np.float32) / 255.0).permute(2, 0, 1)\n",
        "        else:\n",
        "            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        img = self.normalize(img)\n",
        "        return idx, img, {\"boxes\": torch.tensor(bboxes, dtype=torch.float32),\n",
        "                          \"labels\": torch.tensor(labels, dtype=torch.int64)}\n",
        "def list_subfolders(main_folder):\n",
        "    if isinstance(main_folder, list):\n",
        "        return main_folder\n",
        "    if not os.path.isdir(main_folder):\n",
        "        raise ValueError(f\"The provided path '{main_folder}' is not valid.\")\n",
        "    return [name for name in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, name))]\n",
        "\n",
        "def custom_collate(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    return torch.stack(images, 0), list(targets)\n",
        "\n",
        "def get_augmentations():\n",
        "    bbox_params = BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"])\n",
        "    return [\n",
        "        Compose([HorizontalFlip()], bbox_params=bbox_params),\n",
        "        Compose([VerticalFlip()], bbox_params=bbox_params)\n",
        "    ]\n",
        "\n",
        "class NormalizeTransform:\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return (tensor - self.mean) / self.std\n",
        "\n",
        "def denormalize(tensor, mean, std):\n",
        "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "    std = torch.tensor(std).view(-1, 1, 1)\n",
        "    return tensor * std + mean\n",
        "\n",
        "transform = ToTensorV2()\n",
        "\n",
        "def save_model(model, directory, model_name=\"custom_object_detection_model.pth\"):\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    save_path = os.path.join(directory, model_name)\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "def load_model(model, directory, model_name):\n",
        "    load_path = os.path.join(directory, model_name)\n",
        "    model.load_state_dict(torch.load(load_path))\n",
        "    print(f\"Model loaded from {load_path}\")\n",
        "    return model"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/Object_Detection/lib/python3.10/site-packages/albumentations/core/composition.py:250: UserWarning: Got processor for bboxes, but no transform to process it.\n  self._set_keys()\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1743749898641
        },
        "id": "9HSKYqDx9Ly3",
        "outputId": "3b345d14-53a8-4768-a020-257d1434c02c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset instances\n",
        "train_dataset = CustomDataset(file_list = train_files, classes_file=  classes_file , augmentation= False, image_size=(224,224))\n",
        "test_dataset = CustomDataset(file_list= test_files, classes_file=  classes_file)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers= num_workers,collate_fn=lambda batch: tuple(zip(*batch)))\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda batch: tuple(zip(*batch)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of training images: 275\nNumber of validation images: 77\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1743749898904
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the backbone\n",
        "backbone_1 = resnet_fpn_backbone(\"resnet50\", pretrained= False) # or pretrained=True if you want a pretrained backbone\n",
        "backbone_2 = resnet_fpn_backbone(\"resnet101\", pretrained= False) # or pretrained=True if you want a pretrained backbone\n",
        "backbone_3 = resnet_fpn_backbone(\"resnet34\", pretrained= False) # or pretrained=True if you want a pretrained backbone\n",
        "\n",
        "backbone = [backbone_1, backbone_2, backbone_3]\n",
        "\n",
        "# Define custom anchor sizes and aspect ratios\n",
        "anchor_sizes = ((32,), (64,), (128,),(140,),(224,)) # Sizes for each FPN level\n",
        "aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)  # Aspect ratios for all levels\n",
        "# Create a custom AnchorGenerator\n",
        "custom_anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/Object_Detection/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n  warnings.warn(\n/anaconda/envs/Object_Detection/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/anaconda/envs/Object_Detection/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/anaconda/envs/Object_Detection/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/anaconda/envs/Object_Detection/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1743749906026
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path_list = [\"1000204_.pth\", \"1000206_.pth\", \"1000202_.pth\", \"1000203_.pth\", \"1000201_.pth\", \"1000205_.pth\", \"1000207_.pth\", \"1000208_.pth\"]\n",
        "\n",
        "# Load the pretrained model\n",
        "def making_models_list(backbone_list, model_path_list, num_classes, pred_per_image, device, custom_anchor_generator):\n",
        "    model_list = []\n",
        "    backbones_list = []\n",
        "\n",
        "    for path in model_path_list:   \n",
        "        if path in [\"1000203_.pth\"]: #ResNet101\n",
        "            backbone = backbone_list[1]\n",
        "            backbones_list.append(\"ResNet101\")\n",
        "        elif path in [\"1000202_.pth\"]:  #ResNet34\n",
        "            backbone = backbone_list[2]\n",
        "            backbones_list.append(\"ResNet34\")\n",
        "        else:                            #ResNet50\n",
        "            backbone = backbone_list[0]\n",
        "            backbones_list.append(\"ResNet50\")\n",
        "        checkpoint = torch.load(path, map_location = device)\n",
        "        model = torchvision.models.detection.FasterRCNN(backbone,\n",
        "                                                        num_classes= num_classes,\n",
        "                                                        #class_weights=class_weights,\n",
        "                                                        rpn_pre_nms_top_n_train=100,\n",
        "                                                        rpn_post_nms_top_n_train=100,\n",
        "                                                        rpn_nms_thresh= 0.5, #Lower (e.g., 0.3): Stricter filtering (removes more overlapping boxes),\n",
        "                                                        #Higher (e.g., 0.6): Allows more overlapping boxes (useful if objects are close together).\n",
        "                                                        rpn_score_thresh=0.4,\n",
        "                                                        rpn_anchor_generator= custom_anchor_generator,\n",
        "                                                        box_fg_iou_thresh= 0.5, # Positive threshold,\n",
        "                                                        box_bg_iou_thresh= 0.5) # Negative threshold)\n",
        "\n",
        "        representation_size = model.roi_heads.box_predictor.cls_score.in_features\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictor(representation_size, num_classes)\n",
        "        model.roi_heads.detections_per_img = pred_per_image  # Reduce number of false positives\n",
        "        #print(model.roi_heads)\n",
        "        #print(model.roi_heads.__dict__.keys())\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
        "        model_list.append(model)\n",
        "    return model_list, backbone_list"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1743749906267
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models, backbones = making_models_list(backbone_list=backbone, model_path_list= model_path_list, num_classes= num_classes,\n",
        "                                                pred_per_image =pred_per_image, device=device,\n",
        "                                                custom_anchor_generator = custom_anchor_generator)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1743749947732
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "weights = {str(i): 1/len(train_dataset) for i in range(len(train_dataset))}\n",
        "# Create a sampler based on the weights\n",
        "\n",
        "def compute_weighted_error(model, sample_weights, dataset, device):\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    weights_list = [sample_weights[str(i)] for i in range(len(dataset))]\n",
        "\n",
        "    sampler = WeightedRandomSampler(weights_list, num_samples=len(weights), replacement=True)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers= num_workers,\n",
        "                                                sampler= sampler, collate_fn=lambda batch: tuple(zip(*batch)))\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    update_status= {}\n",
        "    for idxs, images, targets in dataloader:\n",
        "        idx = [i for i in idxs]\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        with torch.no_grad():\n",
        "            for k in range(len(images)):\n",
        "                loss_dict = model(images[k].unsqueeze(0), [targets[k]])\n",
        "                loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "                if 'loss_classifier' in loss_dict and 'loss_box_reg' in loss_dict:\n",
        "                    if loss_dict['loss_classifier'] < 0.05 and loss_dict['loss_box_reg'] < 0.05:\n",
        "                        update_status[str(idx[k])] = 'NO'\n",
        "                        print(f\" id : {str(idx[k])} value: {update_status[str(idx[k])]}\")\n",
        "                    else:\n",
        "                        update_status[str(idx[k])] = 'YES'\n",
        "                        total_loss += loss * weights[str(idx[k])]\n",
        "                        print(f\" id : {str(idx[k])} value: {update_status[str(idx[k])]}\")\n",
        "                elif isinstance(loss_dict, list):\n",
        "                    total_classifier_loss = sum(d.get('loss_classifier', 0) for d in loss_dict)\n",
        "                    total_box_reg_loss = sum(d.get('loss_box_reg', 0) for d in loss_dict)\n",
        "                    if total_classifier_loss <= 0.05 and total_box_reg_loss < 0.05:\n",
        "                            update_status[str(idx[k])] = 'NO'\n",
        "                            print(f\" id : {str(idx[k])} value: {update_status[str(idx[k])]}\")\n",
        "                    else:\n",
        "                            update_status[str(idx[k])] = 'YES'\n",
        "                            print(f\" id : {str(idx[k])} value: {update_status[str(idx[k])]}\")\n",
        "                            total_loss += loss * weights[str(idx[k])]\n",
        "\n",
        "    error = total_loss / len(dataloader.dataset)\n",
        "    print(\"Model training Done!\")\n",
        "    return error, update_status\n",
        "\n",
        "def adaboost_faster_rcnn(models, dataset, sample_weights, device):\n",
        "    model_weights = []\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        print(\"Start updating weights!\")\n",
        "        error, update_status = compute_weighted_error(model=model, sample_weights = sample_weights,\n",
        "                                                                                dataset=dataset,  device=device)\n",
        "        if error == 0:\n",
        "            alpha = 1  # Strong classifier\n",
        "        else:\n",
        "            alpha = 0.5 * math.log((1 - error) / error)\n",
        "\n",
        "        model_weights.append(alpha)\n",
        "\n",
        "        for i in sample_weights.keys():\n",
        "            if i in update_status.keys():\n",
        "                if update_status[i] == \"NO\":  # Correctly referencing dictionary keys\n",
        "                    sample_weights[i] *= math.exp(-alpha)\n",
        "                else:\n",
        "                    sample_weights[i] *= math.exp(alpha)\n",
        "\n",
        "        # Normalize sample weights\n",
        "        total_weight = np.sum(list(sample_weights.values()))  # Convert to list for sum\n",
        "        if total_weight > 0:\n",
        "            sample_weights = {k: v / total_weight for k, v in sample_weights.items()}\n",
        "\n",
        "    return model_weights, sample_weights\n",
        "\n",
        "def weighted_nms(predictions, model_weights, iou_threshold=0.5):\n",
        "    batch_size = len(predictions[0])  # Get batch size\n",
        "    final_preds = []\n",
        "\n",
        "    for batch_idx in range(batch_size):\n",
        "        combined_preds = []\n",
        "\n",
        "        for preds, weight in zip(predictions, model_weights):\n",
        "            for box, score, label in zip(\n",
        "                preds[batch_idx]['boxes'], preds[batch_idx]['scores'], preds[batch_idx]['labels']\n",
        "            ):\n",
        "                combined_preds.append((box, score * weight, label))\n",
        "\n",
        "        combined_preds.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        filtered_preds = []\n",
        "        while combined_preds:\n",
        "            best = combined_preds.pop(0)\n",
        "            filtered_preds.append(best)\n",
        "\n",
        "            combined_preds = [p for p in combined_preds if iou(p[0], best[0]) < iou_threshold]\n",
        "\n",
        "        # Convert back to the required format\n",
        "        if filtered_preds:\n",
        "            final_preds.append({\n",
        "                \"boxes\": torch.stack([p[0] for p in filtered_preds]),\n",
        "                \"scores\": torch.tensor([p[1] for p in filtered_preds]),\n",
        "                \"labels\": torch.tensor([p[2] for p in filtered_preds])\n",
        "            })\n",
        "        else:\n",
        "            # Handle empty case\n",
        "            final_preds.append({\"boxes\": torch.tensor([]), \"scores\": torch.tensor([]), \"labels\": torch.tensor([])})\n",
        "\n",
        "    return final_preds\n",
        "\n",
        "def iou(box1, box2):\n",
        "    box1_tensor = torch.tensor(box1).unsqueeze(0)\n",
        "    box2_tensor = torch.tensor(box2).unsqueeze(0)\n",
        "    return box_iou(box1_tensor, box2_tensor).item() # Returns a single IoU value"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1743749947965
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply AdaBoost\n",
        "model_weights , sample_weights = adaboost_faster_rcnn(models= models, dataset = train_dataset,\n",
        "                                                                sample_weights =weights, device=device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_3146/2660485587.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " id : 20 value: YES\n id : 183 value: YES\n id : 55 value: YES\n id : 39 value: YES\n id : 165 value: YES\n id : 97 value: YES\n id : 21 value: YES\n id : 178 value: YES\n id : 222 value: YES\n id : 116 value: YES\n id : 240 value: YES\n id : 261 value: YES\n id : 192 value: YES\n id : 203 value: YES\n id : 257 value: YES\n id : 217 value: YES\n id : 94 value: YES\n id : 45 value: YES\n id : 98 value: YES\n id : 6 value: YES\n id : 64 value: YES\n id : 27 value: YES\n id : 199 value: YES\n id : 28 value: YES\n id : 230 value: YES\n id : 188 value: YES\n id : 218 value: YES\n id : 168 value: YES\n id : 152 value: YES\n id : 61 value: YES\n id : 3 value: YES\n id : 184 value: YES\n id : 220 value: YES\n id : 105 value: YES\n id : 102 value: YES\n id : 31 value: YES\n id : 169 value: YES\n id : 2 value: YES\n id : 172 value: YES\n id : 19 value: YES\n id : 272 value: YES\n id : 86 value: YES\n id : 187 value: YES\n id : 68 value: YES\n id : 208 value: YES\n id : 80 value: YES\n id : 139 value: YES\n id : 30 value: YES\n id : 192 value: YES\n id : 75 value: YES\n id : 30 value: YES\n id : 239 value: YES\n id : 104 value: YES\n id : 129 value: YES\n id : 110 value: YES\n id : 231 value: YES\n id : 102 value: YES\n id : 202 value: YES\n id : 64 value: YES\n id : 116 value: YES\n id : 210 value: YES\n id : 169 value: YES\n id : 255 value: YES\n id : 191 value: YES\n id : 19 value: YES\n id : 271 value: YES\n id : 124 value: YES\n id : 231 value: YES\n id : 191 value: YES\n id : 230 value: YES\n id : 64 value: YES\n id : 238 value: YES\n id : 225 value: YES\n id : 124 value: YES\n id : 123 value: YES\n id : 263 value: YES\n id : 55 value: YES\n id : 44 value: YES\n id : 231 value: YES\n id : 258 value: YES\n id : 119 value: YES\n id : 30 value: YES\n id : 152 value: YES\n id : 153 value: YES\n id : 262 value: YES\n id : 119 value: YES\n id : 253 value: YES\n id : 77 value: YES\n id : 46 value: YES\n id : 116 value: YES\n id : 197 value: YES\n id : 231 value: YES\n id : 253 value: YES\n id : 174 value: YES\n id : 226 value: YES\n id : 74 value: YES\n id : 51 value: YES\n id : 61 value: YES\n id : 159 value: YES\n id : 168 value: YES\n id : 55 value: YES\n id : 105 value: YES\n id : 191 value: YES\n id : 59 value: YES\n id : 206 value: YES\n id : 65 value: YES\n id : 219 value: YES\n id : 11 value: YES\n id : 18 value: YES\n id : 122 value: YES\n id : 133 value: YES\n id : 1 value: YES\n id : 157 value: YES\n id : 84 value: YES\n id : 27 value: YES\n id : 257 value: YES\n id : 54 value: YES\n id : 127 value: YES\n id : 51 value: YES\n id : 107 value: YES\n id : 272 value: YES\n id : 251 value: YES\n id : 176 value: YES\n id : 94 value: YES\n id : 264 value: YES\n id : 157 value: YES\n id : 45 value: YES\n id : 224 value: YES\n id : 222 value: YES\n id : 52 value: YES\n id : 97 value: YES\n id : 259 value: YES\n id : 179 value: YES\n id : 106 value: YES\n id : 163 value: YES\n id : 197 value: YES\n id : 65 value: YES\n id : 239 value: YES\n id : 258 value: YES\n id : 140 value: YES\n id : 221 value: YES\n id : 211 value: YES\n id : 69 value: YES\n id : 100 value: YES\n id : 145 value: YES\n id : 261 value: YES\n id : 186 value: YES\n id : 90 value: YES\n id : 134 value: YES\n id : 75 value: YES\n id : 195 value: YES\n id : 183 value: YES\n id : 2 value: YES\n id : 204 value: YES\n id : 69 value: YES\n id : 87 value: YES\n id : 94 value: YES\n id : 183 value: YES\n id : 169 value: YES\n id : 151 value: YES\n id : 16 value: YES\n id : 233 value: YES\n id : 257 value: YES\n id : 27 value: YES\n id : 165 value: YES\n id : 21 value: YES\n id : 24 value: YES\n id : 163 value: YES\n id : 18 value: YES\n id : 75 value: YES\n id : 127 value: YES\n id : 82 value: YES\n id : 115 value: YES\n id : 227 value: YES\n id : 233 value: YES\n id : 190 value: YES\n id : 95 value: YES\n id : 189 value: YES\n id : 264 value: YES\n id : 41 value: YES\n id : 53 value: YES\n id : 94 value: YES\n id : 235 value: YES\n id : 185 value: YES\n id : 72 value: YES\n id : 208 value: YES\n id : 150 value: YES\n id : 233 value: YES\n id : 107 value: YES\n id : 228 value: YES\n id : 209 value: YES\n id : 237 value: YES\n id : 27 value: YES\n id : 248 value: YES\n id : 30 value: YES\n id : 38 value: YES\n id : 146 value: YES\n id : 196 value: YES\n id : 134 value: YES\n id : 160 value: YES\n id : 217 value: YES\n id : 215 value: YES\n id : 147 value: YES\n id : 266 value: YES\n id : 43 value: YES\n id : 81 value: YES\n id : 70 value: YES\n id : 236 value: YES\n id : 208 value: YES\n id : 137 value: YES\n id : 11 value: YES\n id : 112 value: YES\n id : 64 value: YES\n id : 172 value: YES\n id : 52 value: YES\n id : 91 value: YES\n id : 151 value: YES\n id : 22 value: YES\n id : 6 value: YES\n id : 213 value: YES\n id : 274 value: YES\n id : 111 value: YES\n id : 220 value: YES\n id : 108 value: YES\n id : 36 value: YES\n id : 69 value: YES\n id : 58 value: YES\n id : 208 value: YES\n id : 123 value: YES\n id : 2 value: YES\n id : 213 value: YES\n id : 192 value: YES\n id : 4 value: YES\n id : 80 value: YES\n id : 156 value: YES\n id : 23 value: YES\n id : 98 value: YES\n id : 79 value: YES\n id : 251 value: YES\n id : 128 value: YES\n id : 73 value: YES\n id : 59 value: YES\n id : 248 value: YES\n id : 202 value: YES\n id : 64 value: YES\n id : 222 value: YES\n id : 10 value: YES\n id : 141 value: YES\n id : 88 value: YES\n id : 70 value: YES\n id : 64 value: YES\n id : 13 value: YES\n id : 79 value: YES\n id : 122 value: YES\n id : 77 value: YES\n id : 39 value: YES\n id : 21 value: YES\n id : 119 value: YES\n id : 47 value: YES\n id : 150 value: YES\n id : 157 value: YES\n id : 68 value: YES\n id : 136 value: YES\n id : 209 value: YES\n id : 16 value: YES\n id : 177 value: YES\n id : 172 value: YES\n id : 190 value: YES\n id : 186 value: YES\n id : 239 value: YES\n id : 23 value: YES\n id : 260 value: YES\n id : 263 value: YES\n id : 229 value: YES\n id : 225 value: YES\nModel training Done!\nStart updating weights!\n id : 192 value: YES\n id : 87 value: YES\n id : 3 value: YES\n id : 229 value: YES\n id : 107 value: YES\n id : 190 value: YES\n id : 202 value: YES\n id : 23 value: YES\n id : 164 value: YES\n id : 196 value: YES\n id : 208 value: YES\n id : 255 value: YES\n id : 124 value: YES\n id : 86 value: YES\n id : 88 value: YES\n id : 224 value: YES\n id : 222 value: YES\n id : 253 value: YES\n id : 128 value: YES\n id : 271 value: YES\n id : 185 value: YES\n id : 160 value: YES\n id : 272 value: YES\n id : 220 value: YES\n id : 177 value: YES\n id : 239 value: YES\n id : 208 value: YES\n id : 137 value: YES\n id : 226 value: YES\n id : 24 value: YES\n id : 21 value: YES\n id : 174 value: YES\n id : 238 value: YES\n id : 199 value: YES\n id : 163 value: YES\n id : 123 value: YES\n id : 169 value: YES\n id : 75 value: YES\n id : 52 value: YES\n id : 74 value: YES\n id : 272 value: YES\n id : 28 value: YES\n id : 10 value: YES\n id : 147 value: YES\n id : 2 value: YES\n id : 140 value: YES\n id : 115 value: YES\n id : 204 value: YES\n id : 94 value: YES\n id : 263 value: YES\n id : 53 value: YES\n id : 209 value: YES\n id : 263 value: YES\n id : 253 value: YES\n id : 47 value: YES\n id : 30 value: YES\n id : 105 value: YES\n id : 147 value: YES\n id : 188 value: YES\n id : 145 value: YES\n id : 64 value: YES\n id : 110 value: YES\n id : 44 value: YES\n id : 27 value: YES\n id : 98 value: YES\n id : 236 value: YES\n id : 264 value: YES\n id : 22 value: YES\n id : 192 value: YES\n id : 86 value: YES\n id : 75 value: YES\n id : 84 value: YES\n id : 220 value: YES\n id : 253 value: YES\n id : 68 value: YES\n id : 259 value: YES\n id : 151 value: YES\n id : 82 value: YES\n id : 259 value: YES\n id : 90 value: YES\n id : 199 value: YES\n id : 239 value: YES\n id : 185 value: YES\n id : 264 value: YES\n id : 53 value: YES\n id : 224 value: YES\n id : 107 value: YES\n id : 221 value: YES\n id : 10 value: YES\n id : 168 value: YES\n id : 82 value: YES\n id : 233 value: YES\n id : 145 value: YES\n id : 14 value: YES\n id : 177 value: YES\n id : 44 value: YES\n id : 157 value: YES\n id : 235 value: YES\n id : 156 value: YES\n id : 69 value: YES\n id : 4 value: YES\n id : 88 value: YES\n id : 105 value: YES\n id : 51 value: YES\n id : 184 value: YES\n id : 231 value: YES\n id : 229 value: YES\n id : 203 value: YES\n id : 102 value: YES\n id : 170 value: YES\n id : 222 value: YES\n id : 44 value: YES\n id : 90 value: YES\n id : 145 value: YES\n id : 243 value: YES\n id : 248 value: YES\n id : 174 value: YES\n id : 105 value: YES\n id : 24 value: YES\n id : 59 value: YES\n id : 132 value: YES\n id : 191 value: YES\n id : 157 value: YES\n id : 215 value: YES\n id : 24 value: YES\n id : 88 value: YES\n id : 70 value: YES\n id : 271 value: YES\n id : 255 value: YES\n id : 81 value: YES\n id : 141 value: YES\n id : 24 value: YES\n id : 100 value: YES\n id : 174 value: YES\n id : 226 value: YES\n id : 236 value: YES\n id : 187 value: YES\n id : 87 value: YES\n id : 267 value: YES\n id : 28 value: YES\n id : 82 value: YES\n id : 21 value: YES\n id : 86 value: YES\n id : 220 value: YES\n id : 41 value: YES\n id : 202 value: YES\n id : 179 value: YES\n id : 178 value: YES\n id : 36 value: YES\n id : 274 value: YES\n id : 51 value: YES\n id : 262 value: YES\n id : 49 value: YES\n id : 116 value: YES\n id : 61 value: YES\n id : 227 value: YES\n id : 264 value: YES\n id : 64 value: YES\n id : 190 value: YES\n id : 232 value: YES\n id : 220 value: YES\n id : 213 value: YES\n id : 19 value: YES\n id : 111 value: YES\n id : 183 value: YES\n id : 69 value: YES\n id : 196 value: YES\n id : 129 value: YES\n id : 156 value: YES\n id : 206 value: YES\n id : 264 value: YES\n id : 95 value: YES\n id : 204 value: YES\n id : 156 value: YES\n id : 203 value: YES\n id : 189 value: YES\n id : 262 value: YES\n id : 141 value: YES\n id : 197 value: YES\n id : 260 value: YES\n id : 151 value: YES\n id : 226 value: YES\n id : 91 value: YES\n id : 75 value: YES\n id : 253 value: YES\n id : 43 value: YES\n id : 217 value: YES\n id : 128 value: YES\n id : 84 value: YES\n id : 262 value: YES\n id : 188 value: YES\n id : 224 value: YES\n id : 45 value: YES\n id : 163 value: YES\n id : 91 value: YES\n id : 124 value: YES\n id : 221 value: YES\n id : 107 value: YES\n id : 87 value: YES\n id : 184 value: YES\n id : 165 value: YES\n id : 2 value: YES\n id : 160 value: YES\n id : 100 value: YES\n id : 263 value: YES\n id : 23 value: YES\n id : 106 value: YES\n id : 51 value: YES\n id : 59 value: YES\n id : 39 value: YES\n id : 116 value: YES\n id : 228 value: YES\n id : 39 value: YES\n id : 239 value: YES\n id : 259 value: YES\n id : 226 value: YES\n id : 165 value: YES\n id : 187 value: YES\n id : 23 value: YES\n id : 6 value: YES\n id : 13 value: YES\n id : 141 value: YES\n id : 233 value: YES\n id : 4 value: YES\n id : 185 value: YES\n id : 87 value: YES\n id : 262 value: YES\n id : 240 value: YES\n id : 4 value: YES\n id : 54 value: YES\n id : 2 value: YES\n id : 172 value: YES\n id : 226 value: YES\n id : 81 value: YES\n id : 19 value: YES\n id : 105 value: YES\n id : 233 value: YES\n id : 163 value: YES\n id : 128 value: YES\n id : 188 value: YES\n id : 46 value: YES\n id : 55 value: YES\n id : 122 value: YES\n id : 72 value: YES\n id : 28 value: YES\n id : 215 value: YES\n id : 69 value: YES\n id : 228 value: YES\n id : 253 value: YES\n id : 177 value: YES\n id : 225 value: YES\n id : 274 value: YES\n id : 111 value: YES\n id : 177 value: YES\n id : 38 value: YES\n id : 221 value: YES\n id : 3 value: YES\n id : 54 value: YES\n id : 211 value: YES\n id : 89 value: YES\n id : 209 value: YES\n id : 217 value: YES\n id : 221 value: YES\n id : 105 value: YES\n id : 22 value: YES\n id : 169 value: YES\n id : 218 value: YES\n id : 79 value: YES\n id : 44 value: YES\n id : 235 value: YES\n id : 105 value: YES\n id : 238 value: YES\n id : 142 value: YES\n id : 68 value: YES\n id : 153 value: YES\nModel training Done!\nStart updating weights!\n id : 47 value: YES\n id : 52 value: YES\n id : 225 value: YES\n id : 163 value: YES\n id : 36 value: YES\n id : 36 value: YES\n id : 272 value: YES\n id : 195 value: YES\n id : 274 value: YES\n id : 39 value: YES\n id : 165 value: YES\n id : 94 value: YES\n id : 72 value: YES\n id : 45 value: YES\n id : 168 value: YES\n id : 51 value: YES\n id : 64 value: YES\n id : 54 value: YES\n id : 100 value: YES\n id : 30 value: YES\n id : 137 value: YES\n id : 28 value: YES\n id : 188 value: YES\n id : 4 value: YES\n id : 112 value: YES\n id : 94 value: YES\n id : 55 value: YES\n id : 27 value: YES\n id : 95 value: YES\n id : 255 value: YES\n id : 165 value: YES\n id : 80 value: YES\n id : 98 value: YES\n id : 91 value: YES\n id : 123 value: YES\n id : 209 value: YES\n id : 54 value: YES\n id : 179 value: YES\n id : 10 value: YES\n id : 21 value: YES\n id : 184 value: YES\n id : 174 value: YES\n id : 52 value: YES\n id : 217 value: YES\n id : 199 value: YES\n id : 54 value: YES\n id : 156 value: YES\n id : 188 value: YES\n id : 271 value: YES\n id : 105 value: YES\n id : 199 value: YES\n id : 208 value: YES\n id : 211 value: YES\n id : 13 value: YES\n id : 13 value: YES\n id : 6 value: YES\n id : 36 value: YES\n id : 52 value: YES\n id : 74 value: YES\n id : 178 value: YES\n id : 122 value: YES\n id : 271 value: YES\n id : 199 value: YES\n id : 91 value: YES\n id : 199 value: YES\n id : 107 value: YES\n id : 19 value: YES\n id : 111 value: YES\n id : 86 value: YES\n id : 203 value: YES\n id : 61 value: YES\n id : 41 value: YES\n id : 206 value: YES\n id : 59 value: YES\n id : 229 value: YES\n id : 79 value: YES\n id : 189 value: YES\n id : 251 value: YES\n id : 16 value: YES\n id : 213 value: YES\n id : 228 value: YES\n id : 235 value: YES\n id : 168 value: YES\n id : 179 value: YES\n id : 226 value: YES\n id : 3 value: YES\n id : 206 value: YES\n id : 68 value: YES\n id : 197 value: YES\n id : 6 value: YES\n id : 157 value: YES\n id : 8 value: YES\n id : 217 value: YES\n id : 13 value: YES\n id : 221 value: YES\n id : 30 value: YES\n id : 145 value: YES\n id : 128 value: YES\n id : 272 value: YES\n id : 87 value: YES\n id : 151 value: YES\n id : 22 value: YES\n id : 115 value: YES\n id : 73 value: YES\n id : 208 value: YES\n id : 129 value: YES\n id : 65 value: YES\n id : 98 value: YES\n id : 206 value: YES\n id : 235 value: YES\n id : 84 value: YES\n id : 240 value: YES\n id : 115 value: YES\n id : 208 value: YES\n id : 178 value: YES\n id : 160 value: YES\n id : 74 value: YES\n id : 87 value: YES\n id : 82 value: YES\n id : 40 value: YES\n id : 206 value: YES\n id : 111 value: YES\n id : 41 value: YES\n id : 94 value: YES\n id : 90 value: YES\n id : 160 value: YES\n id : 115 value: YES\n id : 215 value: YES\n id : 105 value: YES\n id : 156 value: YES\n id : 177 value: YES\n id : 260 value: YES\n id : 2 value: YES\n id : 196 value: YES\n id : 188 value: YES\n id : 10 value: YES\n id : 220 value: YES\n id : 233 value: YES\n id : 4 value: YES\n id : 38 value: YES\n id : 28 value: YES\n id : 145 value: YES\n id : 188 value: YES\n id : 257 value: YES\n id : 253 value: YES\n id : 163 value: YES\n id : 257 value: YES\n id : 43 value: YES\n id : 231 value: YES\n id : 271 value: YES\n id : 141 value: YES\n id : 226 value: YES\n id : 225 value: YES\n id : 140 value: YES\n id : 184 value: YES\n id : 235 value: YES\n id : 74 value: YES\n id : 216 value: YES\n id : 145 value: YES\n id : 208 value: YES\n id : 21 value: YES\n id : 204 value: YES\n id : 47 value: YES\n id : 30 value: YES\n id : 98 value: YES\n id : 22 value: YES\n id : 108 value: YES\n id : 163 value: YES\n id : 191 value: YES\n id : 170 value: YES\n id : 87 value: YES\n id : 6 value: YES\n id : 74 value: YES\n id : 124 value: YES\n id : 75 value: YES\n id : 28 value: YES\n id : 38 value: YES\n id : 102 value: YES\n id : 208 value: YES\n id : 174 value: YES\n id : 75 value: YES\n id : 30 value: YES\n id : 238 value: YES\n id : 260 value: YES\n id : 235 value: YES\n id : 183 value: YES\n id : 55 value: YES\n id : 82 value: YES\n id : 184 value: YES\n id : 236 value: YES\n id : 102 value: YES\n id : 238 value: YES\n id : 185 value: YES\n id : 74 value: YES\n id : 110 value: YES\n id : 196 value: YES\n id : 260 value: YES\n id : 272 value: YES\n id : 243 value: YES\n id : 19 value: YES\n id : 37 value: YES\n id : 47 value: YES\n id : 163 value: YES\n id : 61 value: YES\n id : 157 value: YES\n id : 208 value: YES\n id : 36 value: YES\n id : 55 value: YES\n id : 264 value: YES\n id : 199 value: YES\n id : 236 value: YES\n id : 255 value: YES\n id : 116 value: YES\n id : 15 value: YES\n id : 218 value: YES\n id : 98 value: YES\n id : 69 value: YES\n id : 271 value: YES\n id : 74 value: YES\n id : 160 value: YES\n id : 219 value: YES\n id : 45 value: YES\n id : 105 value: YES\n id : 100 value: YES\n id : 179 value: YES\n id : 55 value: YES\n id : 98 value: YES\n id : 90 value: YES\n id : 88 value: YES\n id : 147 value: YES\n id : 129 value: YES\n id : 16 value: YES\n id : 271 value: YES\n id : 82 value: YES\n id : 147 value: YES\n id : 38 value: YES\n id : 10 value: YES\n id : 208 value: YES\n id : 203 value: YES\n id : 124 value: YES\n id : 218 value: YES\n id : 211 value: YES\n id : 100 value: YES\n id : 10 value: YES\n id : 240 value: YES\n id : 274 value: YES\n id : 81 value: YES\n id : 165 value: YES\n id : 199 value: YES\n id : 147 value: YES\n id : 69 value: YES\n id : 178 value: YES\n id : 163 value: YES\n id : 227 value: YES\n id : 43 value: YES\n id : 271 value: YES\n id : 137 value: YES\n id : 123 value: YES\n id : 183 value: YES\n id : 4 value: YES\n id : 204 value: YES\n id : 4 value: YES\n id : 189 value: YES\n id : 184 value: YES\n id : 227 value: YES\n id : 154 value: YES\n id : 100 value: YES\n id : 1 value: YES\n id : 172 value: YES\n id : 145 value: YES\n id : 186 value: YES\n id : 61 value: YES\n id : 191 value: YES\n id : 151 value: YES\n id : 235 value: YES\nModel training Done!\nStart updating weights!\n id : 147 value: YES\n id : 72 value: YES\n id : 140 value: YES\n id : 271 value: YES\n id : 16 value: YES\n id : 28 value: YES\n id : 61 value: YES\n id : 10 value: YES\n id : 219 value: YES\n id : 189 value: YES\n id : 199 value: YES\n id : 240 value: YES\n id : 183 value: YES\n id : 220 value: YES\n id : 3 value: YES\n id : 174 value: YES\n id : 220 value: YES\n id : 6 value: YES\n id : 209 value: YES\n id : 115 value: YES\n id : 260 value: YES\n id : 100 value: YES\n id : 183 value: YES\n id : 184 value: YES\n id : 52 value: YES\n id : 82 value: YES\n id : 100 value: YES\n id : 43 value: YES\n id : 233 value: YES\n id : 236 value: YES\n id : 79 value: YES\n id : 38 value: YES\n id : 165 value: YES\n id : 165 value: YES\n id : 79 value: YES\n id : 218 value: YES\n id : 38 value: YES\n id : 54 value: YES\n id : 91 value: YES\n id : 174 value: YES\n id : 128 value: YES\n id : 199 value: YES\n id : 90 value: YES\n id : 13 value: YES\n id : 6 value: YES\n id : 30 value: YES\n id : 163 value: YES\n id : 174 value: YES\n id : 264 value: YES\n id : 81 value: YES\n id : 206 value: YES\n id : 123 value: YES\n id : 72 value: YES\n id : 64 value: YES\n id : 160 value: YES\n id : 156 value: YES\n id : 111 value: YES\n id : 51 value: YES\n id : 274 value: YES\n id : 59 value: YES\n id : 13 value: YES\n id : 64 value: YES\n id : 272 value: YES\n id : 24 value: YES\n id : 105 value: YES\n id : 95 value: YES\n id : 59 value: YES\n id : 111 value: YES\n id : 160 value: YES\n id : 13 value: YES\n id : 81 value: YES\n id : 157 value: YES\n id : 218 value: YES\n id : 272 value: YES\n id : 185 value: YES\n id : 226 value: YES\n id : 217 value: YES\n id : 188 value: YES\n id : 118 value: YES\n id : 75 value: YES\n id : 225 value: YES\n id : 82 value: YES\n id : 27 value: YES\n id : 206 value: YES\n id : 91 value: YES\n id : 211 value: YES\n id : 105 value: YES\n id : 86 value: YES\n id : 189 value: YES\n id : 82 value: YES\n id : 264 value: YES\n id : 253 value: YES\n id : 39 value: YES\n id : 188 value: YES\n id : 226 value: YES\n id : 87 value: YES\n id : 91 value: YES\n id : 13 value: YES\n id : 19 value: YES\n id : 110 value: YES\n id : 36 value: YES\n id : 54 value: YES\n id : 196 value: YES\n id : 91 value: YES\n id : 255 value: YES\n id : 235 value: YES\n id : 191 value: YES\n id : 102 value: YES\n id : 184 value: YES\n id : 28 value: YES\n id : 188 value: YES\n id : 21 value: YES\n id : 140 value: YES\n id : 235 value: YES\n id : 206 value: YES\n id : 21 value: YES\n id : 160 value: YES\n id : 271 value: YES\n id : 28 value: YES\n id : 28 value: YES\n id : 64 value: YES\n id : 211 value: YES\n id : 203 value: YES\n id : 255 value: YES\n id : 240 value: YES\n id : 147 value: YES\n id : 69 value: YES\n id : 90 value: YES\n id : 100 value: YES\n id : 84 value: YES\n id : 220 value: YES\n id : 272 value: YES\n id : 59 value: YES\n id : 88 value: YES\n id : 105 value: YES\n id : 45 value: YES\n id : 105 value: YES\n id : 145 value: YES\n id : 94 value: YES\n id : 233 value: YES\n id : 208 value: YES\n id : 107 value: YES\n id : 111 value: YES\n id : 141 value: YES\n id : 236 value: YES\n id : 81 value: YES\n id : 84 value: YES\n id : 128 value: YES\n id : 209 value: YES\n id : 22 value: YES\n id : 30 value: YES\n id : 141 value: YES\n id : 211 value: YES\n id : 4 value: YES\n id : 218 value: YES\n id : 196 value: YES\n id : 218 value: YES\n id : 150 value: YES\n id : 129 value: YES\n id : 235 value: YES\n id : 137 value: YES\n id : 235 value: YES\n id : 10 value: YES\n id : 52 value: YES\n id : 221 value: YES\n id : 24 value: YES\n id : 28 value: YES\n id : 19 value: YES\n id : 151 value: YES\n id : 95 value: YES\n id : 123 value: YES\n id : 74 value: YES\n id : 263 value: YES\n id : 27 value: YES\n id : 19 value: YES\n id : 82 value: YES\n id : 90 value: YES\n id : 140 value: YES\n id : 122 value: YES\n id : 188 value: YES\n id : 229 value: YES\n id : 124 value: YES\n id : 165 value: YES\n id : 13 value: YES\n id : 129 value: YES\n id : 27 value: YES\n id : 226 value: YES\n id : 129 value: YES\n id : 185 value: YES\n id : 69 value: YES\n id : 235 value: YES\n id : 204 value: YES\n id : 13 value: YES\n id : 4 value: YES\n id : 105 value: YES\n id : 22 value: YES\n id : 272 value: YES\n id : 105 value: YES\n id : 39 value: YES\n id : 107 value: YES\n id : 43 value: YES\n id : 128 value: YES\n id : 209 value: YES\n id : 271 value: YES\n id : 91 value: YES\n id : 208 value: YES\n id : 21 value: YES\n id : 178 value: YES\n id : 45 value: YES\n id : 228 value: YES\n id : 69 value: YES\n id : 59 value: YES\n id : 61 value: YES\n id : 94 value: YES\n id : 45 value: YES\n id : 225 value: YES\n id : 87 value: YES\n id : 3 value: YES\n id : 45 value: YES\n id : 151 value: YES\n id : 177 value: YES\n id : 213 value: YES\n id : 115 value: YES\n id : 10 value: YES\n id : 236 value: YES\n id : 82 value: YES\n id : 124 value: YES\n id : 52 value: YES\n id : 184 value: YES\n id : 54 value: YES\n id : 137 value: YES\n id : 82 value: YES\n id : 240 value: YES\n id : 111 value: YES\n id : 238 value: YES\n id : 236 value: YES\n id : 184 value: YES\n id : 36 value: YES\n id : 115 value: YES\n id : 128 value: YES\n id : 213 value: YES\n id : 27 value: YES\n id : 94 value: YES\n id : 240 value: YES\n id : 190 value: YES\n id : 174 value: YES\n id : 4 value: YES\n id : 191 value: YES\n id : 147 value: YES\n id : 260 value: YES\n id : 221 value: YES\n id : 100 value: YES\n id : 227 value: YES\n id : 191 value: YES\n id : 160 value: YES\n id : 36 value: YES\n id : 91 value: YES\n id : 39 value: YES\n id : 68 value: YES\n id : 82 value: YES\n id : 233 value: YES\n id : 36 value: YES\n id : 213 value: YES\n id : 264 value: YES\n id : 219 value: YES\n id : 38 value: YES\n id : 13 value: YES\n id : 88 value: YES\n id : 227 value: YES\n id : 231 value: YES\n id : 172 value: YES\n id : 229 value: YES\n id : 75 value: YES\n id : 6 value: YES\n id : 153 value: YES\nModel training Done!\nStart updating weights!\n id : 122 value: YES\n id : 260 value: YES\n id : 219 value: YES\n id : 69 value: YES\n id : 218 value: YES\n id : 272 value: YES\n id : 215 value: YES\n id : 220 value: YES\n id : 105 value: YES\n id : 43 value: YES\n id : 86 value: YES\n id : 151 value: YES\n id : 190 value: YES\n id : 211 value: YES\n id : 204 value: YES\n id : 122 value: YES\n id : 111 value: YES\n id : 231 value: YES\n id : 172 value: YES\n id : 213 value: YES\n id : 172 value: YES\n id : 209 value: YES\n id : 151 value: YES\n id : 253 value: YES\n id : 69 value: YES\n id : 100 value: YES\n id : 221 value: YES\n id : 102 value: YES\n id : 151 value: YES\n id : 177 value: YES\n id : 211 value: YES\n id : 68 value: YES\n id : 61 value: YES\n id : 186 value: YES\n id : 274 value: YES\n id : 255 value: YES\n id : 87 value: YES\n id : 145 value: YES\n id : 137 value: YES\n id : 185 value: YES\n id : 141 value: YES\n id : 255 value: YES\n id : 238 value: YES\n id : 22 value: YES\n id : 91 value: YES\n id : 235 value: YES\n id : 129 value: YES\n id : 45 value: YES\n id : 188 value: YES\n id : 183 value: YES\n id : 225 value: YES\n id : 217 value: YES\n id : 235 value: YES\n id : 141 value: YES\n id : 3 value: YES\n id : 220 value: YES\n id : 157 value: YES\n id : 195 value: YES\n id : 253 value: YES\n id : 238 value: YES\n id : 128 value: YES\n id : 54 value: YES\n id : 160 value: YES\n id : 203 value: YES\n id : 271 value: YES\n id : 185 value: YES\n id : 226 value: YES\n id : 189 value: YES\n id : 52 value: YES\n id : 255 value: YES\n id : 27 value: YES\n id : 229 value: YES\n id : 105 value: YES\n id : 238 value: YES\n id : 172 value: YES\n id : 22 value: YES\n id : 115 value: YES\n id : 128 value: YES\n id : 129 value: YES\n id : 235 value: YES\n id : 59 value: YES\n id : 102 value: YES\n id : 81 value: YES\n id : 21 value: YES\n id : 75 value: YES\n id : 87 value: YES\n id : 74 value: YES\n id : 217 value: YES\n id : 229 value: YES\n id : 95 value: YES\n id : 10 value: YES\n id : 55 value: YES\n id : 160 value: YES\n id : 236 value: YES\n id : 157 value: YES\n id : 218 value: YES\n id : 100 value: YES\n id : 271 value: YES\n id : 102 value: YES\n id : 51 value: YES\n id : 86 value: YES\n id : 115 value: YES\n id : 10 value: YES\n id : 6 value: YES\n id : 151 value: YES\n id : 203 value: YES\n id : 272 value: YES\n id : 183 value: YES\n id : 44 value: YES\n id : 185 value: YES\n id : 240 value: YES\n id : 98 value: YES\n id : 6 value: YES\n id : 257 value: YES\n id : 178 value: YES\n id : 86 value: YES\n id : 38 value: YES\n id : 45 value: YES\n id : 220 value: YES\n id : 264 value: YES\n id : 140 value: YES\n id : 45 value: YES\n id : 174 value: YES\n id : 160 value: YES\n id : 177 value: YES\n id : 75 value: YES\n id : 191 value: YES\n id : 4 value: YES\n id : 115 value: YES\n id : 54 value: YES\n id : 209 value: YES\n id : 228 value: YES\n id : 228 value: YES\n id : 90 value: YES\n id : 185 value: YES\n id : 220 value: YES\n id : 141 value: YES\n id : 253 value: YES\n id : 128 value: YES\n id : 28 value: YES\n id : 137 value: YES\n id : 233 value: YES\n id : 22 value: YES\n id : 231 value: YES\n id : 128 value: YES\n id : 228 value: YES\n id : 218 value: YES\n id : 74 value: YES\n id : 90 value: YES\n id : 91 value: YES\n id : 157 value: YES\n id : 45 value: YES\n id : 59 value: YES\n id : 123 value: YES\n id : 122 value: YES\n id : 157 value: YES\n id : 75 value: YES\n id : 217 value: YES\n id : 229 value: YES\n id : 185 value: YES\n id : 111 value: YES\n id : 157 value: YES\n id : 211 value: YES\n id : 219 value: YES\n id : 240 value: YES\n id : 178 value: YES\n id : 140 value: YES\n id : 82 value: YES\n id : 64 value: YES\n id : 79 value: YES\n id : 39 value: YES\n id : 178 value: YES\n id : 129 value: YES\n id : 160 value: YES\n id : 213 value: YES\n id : 39 value: YES\n id : 231 value: YES\n id : 59 value: YES\n id : 156 value: YES\n id : 255 value: YES\n id : 68 value: YES\n id : 253 value: YES\n id : 55 value: YES\n id : 72 value: YES\n id : 208 value: YES\n id : 226 value: YES\n id : 45 value: YES\n id : 74 value: YES\n id : 253 value: YES\n id : 264 value: YES\n id : 240 value: YES\n id : 122 value: YES\n id : 28 value: YES\n id : 160 value: YES\n id : 229 value: YES\n id : 260 value: YES\n id : 178 value: YES\n id : 177 value: YES\n id : 59 value: YES\n id : 81 value: YES\n id : 199 value: YES\n id : 100 value: YES\n id : 122 value: YES\n id : 4 value: YES\n id : 52 value: YES\n id : 163 value: YES\n id : 199 value: YES\n id : 248 value: YES\n id : 74 value: YES\n id : 107 value: YES\n id : 82 value: YES\n id : 206 value: YES\n id : 88 value: YES\n id : 30 value: YES\n id : 233 value: YES\n id : 238 value: YES\n id : 61 value: YES\n id : 88 value: YES\n id : 228 value: YES\n id : 208 value: YES\n id : 69 value: YES\n id : 6 value: YES\n id : 51 value: YES\n id : 260 value: YES\n id : 177 value: YES\n id : 4 value: YES\n id : 98 value: YES\n id : 64 value: YES\n id : 105 value: YES\n id : 217 value: YES\n id : 233 value: YES\n id : 84 value: YES\n id : 184 value: YES\n id : 115 value: YES\n id : 233 value: YES\n id : 51 value: YES\n id : 188 value: YES\n id : 64 value: YES\n id : 87 value: YES\n id : 188 value: YES\n id : 38 value: YES\n id : 4 value: YES\n id : 147 value: YES\n id : 79 value: YES\n id : 3 value: YES\n id : 27 value: YES\n id : 227 value: YES\n id : 172 value: YES\n id : 163 value: YES\n id : 160 value: YES\n id : 260 value: YES\n id : 221 value: YES\n id : 236 value: YES\n id : 21 value: YES\n id : 21 value: YES\n id : 184 value: YES\n id : 68 value: YES\n id : 3 value: YES\n id : 74 value: YES\n id : 51 value: YES\n id : 264 value: YES\n id : 79 value: YES\n id : 38 value: YES\n id : 147 value: YES\n id : 145 value: YES\n id : 52 value: YES\n id : 82 value: YES\n id : 51 value: YES\n id : 137 value: YES\n id : 255 value: YES\n id : 45 value: YES\n id : 43 value: YES\n id : 238 value: YES\n id : 28 value: YES\n id : 172 value: YES\nModel training Done!\nStart updating weights!\n id : 110 value: YES\n id : 233 value: YES\n id : 231 value: YES\n id : 271 value: YES\n id : 213 value: YES\n id : 79 value: YES\n id : 240 value: YES\n id : 218 value: YES\n id : 43 value: YES\n id : 213 value: YES\n id : 238 value: YES\n id : 172 value: YES\n id : 105 value: YES\n id : 264 value: YES\n id : 191 value: YES\n id : 233 value: YES\n id : 28 value: YES\n id : 163 value: YES\n id : 115 value: YES\n id : 189 value: YES\n id : 105 value: YES\n id : 211 value: YES\n id : 30 value: YES\n id : 122 value: YES\n id : 10 value: YES\n id : 123 value: YES\n id : 68 value: YES\n id : 111 value: YES\n id : 160 value: YES\n id : 90 value: YES\n id : 27 value: YES\n id : 233 value: YES\n id : 81 value: YES\n id : 156 value: YES\n id : 274 value: YES\n id : 260 value: YES\n id : 68 value: YES\n id : 75 value: YES\n id : 54 value: YES\n id : 107 value: YES\n id : 163 value: YES\n id : 64 value: YES\n id : 45 value: YES\n id : 199 value: YES\n id : 140 value: YES\n id : 129 value: YES\n id : 43 value: YES\n id : 38 value: YES\n id : 191 value: YES\n id : 75 value: YES\n id : 238 value: YES\n id : 191 value: YES\n id : 195 value: YES\n id : 225 value: YES\n id : 45 value: YES\n id : 255 value: YES\n id : 30 value: YES\n id : 191 value: YES\n id : 226 value: YES\n id : 91 value: YES\n id : 228 value: YES\n id : 220 value: YES\n id : 28 value: YES\n id : 102 value: YES\n id : 21 value: YES\n id : 238 value: YES\n id : 51 value: YES\n id : 225 value: YES\n id : 264 value: YES\n id : 3 value: YES\n id : 91 value: YES\n id : 111 value: YES\n id : 213 value: YES\n id : 199 value: YES\n id : 185 value: YES\n id : 128 value: YES\n id : 107 value: YES\n id : 88 value: YES\n id : 208 value: YES\n id : 52 value: YES\n id : 22 value: YES\n id : 226 value: YES\n id : 30 value: YES\n id : 36 value: YES\n id : 88 value: YES\n id : 55 value: YES\n id : 10 value: YES\n id : 199 value: YES\n id : 100 value: YES\n id : 72 value: YES\n id : 86 value: YES\n id : 157 value: YES\n id : 13 value: YES\n id : 156 value: YES\n id : 160 value: YES\n id : 30 value: YES\n id : 88 value: YES\n id : 235 value: YES\n id : 145 value: YES\n id : 90 value: YES\n id : 74 value: YES\n id : 54 value: YES\n id : 231 value: YES\n id : 189 value: YES\n id : 211 value: YES\n id : 82 value: YES\n id : 147 value: YES\n id : 217 value: YES\n id : 156 value: YES\n id : 55 value: YES\n id : 218 value: YES\n id : 79 value: YES\n id : 21 value: YES\n id : 107 value: YES\n id : 87 value: YES\n id : 6 value: YES\n id : 231 value: YES\n id : 203 value: YES\n id : 107 value: YES\n id : 4 value: YES\n id : 199 value: YES\n id : 229 value: YES\n id : 231 value: YES\n id : 82 value: YES\n id : 160 value: YES\n id : 157 value: YES\n id : 105 value: YES\n id : 147 value: YES\n id : 188 value: YES\n id : 21 value: YES\n id : 229 value: YES\n id : 190 value: YES\n id : 236 value: YES\n id : 229 value: YES\n id : 45 value: YES\n id : 177 value: YES\n id : 217 value: YES\n id : 75 value: YES\n id : 157 value: YES\n id : 184 value: YES\n id : 79 value: YES\n id : 184 value: YES\n id : 81 value: YES\n id : 229 value: YES\n id : 184 value: YES\n id : 74 value: YES\n id : 95 value: YES\n id : 206 value: YES\n id : 102 value: YES\n id : 174 value: YES\n id : 151 value: YES\n id : 41 value: YES\n id : 213 value: YES\n id : 264 value: YES\n id : 189 value: YES\n id : 123 value: YES\n id : 90 value: YES\n id : 233 value: YES\n id : 157 value: YES\n id : 79 value: YES\n id : 260 value: YES\n id : 22 value: YES\n id : 147 value: YES\n id : 87 value: YES\n id : 115 value: YES\n id : 27 value: YES\n id : 156 value: YES\n id : 229 value: YES\n id : 213 value: YES\n id : 160 value: YES\n id : 75 value: YES\n id : 226 value: YES\n id : 21 value: YES\n id : 55 value: YES\n id : 264 value: YES\n id : 255 value: YES\n id : 188 value: YES\n id : 6 value: YES\n id : 156 value: YES\n id : 163 value: YES\n id : 229 value: YES\n id : 10 value: YES\n id : 140 value: YES\n id : 238 value: YES\n id : 3 value: YES\n id : 123 value: YES\n id : 128 value: YES\n id : 151 value: YES\n id : 105 value: YES\n id : 213 value: YES\n id : 111 value: YES\n id : 45 value: YES\n id : 178 value: YES\n id : 225 value: YES\n id : 163 value: YES\n id : 209 value: YES\n id : 3 value: YES\n id : 227 value: YES\n id : 177 value: YES\n id : 227 value: YES\n id : 107 value: YES\n id : 185 value: YES\n id : 163 value: YES\n id : 219 value: YES\n id : 51 value: YES\n id : 229 value: YES\n id : 3 value: YES\n id : 157 value: YES\n id : 84 value: YES\n id : 238 value: YES\n id : 233 value: YES\n id : 218 value: YES\n id : 145 value: YES\n id : 115 value: YES\n id : 98 value: YES\n id : 218 value: YES\n id : 115 value: YES\n id : 28 value: YES\n id : 90 value: YES\n id : 30 value: YES\n id : 52 value: YES\n id : 128 value: YES\n id : 61 value: YES\n id : 264 value: YES\n id : 128 value: YES\n id : 54 value: YES\n id : 128 value: YES\n id : 3 value: YES\n id : 260 value: YES\n id : 209 value: YES\n id : 55 value: YES\n id : 236 value: YES\n id : 68 value: YES\n id : 79 value: YES\n id : 51 value: YES\n id : 160 value: YES\n id : 3 value: YES\n id : 204 value: YES\n id : 19 value: YES\n id : 74 value: YES\n id : 236 value: YES\n id : 141 value: YES\n id : 51 value: YES\n id : 229 value: YES\n id : 204 value: YES\n id : 140 value: YES\n id : 177 value: YES\n id : 178 value: YES\n id : 54 value: YES\n id : 3 value: YES\n id : 163 value: YES\n id : 28 value: YES\n id : 177 value: YES\n id : 236 value: YES\n id : 191 value: YES\n id : 10 value: YES\n id : 28 value: YES\n id : 64 value: YES\n id : 79 value: YES\n id : 174 value: YES\n id : 51 value: YES\n id : 111 value: YES\n id : 253 value: YES\n id : 203 value: YES\n id : 6 value: YES\n id : 90 value: YES\n id : 240 value: YES\n id : 264 value: YES\n id : 253 value: YES\n id : 217 value: YES\n id : 157 value: YES\n id : 38 value: YES\n id : 227 value: YES\n id : 75 value: YES\n id : 145 value: YES\nModel training Done!\nStart updating weights!\n id : 189 value: YES\n id : 226 value: YES\n id : 160 value: YES\n id : 52 value: YES\n id : 235 value: YES\n id : 235 value: YES\n id : 185 value: YES\n id : 45 value: YES\n id : 209 value: YES\n id : 189 value: YES\n id : 184 value: YES\n id : 100 value: YES\n id : 123 value: YES\n id : 160 value: YES\n id : 90 value: YES\n id : 255 value: YES\n id : 191 value: YES\n id : 178 value: YES\n id : 95 value: YES\n id : 157 value: YES\n id : 184 value: YES\n id : 185 value: YES\n id : 156 value: YES\n id : 183 value: YES\n id : 240 value: YES\n id : 157 value: YES\n id : 79 value: YES\n id : 140 value: YES\n id : 107 value: YES\n id : 123 value: YES\n id : 204 value: YES\n id : 204 value: YES\n id : 145 value: YES\n id : 157 value: YES\n id : 208 value: YES\n id : 235 value: YES\n id : 129 value: YES\n id : 174 value: YES\n id : 87 value: YES\n id : 217 value: YES\n id : 203 value: YES\n id : 220 value: YES\n id : 122 value: YES\n id : 82 value: YES\n id : 115 value: YES\n id : 55 value: YES\n id : 86 value: YES\n id : 255 value: YES\n id : 72 value: YES\n id : 10 value: YES\n id : 189 value: YES\n id : 115 value: YES\n id : 255 value: YES\n id : 64 value: YES\n id : 225 value: YES\n id : 30 value: YES\n id : 145 value: YES\n id : 217 value: YES\n id : 271 value: YES\n id : 238 value: YES\n id : 141 value: YES\n id : 115 value: YES\n id : 191 value: YES\n id : 225 value: YES\n id : 255 value: YES\n id : 213 value: YES\n id : 91 value: YES\n id : 229 value: YES\n id : 177 value: YES\n id : 208 value: YES\n id : 209 value: YES\n id : 21 value: YES\n id : 84 value: YES\n id : 81 value: YES\n id : 100 value: YES\n id : 156 value: YES\n id : 52 value: YES\n id : 91 value: YES\n id : 160 value: YES\n id : 10 value: YES\n id : 54 value: YES\n id : 185 value: YES\n id : 10 value: YES\n id : 10 value: YES\n id : 156 value: YES\n id : 206 value: YES\n id : 189 value: YES\n id : 82 value: YES\n id : 220 value: YES\n id : 140 value: YES\n id : 123 value: YES\n id : 87 value: YES\n id : 233 value: YES\n id : 129 value: YES\n id : 174 value: YES\n id : 111 value: YES\n id : 27 value: YES\n id : 191 value: YES\n id : 220 value: YES\n id : 68 value: YES\n id : 45 value: YES\n id : 100 value: YES\n id : 220 value: YES\n id : 260 value: YES\n id : 74 value: YES\n id : 84 value: YES\n id : 13 value: YES\n id : 30 value: YES\n id : 6 value: YES\n id : 129 value: YES\n id : 217 value: YES\n id : 229 value: YES\n id : 86 value: YES\n id : 163 value: YES\n id : 82 value: YES\n id : 185 value: YES\n id : 105 value: YES\n id : 69 value: YES\n id : 217 value: YES\n id : 174 value: YES\n id : 220 value: YES\n id : 213 value: YES\n id : 226 value: YES\n id : 27 value: YES\n id : 95 value: YES\n id : 190 value: YES\n id : 233 value: YES\n id : 75 value: YES\n id : 253 value: YES\n id : 160 value: YES\n id : 22 value: YES\n id : 260 value: YES\n id : 28 value: YES\n id : 72 value: YES\n id : 30 value: YES\n id : 105 value: YES\n id : 185 value: YES\n id : 72 value: YES\n id : 79 value: YES\n id : 10 value: YES\n id : 6 value: YES\n id : 235 value: YES\n id : 74 value: YES\n id : 233 value: YES\n id : 28 value: YES\n id : 88 value: YES\n id : 115 value: YES\n id : 86 value: YES\n id : 177 value: YES\n id : 147 value: YES\n id : 122 value: YES\n id : 240 value: YES\n id : 190 value: YES\n id : 10 value: YES\n id : 226 value: YES\n id : 221 value: YES\n id : 233 value: YES\n id : 213 value: YES\n id : 253 value: YES\n id : 123 value: YES\n id : 111 value: YES\n id : 69 value: YES\n id : 38 value: YES\n id : 225 value: YES\n id : 272 value: YES\n id : 255 value: YES\n id : 191 value: YES\n id : 72 value: YES\n id : 255 value: YES\n id : 229 value: YES\n id : 172 value: YES\n id : 172 value: YES\n id : 199 value: YES\n id : 90 value: YES\n id : 64 value: YES\n id : 69 value: YES\n id : 156 value: YES\n id : 147 value: YES\n id : 157 value: YES\n id : 87 value: YES\n id : 208 value: YES\n id : 43 value: YES\n id : 199 value: YES\n id : 160 value: YES\n id : 86 value: YES\n id : 204 value: YES\n id : 64 value: YES\n id : 122 value: YES\n id : 228 value: YES\n id : 86 value: YES\n id : 13 value: YES\n id : 218 value: YES\n id : 84 value: YES\n id : 87 value: YES\n id : 221 value: YES\n id : 84 value: YES\n id : 228 value: YES\n id : 157 value: YES\n id : 52 value: YES\n id : 253 value: YES\n id : 238 value: YES\n id : 61 value: YES\n id : 204 value: YES\n id : 274 value: YES\n id : 95 value: YES\n id : 185 value: YES\n id : 229 value: YES\n id : 225 value: YES\n id : 145 value: YES\n id : 107 value: YES\n id : 156 value: YES\n id : 218 value: YES\n id : 54 value: YES\n id : 189 value: YES\n id : 4 value: YES\n id : 157 value: YES\n id : 156 value: YES\n id : 84 value: YES\n id : 72 value: YES\n id : 86 value: YES\n id : 30 value: YES\n id : 45 value: YES\n id : 274 value: YES\n id : 95 value: YES\n id : 74 value: YES\n id : 233 value: YES\n id : 61 value: YES\n id : 102 value: YES\n id : 59 value: YES\n id : 227 value: YES\n id : 87 value: YES\n id : 231 value: YES\n id : 229 value: YES\n id : 235 value: YES\n id : 203 value: YES\n id : 51 value: YES\n id : 3 value: YES\n id : 105 value: YES\n id : 72 value: YES\n id : 61 value: YES\n id : 68 value: YES\n id : 72 value: YES\n id : 10 value: YES\n id : 30 value: YES\n id : 90 value: YES\n id : 123 value: YES\n id : 204 value: YES\n id : 19 value: YES\n id : 218 value: YES\n id : 231 value: YES\n id : 10 value: YES\n id : 211 value: YES\n id : 6 value: YES\n id : 226 value: YES\n id : 22 value: YES\n id : 129 value: YES\n id : 191 value: YES\n id : 231 value: YES\n id : 91 value: YES\n id : 163 value: YES\n id : 203 value: YES\n id : 68 value: YES\n id : 84 value: YES\n id : 30 value: YES\n id : 3 value: YES\n id : 227 value: YES\n id : 45 value: YES\n id : 81 value: YES\n id : 68 value: YES\n id : 141 value: YES\n id : 52 value: YES\n id : 255 value: YES\n id : 274 value: YES\n id : 45 value: YES\n id : 64 value: YES\nModel training Done!\nStart updating weights!\n id : 4 value: YES\n id : 199 value: YES\n id : 10 value: YES\n id : 110 value: YES\n id : 185 value: YES\n id : 82 value: YES\n id : 157 value: YES\n id : 110 value: YES\n id : 87 value: YES\n id : 204 value: YES\n id : 75 value: YES\n id : 191 value: YES\n id : 145 value: YES\n id : 231 value: YES\n id : 160 value: YES\n id : 88 value: YES\n id : 21 value: YES\n id : 217 value: YES\n id : 228 value: YES\n id : 174 value: YES\n id : 54 value: YES\n id : 204 value: YES\n id : 218 value: YES\n id : 178 value: YES\n id : 217 value: YES\n id : 141 value: YES\n id : 147 value: YES\n id : 102 value: YES\n id : 72 value: YES\n id : 227 value: YES\n id : 129 value: YES\n id : 238 value: YES\n id : 111 value: YES\n id : 19 value: YES\n id : 45 value: YES\n id : 81 value: YES\n id : 172 value: YES\n id : 145 value: YES\n id : 68 value: YES\n id : 72 value: YES\n id : 225 value: YES\n id : 163 value: YES\n id : 64 value: YES\n id : 51 value: YES\n id : 10 value: YES\n id : 203 value: YES\n id : 199 value: YES\n id : 59 value: YES\n id : 82 value: YES\n id : 209 value: YES\n id : 228 value: YES\n id : 79 value: YES\n id : 91 value: YES\n id : 174 value: YES\n id : 91 value: YES\n id : 22 value: YES\n id : 107 value: YES\n id : 51 value: YES\n id : 81 value: YES\n id : 107 value: YES\n id : 209 value: YES\n id : 217 value: YES\n id : 88 value: YES\n id : 140 value: YES\n id : 72 value: YES\n id : 91 value: YES\n id : 79 value: YES\n id : 140 value: YES\n id : 6 value: YES\n id : 27 value: YES\n id : 4 value: YES\n id : 75 value: YES\n id : 10 value: YES\n id : 4 value: YES\n id : 189 value: YES\n id : 231 value: YES\n id : 88 value: YES\n id : 122 value: YES\n id : 107 value: YES\n id : 233 value: YES\n id : 233 value: YES\n id : 123 value: YES\n id : 115 value: YES\n id : 21 value: YES\n id : 185 value: YES\n id : 177 value: YES\n id : 102 value: YES\n id : 102 value: YES\n id : 274 value: YES\n id : 91 value: YES\n id : 271 value: YES\n id : 221 value: YES\n id : 227 value: YES\n id : 6 value: YES\n id : 82 value: YES\n id : 95 value: YES\n id : 274 value: YES\n id : 28 value: YES\n id : 123 value: YES\n id : 45 value: YES\n id : 163 value: YES\n id : 236 value: YES\n id : 64 value: YES\n id : 253 value: YES\n id : 4 value: YES\n id : 177 value: YES\n id : 84 value: YES\n id : 74 value: YES\n id : 185 value: YES\n id : 225 value: YES\n id : 160 value: YES\n id : 253 value: YES\n id : 115 value: YES\n id : 75 value: YES\n id : 52 value: YES\n id : 43 value: YES\n id : 231 value: YES\n id : 235 value: YES\n id : 30 value: YES\n id : 264 value: YES\n id : 206 value: YES\n id : 274 value: YES\n id : 91 value: YES\n id : 238 value: YES\n id : 172 value: YES\n id : 185 value: YES\n id : 64 value: YES\n id : 211 value: YES\n id : 28 value: YES\n id : 10 value: YES\n id : 86 value: YES\n id : 229 value: YES\n id : 235 value: YES\n id : 6 value: YES\n id : 163 value: YES\n id : 79 value: YES\n id : 129 value: YES\n id : 211 value: YES\n id : 82 value: YES\n id : 75 value: YES\n id : 10 value: YES\n id : 28 value: YES\n id : 220 value: YES\n id : 233 value: YES\n id : 95 value: YES\n id : 51 value: YES\n id : 209 value: YES\n id : 88 value: YES\n id : 204 value: YES\n id : 82 value: YES\n id : 45 value: YES\n id : 229 value: YES\n id : 240 value: YES\n id : 95 value: YES\n id : 45 value: YES\n id : 274 value: YES\n id : 72 value: YES\n id : 225 value: YES\n id : 43 value: YES\n id : 129 value: YES\n id : 87 value: YES\n id : 68 value: YES\n id : 87 value: YES\n id : 4 value: YES\n id : 191 value: YES\n id : 81 value: YES\n id : 4 value: YES\n id : 86 value: YES\n id : 45 value: YES\n id : 209 value: YES\n id : 95 value: YES\n id : 86 value: YES\n id : 208 value: YES\n id : 240 value: YES\n id : 145 value: YES\n id : 100 value: YES\n id : 4 value: YES\n id : 43 value: YES\n id : 217 value: YES\n id : 10 value: YES\n id : 172 value: YES\n id : 236 value: YES\n id : 51 value: YES\n id : 81 value: YES\n id : 86 value: YES\n id : 81 value: YES\n id : 271 value: YES\n id : 189 value: YES\n id : 209 value: YES\n id : 174 value: YES\n id : 59 value: YES\n id : 91 value: YES\n id : 115 value: YES\n id : 172 value: YES\n id : 79 value: YES\n id : 140 value: YES\n id : 4 value: YES\n id : 228 value: YES\n id : 30 value: YES\n id : 22 value: YES\n id : 208 value: YES\n id : 203 value: YES\n id : 184 value: YES\n id : 227 value: YES\n id : 145 value: YES\n id : 203 value: YES\n id : 218 value: YES\n id : 188 value: YES\n id : 21 value: YES\n id : 211 value: YES\n id : 51 value: YES\n id : 107 value: YES\n id : 211 value: YES\n id : 172 value: YES\n id : 54 value: YES\n id : 122 value: YES\n id : 87 value: YES\n id : 240 value: YES\n id : 84 value: YES\n id : 123 value: YES\n id : 231 value: YES\n id : 206 value: YES\n id : 163 value: YES\n id : 64 value: YES\n id : 208 value: YES\n id : 102 value: YES\n id : 228 value: YES\n id : 145 value: YES\n id : 227 value: YES\n id : 59 value: YES\n id : 225 value: YES\n id : 233 value: YES\n id : 79 value: YES\n id : 28 value: YES\n id : 213 value: YES\n id : 122 value: YES\n id : 30 value: YES\n id : 255 value: YES\n id : 91 value: YES\n id : 10 value: YES\n id : 157 value: YES\n id : 157 value: YES\n id : 225 value: YES\n id : 21 value: YES\n id : 107 value: YES\n id : 45 value: YES\n id : 140 value: YES\n id : 21 value: YES\n id : 238 value: YES\n id : 55 value: NO\n id : 81 value: YES\n id : 227 value: YES\n id : 228 value: YES\n id : 235 value: YES\n id : 271 value: YES\n id : 102 value: YES\n id : 226 value: YES\n id : 123 value: YES\n id : 27 value: YES\n id : 140 value: YES\n id : 145 value: YES\n id : 64 value: YES\n id : 45 value: YES\n id : 22 value: YES\n id : 123 value: YES\n id : 72 value: YES\n id : 54 value: YES\n id : 64 value: YES\n id : 189 value: YES\n id : 253 value: YES\n id : 203 value: YES\n id : 81 value: YES\n id : 229 value: YES\n id : 271 value: YES\n id : 231 value: YES\nModel training Done!\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1743750105335
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_weights)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[2.5798268603328642, 1.355877325060082, 1.7440529047542193, 1.7606545104640545, 1.4459289276104286, 1.3422684219743737, 1.4650970537150028, 1.94854436956064]\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1743750105558
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = MeanAveragePrecision()\n",
        "\n",
        "#images, _ = next(iter(val_loader))\n",
        "for model in models:\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "with torch.no_grad():\n",
        "    for idxs, images, targets in test_loader:\n",
        "                images = [img.to(device) for img in images]\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "                \"\"\"for img in images:\n",
        "                        preds1 = models[0](img.unsqueeze(0))\n",
        "                        print(preds1)\n",
        "                        preds2 = models[1](img.unsqueeze(0))\n",
        "                        preds3 = models[2](img.unsqueeze(0))\n",
        "                        preds4 = models[3](img.unsqueeze(0))\"\"\"\n",
        "                preds = []\n",
        "                for model in models:\n",
        "\n",
        "                    pred = model(images)\n",
        "                    #print(preds1)\n",
        "                    #preds2 = models[1](images)\n",
        "                    #preds3 = models[2](images)\n",
        "                    #preds4 = models[3](images)\n",
        "                    preds.append(pred)\n",
        "                # Combine predictions\n",
        "                final_preds = weighted_nms(preds, model_weights)\n",
        "\n",
        "                gt_targets = [\n",
        "                            dict(\n",
        "                                boxes=t[\"boxes\"].detach().cpu(),\n",
        "                                labels=t[\"labels\"].detach().cpu()\n",
        "                            ) for t in targets\n",
        "                        ]\n",
        "\n",
        "                metric.update(final_preds, gt_targets)\n",
        "\n",
        "                #print(final_preds)\n",
        "\n",
        "    mAP_result = metric.compute()\n",
        "    mAP_score = mAP_result[\"map\"].item()\n",
        "    mAP_50 = mAP_result[\"map_50\"].item()\n",
        "    print(mAP_score)\n",
        "    print(mAP_50)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_3146/2660485587.py:119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  box1_tensor = torch.tensor(box1).unsqueeze(0)\n/tmp/ipykernel_3146/2660485587.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  box2_tensor = torch.tensor(box2).unsqueeze(0)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.7960504293441772\n0.962081253528595\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1743750226179
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save each model with its assigned AdaBoost weight\n",
        "model_data = {\n",
        "    \"models\": [],\n",
        "    \"model_weights\": model_weights,  # Save AdaBoost model weights (alphas)\n",
        "    \"model_backbone\": backbones,  #list of backbones of the models\n",
        "}\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    model_state = {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"alpha\": model_weights[i],  # Save the weight assigned by AdaBoost\n",
        "    }\n",
        "    model_data[\"models\"].append(model_state)\n",
        "\n",
        "torch.save(model_data, \"adaboost_faster_rcnn.pth\")\n",
        "print(\"Model and weights saved successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model and weights saved successfully!\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1743750245522
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "class_names = test_dataset.classes\n",
        "\n",
        "def test_model_adaboost(models, test_loader, device, class_names, model_weights, visualize=False):\n",
        "    # Move models to device and set to eval mode\n",
        "    for model in models:\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "    # Initialize metrics and accumulators\n",
        "    metric = MeanAveragePrecision()\n",
        "    total_test_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idxs, images, targets in test_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Collect predictions from all models\n",
        "            all_predictions = []\n",
        "            for model in models:\n",
        "                pred = model(images)\n",
        "                all_predictions.append(pred)\n",
        "\n",
        "            # Apply Weighted NMS using AdaBoost weights\n",
        "            final_preds = weighted_nms(all_predictions, model_weights)\n",
        "\n",
        "            # Compute loss (optional, only for evaluation)\n",
        "            total_loss = 0\n",
        "            for model in models:\n",
        "                loss_dict = model(images, targets)\n",
        "                if isinstance(loss_dict, list):\n",
        "                    losses = sum(sum(loss_value.sum() for loss_value in loss.values()) for loss in loss_dict)\n",
        "                elif isinstance(loss_dict, dict):\n",
        "                    losses = sum(loss_value.sum() for loss_value in loss_dict.values())\n",
        "                else:\n",
        "                    raise TypeError(f\"Unexpected loss format: {type(loss_dict)}\")\n",
        "                total_loss += losses.item()\n",
        "\n",
        "            total_test_loss += total_loss / len(models)  # Averaging loss over models\n",
        "\n",
        "            # Convert ground truth to required format for mAP computation\n",
        "            gt_targets = [\n",
        "                dict(\n",
        "                    boxes=t[\"boxes\"].detach().cpu(),\n",
        "                    labels=t[\"labels\"].detach().cpu()\n",
        "                ) for t in targets\n",
        "            ]\n",
        "\n",
        "            # Update metric\n",
        "            metric.update(final_preds, gt_targets)\n",
        "\n",
        "            # Optional: Visualize some predictions\n",
        "            if visualize:\n",
        "                visualize_predictions(images, final_preds, class_names)\n",
        "\n",
        "    # Compute Mean Average Precision (mAP)\n",
        "    mAP_result = metric.compute()\n",
        "    mAP_score = mAP_result[\"map\"].item()\n",
        "    print(f\"\\nMean Average Precision (mAP): {mAP_score:.4f}\")\n",
        "\n",
        "    # Compute average test loss\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    print(f\"\\nAverage Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "    return mAP_score, avg_test_loss\n",
        "\n",
        "def visualize_predictions(images, predictions, class_names):\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        # Denormalize image\n",
        "        img = denormalize(img.cpu(), mean, std)\n",
        "        img = F_transform.to_pil_image(img)  # Convert to PIL image\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "        ax.imshow(img)\n",
        "\n",
        "        # Get predicted boxes, labels, and scores\n",
        "        pred_boxes = predictions[i]['boxes'].cpu().numpy()\n",
        "        pred_labels = predictions[i]['labels'].cpu().numpy()\n",
        "        pred_scores = predictions[i]['scores'].cpu().numpy()\n",
        "\n",
        "        if len(pred_scores) == 0:\n",
        "            print(\"No predictions found for this image.\")\n",
        "            continue\n",
        "\n",
        "        # Find the highest confidence score\n",
        "        pred_scores_tensor = torch.tensor(pred_scores) \n",
        "        max_score_idx = pred_scores_tensor.argmax()  # Get index of highest score\n",
        "\n",
        "        # Get highest confidence box, label, and score\n",
        "        best_box = pred_boxes[max_score_idx]\n",
        "        best_label = pred_labels[max_score_idx]\n",
        "        best_score = pred_scores[max_score_idx]\n",
        "        ixd_best_label = pred_labels[max_score_idx].item()\n",
        "        predicted_label_name = class_names[ixd_best_label - 1]\n",
        "\n",
        "        # Draw Highest Confidence Bounding Box (Red)\n",
        "        xmin, ymin, xmax, ymax = best_box\n",
        "        rect = plt.Rectangle(\n",
        "            (xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "            fill=False, color=\"red\", linewidth=2\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Display label and score\n",
        "        ax.text(\n",
        "            #xmin, ymin - 5, f\"{predicted_label_name}: {best_score:.2f}\",\n",
        "            xmin, ymin - 5, f\"{predicted_label_name}\",\n",
        "            color=\"red\", fontsize=12, bbox=dict(facecolor='white', alpha=0.7)\n",
        "        )\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Highest Confidence Prediction\")\n",
        "        plt.show()"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1743750245814
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on the test dataset\n",
        "# Run evaluation using AdaBoost predictions\n",
        "mAP_score, avg_loss = test_model_adaboost(\n",
        "    models=models,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    class_names=test_dataset.classes,\n",
        "    model_weights=model_weights,\n",
        "    visualize=True  # Set to False if you don't need visualization\n",
        ")\n",
        "print(mAP_score,avg_loss)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_3146/2660485587.py:119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  box1_tensor = torch.tensor(box1).unsqueeze(0)\n/tmp/ipykernel_3146/2660485587.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  box2_tensor = torch.tensor(box2).unsqueeze(0)\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1743750248894
        }
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernel_info": {
      "name": "mykernel"
    },
    "kernelspec": {
      "display_name": "new_kernel",
      "language": "python",
      "name": "mykernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}